"人工智能;;2016 Code大会精彩回顾:描绘机器智能宏伟蓝图导言：如果你错过了上周的 Code 大会，可能会错过许多。这场技术和媒体盛事汇聚了最顶尖的人物，让他们在同一个平台上畅所欲言。机器学习、人工智能、深度模拟、或如 IBM CEO 所坚称的认知计算，已成为几乎每一位 Code 大会发言者的关注焦点。今年，每个人似乎都没有所隐瞒。Code 大会之前已经见证过了一些颠覆性的变化，举几个例子：数字媒体、移动设备和智能手机。如今，这场盛会也为我们描绘了一幅宏伟蓝图：人工智能技术正以难以置信的速度和深度快速扩展。人工智能已经成为了技术领先企业中的一项主流技术；而且在不久的将来，这项技术就将成为几乎每一种领先的产品和服务的组成部分。当你将顶尖公司领导者、资深科技精英观众、最优秀的面试官这三重身份重合在一起时，会得到什么？答案是 Code 大会。已经是第十三次举办这种独一无二的大会了，大会依旧展示了它一如既往的时效性、启发性和趣味性。尽管大会经历了一系列诸如所有权的更迭、改名、场地变换（包括座位重排）等事件，但 Walt Mossberg 和 Kara Swisher 以及众多支持他们的 Recode 的作者始终在不懈努力着，以期营造一个场所，让我们能够听见（或看见）技术、商业趋势是如何被时下顶尖公司创造、使用或运作的。曾经有许多年代被称为元年（比如 iPod 的出现，甚至Windows 或 Office 的发布），而其他则更多的被关于破坏的争论所淹没（比如网络中立性或音乐发行）。在过去 12 届大会的历程中，或许曾有一些被每个人都提及的主题，但是，我们无法回想起任何一个场景曾在某个技术上汇聚如此一致的前瞻性思考。演讲者们用大量篇幅全面阐述，他们的用户将如何从产品和服务中的智能技术（我们称之为「人工智能」）中受益。为什么这不仅仅是技术发展周期道路上的一个路标？一种简短的回答是，因为作为消费者，我们每天都在手机上「使用」智能。我们必须承认的是，在过去一年中，人工智能已经从理论付诸实践，转变成日常使用。如今对于人工智能的使用不再是纸上谈兵，已成现实。全面解析在聆听演讲者回答关于人工智能在他们各自的企业中将会或正在扮演何种角色的题的过程中，最吸引人的部分就是人工智能技术如何横跨设备、战略以及商业模式。尽管我们每个人可能会对某一个特定案例比较熟悉，演讲者却为我们描绘了一幅宏伟蓝图——人工智能技术正以难以置信的速度和深度快速扩散。Google CEO Sundar Pichai 刚刚结束 Google I/O 大会的 Google CEO Sundar Pichai 带我们领略了人工智能在 Google 的发展史。显然在过去这段时间中，尚未有其他公司在人工智能的钻研和使用上拥有如此的深度和广度。毋庸置疑，人工智能一如既往地定义了谷歌，但直到近几年这一点才被广泛的理解。照片、收件箱、搜索、广告、助手、自动驾驶汽车以及更多项目都被 Pichai 作为例子来阐述 Google 在人工智能方面不间断的投入。如果人工智能本身是一个平台，那么， Google 很有可能是投入最多、最适合成为这个平台领导者的企业。以下是 Walt Mossberg 与 Pichai 对话中有关人工智能的内容：谷歌推出了一系列产品，但其中贯穿一条主题，你给谷歌设定的主题是人工智能，机器学习，这是下一个十年的热点，我们正迈向人工智能，你是怎么看出来的？这是一个很好的总结。我们肯定看到了巨大机遇，看到了拐点。大概三四年前， 我们感觉到了拐点，当时我们将深度学习用于语音识别，还有计算机视觉，都取得了巨大进步。我们花了很长很长时间做这方面的研究，其实很早很早以前，就开始训练我们的算法，在许多特定实例中机器学习真的可以更好地完成很多任务，这都是有原因的。三四年前感觉到拐点后，谷歌内部也经历了一次大的转变，我们开始注重如何将这些技术付诸于外。我们感觉移动端是很大的平台，可以帮助我们实现这个转变。这是一个完全不同的范式。用户一直使用手机，也希望迅速得到帮助。我们就是这样看出拐点的。人工智能方面，微软有 Cortana , FB 有人工智能人工智能, 苹果有siri，亚马逊也有自己的Alexa等，为什么你认为谷歌会比他们做的更好？首先，我们的研究时间更长，看看今天运行规模，无论是其中的计算能力还是我们投入的时间，无论你用什么质量指标给这块市场定基准，比如和谷歌进行各种对话，其回答能力具有内部扩展性，它可以进行 follow on 对话。这些领域展现了谷歌其他公司的真正区别所在。这是我们长时间研究的结果，用户向谷歌问答也很长时间了，语音方面也存在这样的趋势，谷歌的询问体量不逊任何人。无论是使用还是体量规模方面，我们都更好。你是说，你们比微软，苹果等对手要好？你提到的这些公司都是现象级公司。这不是一场《权力游戏》更像是NBA冠军赛。当谈到将机器学习和人工智能以一种有意义的方式带给用户时，还不能自豪地说我们完全做到了，目前我们大家都还处于早期阶段。我们做了各种基准研究，我们大致感觉领先。语音理解方面，比如说，「接通我妻子的电话」，系统最开始要学会谁是我妻子，然后记住。如果你说「接通sweetie」，这个单词语义很多，怎么解决这个问题？理解文本非常难，人类很擅长。计算机很擅长某些事情，不过，情况正在改变，过去两三年，为什么从事这个领域的研究非常让人兴奋，因为我们现在已经开始做一些更加智能的事情，比如理解context,理解所处情景，比如理解谁在问。现在正在解决类似的重要问题。几年前，我拜访过Google Now 的语音团队，还有你，在包装谷歌语音产品方面，你们做的不太好，不像 Siri 和后来的 Cortana，GoogleNow 的语音部分甚至没有自己的名字...部分原因在于，我们认为，这款产品应该因用户不同而有所不同，以某种方式打造专属每个人自己的谷歌。我们思考着如何回答，对用户做出反应，比如一位十岁印尼男孩或者还是你（Walt Mossberg ）这样的身份，我们还不能完全确定什么样的身份可以在所有那些情况中奏效，因此我们希望它用起来有点怪（eccentric），我认为，打造一款能随着时间推移，日益智能化的系统，需要能够理解用户，成为他们的助理，成为朋友，实现这一点，需要时间。我们认为，系统会不断成长和演化，有很多实现的方式，在这么早的阶段，我们还不想把自己局限在一条路上。我们想看它往何处发展。几年前，语音团队告诉我，你们在研究让系统记住对话状态，就像人类对话一样，不用重复对话内容，比如，你问它新西兰有多只羊。谷歌回答说，比如说，4000万只。接着你问，总统是谁？系统会给出新西兰首脑的名字，因为它记得对话领域（domain)。这属于自然语言处理领域。这就是我们可以做多好的例子，比如系统需要理解代词。你说的绵羊的例子只是例子之一，很多时候，你可能谈到电影，第二天说想买电影票，你会希望系统完成这个任务。实现这一点，还需要深入研究。在我看来，智能助力要能真地理解对话，这很难，就像我们接着以前的话题继续聊，实现这个目标，还有很长的路要走，这也是这项研究让人兴奋的地方。人们可以通过很多平台和渠道使用谷歌搜索（不仅仅是安卓系统），有些人认为，其他公司的人工智能助手可能会吞噬谷歌的搜索地盘，你怎么看？在我看来，这就像是 PC 朝移动端的转变。你仍然看到人们在使用 PC，不是取代，而是说 PC 和移动端并存。整体而言，计算在变大，同样，人类对信息的需求也在暴增，因此人们通过很多不同方法去获取信息，我们希望能够帮助他们。这是一个自然演化的过程，前面还有更大的增长，所以，我真的不认为这是一个零和博弈，而是一个重要的拐点。为什么你觉得谷歌的家庭硬件（home hardware）会比亚马逊的 ECHO 更好？下一个五到十年里，真正的对话理解，进行对话，这种技术是我们与对手的区别所在，也是我们一步步计划实现的，硬件不过是这些技术的一个体现。我们想要帮助用户使用这些技术完成工作。很多工作，我们都做在了前面。想想之前的谷歌地图、邮件，都是第一个地图和邮件产品，当然，搜索不是，我们的眼光放得非常长远，也投入了大量金钱和力量。隐私问题，亚马逊的产品在用户家里倾听一切，谷歌产品可能也会给用户这种起鸡皮疙瘩的感觉。人工智能和机器学习可以帮助我们更好地实现隐私控制。Amazon CEO Jeff Bezos 亚马逊 CEO Jeff Bezos 用大量篇幅讲述了人工智能在公司的突破性产品—— Echo 中扮演的角色。从许多方面来说，Echo 已经象征了多重技术的真正潜力——包括语音指令、代理中介、机器学习等，而这些功能都被打包集成在一个极其简单的消费者设备中。此外，Bezos 也概述了 Echo 的基础技术 Alexa 如何既是一个可定制平台又是一个可嵌入技术。开发者可以为 Alexa 构建新的「技能」并使之「学习」以提供新的能力（正如 Mossberg 所说的，Echo 的拥有者每周都会收到邮件，里面详细说明了最新增加的技能）。制造者可以在他们自己的设备中嵌入 Alexa 技术，其中一个被提到的例子是闹钟——这使得一个普通的技术转变为另一个人工智能使能的末端节点。以下是 Walt Mossberg 与 Bezos对话中有关人工智能的内容：亚马逊的家庭助理 Echo 使用 Alexa 作为平台支撑，现在你正逐渐将 Alexa 开放给其他开发者使用，是吗？我们是开放了 Alexa 的语音服务。Alexa 有两个软件开发工具包，一个是 Alexa 语音服务（Alexa Voice Services），你能够通过一套 VPI 将其嵌入到你自己的设备或程序应用中。例如，你可以将它嵌入到你自己制作的闹钟里。另一个开发工具包，是 Alexa 技能包（Alexa Skills Kit），它能让你「教」Alexa 新的技能。目前，其它的大公司也在通过机器学习，加快了在人工智能领域研究的步伐，有些是智能机器人，有些是语音智能。你认为这是继智能手机热潮之后的下一个市场热点吗？是的。我认为再怎么强调人工智能/机器学习在下一个20 年对社会的影响都不为过。但这不意味着智能手机会从人们的视线中消失，智能语音系统并不会取代手机屏幕的地位。只要我们还有眼睛，人们就需要屏幕，并在电子屏幕上进行相关操作。在语音智能这方面，很早的科幻小说里就有各种能够与你自然交谈的智能电脑。我认为，对新的更好的算法的有效结合、计算能力的提升、以及对大型训练数据的使用，是我们能够在机器学习上取得显著进步的主要原因，它们也将为市场提供更多智能产品。那么亚马逊也会逐渐转向以人工智能为重心吗？必须的。这四年来我们一直在背后努力着，光在 Echo 和 Alexa 之后，就有超过一千人的团队在进行研究和维护。但这仅仅是一个开始。我十分激动，因为我认为我们即将踏入一个新的「黄金时代」。不仅仅是大公司，还会有许许多多的创业公司出现，以及许多我们难以预见的新技术。现在大公司更有优势，是因为大公司拥有更大的数据库；但是现在我们所训练的算法，与人类实际上思考的方式是有很大差别的。人脑在使用数据上是极其有效率的：我们无须那么多的数据，就可以总结出复杂事物的规律。同时我们在使用能量上也非常有效率。谈到大数据，隐私保护一直是人们关心的话题，目前也有许多争论进行着，尤其是往往我们收集人们日常活动的信息，仅仅是为了更有针对性地投放广告。对于隐私的问题，你怎么看？从亚马逊的商业操作上来说，我们收集客户的使用数据，是为了向用户推荐他们可能会感兴趣的商品。过去 20年的使用也证明，大部分用户是喜欢看到我们有针对性地推荐商品的。同时，我们在收集和储存数据的时候，会非常明确我们这么做的目的。同时，在必要的情况下，我们会让用户知道，系统是存有他们的数据的。例如，当你再次登录亚马逊的时候，页面会显示「欢迎回来，ＸＸＸ」，这样你知道你在亚马逊上不是匿名的——我们是知道你的名字的。在技术上，我们和苹果使用的是同一种加密协议。但是，即使现在的加密技术已经发展到连开发商本身都无法解密的程度，黑客的攻击也是不可能被根除的。我认为隐私保护问题正是我们这个时代的问题。坏人会越来越先进，那么好人也必须越来越先进。我想我们无法杜绝黑客的入侵，但我们必须不断进步，这就像是一场不停歇的猫追老鼠，我们知道保证永远走在坏人前面就可以了。对于接下来的五年，你有什么新计划吗？我们已经种下了许多种子，接下来我们会继续等待谁会长成参天大树。其中一个我比较看好的项目是Amazon Studios。此外，我们还是会继续发展 Echo, Alexa 和其它自然语言理解项目。事实上，我相信未来出现许多的人工智能代理，就像现在种类繁多的网站和手机应用一样。不同的助理可能会有不同的长处，你不会让一个人工智能完成所有的任务，所以人们可能也会同时使用多个人工智能。对我来说，这是一个非常激动人心的前景，也是我们正在努力的方向。我热爱投身于这样的工作，我们的队伍也特别地酷。（观众）许多大型客机公司都已经在可穿戴设备上投入了许多精力，而亚马逊在这一领域似乎还没有什么动作，对此你是怎么想的？我认为这是一个非常有趣的市场，而同时这也是一个仍处于初步发展阶段的市场，所以，未来会有很多的可能性，我们也会看到许多非常有趣的产品。Elon Musk马斯克分享了一个称作「神经蕾丝（neural lace）」的实验想法，他认为，这或许可以降低人类沦为超智能生物的宠物的风险。「这个似乎是最棒的解决方案，即植入一个AI层，」他说道。「所以想想吧，如果你有自己的表皮系统，皮下组织，以及一个数字层——他们运作良好并与你合而为一。就像你的皮下组织与你的表皮系统共生一样，这个数字层将会与你身体的其他部分共生。」简而言之，这个神经层将会增强我们的输入输出能力，或者处理与交换信息的能力。「我们就是半机械人了，」马斯克说，重复着关于类似手机的设备及诸如因特网的系统实时赋予我们超能力的论点。「但是这种约束就在于输入输出，」他继续道。「我们受到输入输出限制——尤其是输出限制。」尽管我们能够感知并整合巨大的输入信息，我们不能大量炮制同一纬度的信息。我们只能更快的打字和说话。而在神经蕾丝的世界，理论上，允许我们用数字方式连接和交流，以克服生理上的缺陷。马斯克回避了神经蕾丝是通过外科手术亦或种族繁殖的方式植入人体的问题，但是他也暗示了对皮质神经元采用直接接口的必要性，以及这可能会通过连接着神经元的静脉和动脉来实现。马斯克没有提及他已经在研究神经蕾丝，也没有坚持说工作正在进行。「有人会去做的，」他说。「如果没人做，我觉得我就会去做。」另外，虽然没有透露公司名称，但是马斯克承认的确有一家人工智能公司让他担忧。以下是与 Musk 对话中有关人工智能的部分：许多科技行业的人们都相信，人工智能带给人类的未来是光明的，它们能让我们的生活变得更好。在这方面，你似乎有不同的观点，可以详细解释一下吗？人工智能可能带来的未来有很多可能性，我比较关心的是其中不那么乐观的结果。如果我们真的能够创造比我们强大很多的人工智能，那么我们最好能够保证它们带来的影响是积极的。我创立了 OpenAI，它事实上是一个非盈利的组织，在管理中我试图保证，我们研究技术不是以开发产品为目标。很多此类公司没有一种急迫感，而我们的工作节奏是很快的，近期也有很多非常棒的人才加入了我们。OpenAI 建立的目标，是让人工智能更加民主化。Lord Acton 第一次提出「权力腐败」这个概念，他曾说过，「自由是由权力的合理分配组成的，而独裁就是权力的完全集中。」如果我们拥有强大的人工智能，我们需要保证它不会被少数人操控。你觉得那样的未来是什么样的？许多人称之为奇点，因为我们难以预见那到底会是什么样的。你可以将所有的可能性以光谱的形式表现出来——有最好的结果，也有最坏的。OpenAI 在努力做的事情就是，尽我们所能，让人工智能不被少数人操控，从而能将未来引向好的一面。但是将人工智能技术开放之后，你是否担心它也会被用于邪恶的目的？是的，这是一个潜在的问题。所以关键在于，人工智能的能力应该被广泛地分布，例如，让每个人拥有一个人工智能助理。这样，即使有少数个体想要利用人工智能制造麻烦，其它的人工智能能够共同协作，来对抗邪恶的那一个。但是，如果那一个人工智能比其余的要先进上百万倍的话，当它被操控于邪恶的目的时，我们恐怕就拿它无能为力了。所以这是一个实实在在的风险，而最大化避免它发生的关键，就在于民主化。（观众）这是一个奇怪的问题，但我觉得你能给出正确的回答。如果我们真的创造出全面的人工智能，那么它们可能就是一种模拟人类，而这证明，任何足够先进的文明都能够创造一个像我们人类这样的物种。所以，这是否意味着，我们人类本身可能就是另一种生物的模拟？首先，我想能够支撑我们是某种模拟的最有力的论证是这样的：40 年前，苹果开发出了最初级的电脑游戏 Pong——它的组成部分只有两个长方形和一个球；现在，我们已经有像照片一样逼真的 3D模拟游戏。这些游戏拥有成千上万的玩家，并且它们的技术每年都在不断进步，未来，我们还会有虚拟现实等等。不论你假设技术会以怎样的速度发展，我们最终都会达到一个终极阶段，使得人们难以区分游戏和现实。即使你假设技术发展速度为当前的千分之一，或者说我们一万年后才能达到这一步，这点时间从进化的角度看，都根本不算什么。既然我们难以避免地一定会制作出与现实无二的游戏，而且成千上万的人们可以在任何设备上运行这些游戏，而且可能有数十亿的设备在运行这些游戏——那么这样看来，可能有十亿分之一的概率，我们是在某一个游戏中。那你怎么看？我认为我们是那十亿分之一，因为这是唯一的结论。事实上如果这是真的，那么这是件好事，因为一旦文明停止进化，那么这就可能是因为生成文明的东西出现了问题，（这样我们就有办法重启）。所以，如果我们无法创造出模拟人类世界的软件，那么这意味着文明总有一天会消失。（观众）你认为完全自动化的无人车什么时候可以成为现实？例如，一个不会开车的老人家能够让车把她安全带到机场。同时，你认为什么时候人类驾驶汽车会成为严格限制的行为，甚至会成为违法的？我认为自动驾驶汽车很大程度上可以被视为已经解决的问题了。即使是在北京这样的地方？北京确实是路况很复杂的地方，车流量也非常大。但是，事实上，密集的车流反而是非常易于自动驾驶的，因为你不需要考虑超车等等的问题，只需要和周围车辆保持安全距离就可以了。在没有路障、路况良好的高速公路上，使用自动驾驶汽车也是很容易的事情。我认为，让普通人使用无人工操作的汽车，应该在汽车完全自动化的两年内就能实现。但是，政府可能至少还需要一年来通过，因为决策者们需要看到在长距离、各种路况下的大量试驾结果，来证明这是百分之百安全的。同时，我个人希望人们拥有充分的自由，所以我不支持完全禁止人类驾驶汽车，但是，未来的驾驶执照可能会对驾驶人有更高的技术要求。IBM CEO Ginni Romety IBM CEO Ginni Romety 基于 IBM 长久以来在人工智能领域的耕耘成果，概述了是什么使得公司近乎押注在了技术投入上。她讨论了从卫生保健到教育、从商业 IT 到第三方开发者、从云到企业预置等多种场景中 IBM 努力促进和支持人工智能的使用。最有趣的是那些 IBM 努力提供的开源或免费的解决方案——Rometty 说这些行为时有悖于 IBM 的历史的，但对于公司继续前进却是本质而必要的。当我们许多人认为是 IBM 的 Big Blue 或Jeopardy促成了 Watson 时，IBM 关于人工智能的历史却可以追溯到这个领域最初的时候，那时享誉世界的 Watson 实验室是在翻译、语言处理、演讲和手写识别甚至更多方面拥有最早构想的先驱者。尽管困难重重，Rometty 关于技术的展望以及IBM如此深度的参与程度还是给人们留下了深刻的印象。以下是 Kara Swisher 与 Ginni Romety 对话中有关人工智能的部分。你在人工智能领域已经拥有超过十年的名气了。我想在每个人的一生中，都有那么几个需要作出重要决策的时刻，而我们做出了这样的决定。事实上，我一般不称之为人工智能（尽管它好读又好拼），原因是这样的：在 2005 年，我们意识到当时的信息量和数据量在飞速膨胀，已经远远超出了人脑能够处理的范围，我们认为我们需要做些什么，于是我们开始了在这一领域的研究。我们研究的目的，是做出更好的决策，以及解决悬而未决的问题，这也是为什么我们最先选择了健康医疗领域。这一领域仍旧缺乏一个整体的系统，并且其资金浪费是巨大的，我记得是高达 8 兆美元。沃森在 2011 年成为智力竞猜冠军时引发了巨大的反响，而你事实上很早就开始这方面的工作了。你是如何将沃森的技术商业化的？当我们开发出沃森之后，我想：为什么我们不能把它应用到日常生活中呢？因为我已经确确实实预见到，我们的未来无法避免认知人工智能的普及化。在商业应用方面，我们将其应用到健康医疗、教育、金融服务等等行业。前段时间我参观了我们一个客户的工厂，负责人说，一般他们先人工处理任务，然后再让沃森处理一遍，而往往沃森所发现的问题，是工作人员永远无法发现的。我认为，对于人工智能的未来，我们还需要考虑一些方面。对此人们往往会想到，我们需要处理很多无结构的数据，例如之前在大会上谈过的自然语言处理和图像等等。然而，除此之外，我认为我们还需要考虑专业领域的知识，以及理解因果和进行学习的能力。如果你能在某一领域达到这样的水平，一切就会完全不一样了。如果你是一个生态学者，你会如何观察白板上的所有信息和材料，形成你自己的假设，并对其作出评估？而当专业人士需要决策建议的时候，他们需要的不是一个单一的答案，而是一套比较和分析的过程，包括不同选择的利弊和潜在影响等等。例如，一个癌症病人可能非常不希望自己的头发掉光，因为这对他来说很重要。所以，没有绝对正确的答案，而这就是帮助我们处理灰色地带的工具。Facebook COO Sheryl Sandberg , Facebook CTO Mike Schroepfer 或许我们体验到的对于人工智能最频繁的使用，就是在手机上使用 Facebook 。Facebook COO Sheryl Sandberg 和 CTO MikeSchroepfer 概述了在网站、广告以及虚拟现实等领域关于人工智能的诸多努力。有人认为，在人工智能方面，产品功能以及应用研究深度都未得到广泛认可。比如，几周以前，Facebook 向 ICLR 提交了十几页论文。正如下文所说，我们每个人作为「人」的角色（而非「用户」——根据两位演讲者的说法），为公司提供基于人工智能的功能服务做出了巨大贡献，比如，我们觉得十分有价值的照片标记功能。eBay CEO Devin Wenig尽管花费了大量时间区分自己的公司和亚马逊，eBay CEO Devin Wenig 仍详细描述了 eBay 在提升购物体验以及通过使用人工智能消除欺诈方面令人惊叹的成果。Wenig 描绘了通过使用人工智能的方式，公司将会提供一个高度个性化的 eBay ——一个受过调教的、具有高相关度的 eBay。他声称（来源于 Lauren Goode 的后续访谈，来自 Recode 的 Vox 合作伙伴 The Verge）通过使用人工智能技术，欺诈行为已经成为一个「无意义」的数字。Ford CEO Mark Fields福特 CEO Mark Fields 努力使Code 的听众们相信，尽管福特未必拥有人工智能或云技术方面的固有优势，但他们正积极的寻求合作（比如和Pivotal ）并将需要的技能带入到企业内部，以确保福特汽车参与到这一波技术浪潮中。尽管 Fields 没有深度清晰地解读人工智能的远景，但他使用了关于保养维护和导航方面的实例来阐释公司的投入和决心。Bill Gates以下是与盖茨夫妇对话中有关人工智能的内容。人工智能是当下的热门话题，你认为这是否确实是一项重要的话题？它是否有潜在的危险？我认为人工智能确实是当下一件非常重要的事情，实现人工智能可能是所有计算机工程师的梦想。现在，在语音和视觉方面，我们已经开发出了比人类还要强大的系统，所以过去五年里人工智能的发展达到了前所未有的高峰。在接下来的十年内，我们可能会在现实世界的任务上有更多的尝试，例如开车、做家务等等，最终我们会拥有比人工成本更低的机器人，来完成这些体力任务。（梅琳达）同时，我们十分关注女性在计算机行业的发展。在我从计算机专业毕业的时候，34% 的毕业生都是女性，而现在却下降到 17%。但是，我认为在人工智能方面，我们也十分需要大量的女性工程师的加入，来让我们的产品更多样更全面，特别是在类似于健康护理这样的领域。Cisco CEO Chunk Robbins思科 CEO Chunk Robbins 以类似福特的方式谈及了公司数以千计的软件工程师投入到人工智能的研发中，并将集成人工智能到福特现有的产品线中去。他讲述了通过使用人工智能可以带来更好的管理，并帮助理解将会包含数以百万计的各种类型的末端节点的现代网络。为什么是今年？可以肯定地说，Code 大会之前已经见证过了一些颠覆性的变化，举几个例子：数字媒体、移动设备和智能手机。人工智能正被证明是一种不同类型的改变——一个不是让人们抗拒和担忧会破坏其原有业务的变化，而是一个所有人都在拥抱和朝那个方向发展的变化。一路走来，人们关注的有趣问题将不再是哪家公司使用了人工智能，而是哪家公司以全新的方式最大化地利用了人工智能。这一领域还有大量的发明要做，因为即使这项技术正在快速发展，但总归仍旧处在非常早期的起步阶段。例如，微软在两年前的 Code 大会上给出了预览的 Skype Translator 正在使用最现代的深度学习人工智能技术进行打造。今天人工智能是如此的主流，以至于数十位使用该技术明晰其公司执行规划的 CEO 为该技术的愿景中的四个重要改变做出了直接的贡献。这些技术是怎么以这样一种高科技的方式组合到一起的，使从实验室到实际部署的功能成为可能？我认为这是值得反思的。模型的原始计算能力：尽管我们乐于谈论摩尔定律提供了这么大的推动力，但当涉及到人工智能时，推动力则来自应用于并行架构的摩尔定律，而非英特尔的标量的那种。图形处理单元（GPU）上更多晶体管的应用已经成为了人工智能技术的关键推动力。云架构也是推动力中非常重要的组成部分，因为企业再也不需要在它们自己的 GPU 数据中心上构建就能使用人工智能训练模型的力量了，而且还能按需订购所需的规模。用于训练的海量数据容量：所有人都开始了解到更多数据是训练人工智能模型的唯一方法，而且再多数据也无妨（但却很容易陷入数据过少的境地）。只是直到最近，云架构才在保存和获取训练所需的数据量上变得「普遍」和「经济可行」。Facebook 提供了对此的一个最容易想象的视角：其在每天超过 3 亿张照片（大约半个 PB）的基础上训练对人的识别。IBM 使用放射图像的案例为我们理解存储的演进对人工智能的重要性提供了另一个视角。有标签数据的难以置信的可用性：就像数据对训练很重要一样，没有标签（labels）数据也不会很有用。这是我们在技术的使用中扮演了一个重要部分的地方，就像互联网的开放性一样。我们不只是将照片上传到 Facebook 上，我们还标记我们知道的人；而在这么做时，我们就帮助训练了图像识别引擎。对 eBay 来说，说其希望提供一个个性化的商店是不够的，而还需要我们登录并购买商品来为其个性化引擎提供信息。这是在我们过去思考点击流或猜测某人是否是回头客的方式之上的一种进步。除此之外，我们手机中的传感器还提供了运动和位置数据，从而增强了我们所做的一切。清楚而显然的是，所有这些都存在隐私和安全问题，但与此同时，我们每个人在使用这些服务时所得到的个人好处也都是前所未见的。这种数据可用性超越了我个人所产生（和标记）的数据，还包括了因为开放和基于云的解决方案而可用的数据集和 API（例如，来自政府的经济和人口数据），这些数据集和 API 可以被整合为训练模型的一部分。技术基础的开放实现：人工智能解决方案的兴起中最迷人的方面是：如此之多的核心技术在开放的环境中进行开发（通常由许多企业的研究军团主导），或至少在这项技术演化的相对早期阶段以一种开放的方式提供贡献。谷歌的 TensorFlow、Facebook 的 Torch、IBM 的SystemML 和加州大学伯克利分校的 CAFFE 以及一些用于数据的技术（如 Spark）都是公开可用的平台元素。可以很肯定的是，这遵循了类似 HTML/HTTP 的同样模式，这意味着其经济效益将会从系统的其它地方产生（当然是在数据、训练和模型中）。未来如何？在很短的时间跨度内，人工智能实现了一次飞跃，并且很可能已经越过了幻想破灭的低谷。我对此可是很明确的。尽管未来几年出现的一些成果中毫无疑问会有一些让人失望的东西，但同样毫无疑问的是，这样的怀疑将会通过大量受益于人工智能的书写和通信工具进行交流。没有什么能以每个人喜欢的方式做到每个人想要的每一件事。不需认证，很显然人工智能已经成为了技术领先企业中的一项主流技术；而且在不久的将来，这项技术就将成为几乎每一种领先的产品和服务的组成部分。"
"谷歌;;2016 Code大会精彩回顾:描绘机器智能宏伟蓝图导言：如果你错过了上周的 Code 大会，可能会错过许多。这场技术和媒体盛事汇聚了最顶尖的人物，让他们在同一个平台上畅所欲言。机器学习、人工智能、深度模拟、或如 IBM CEO 所坚称的认知计算，已成为几乎每一位 Code 大会发言者的关注焦点。今年，每个人似乎都没有所隐瞒。Code 大会之前已经见证过了一些颠覆性的变化，举几个例子：数字媒体、移动设备和智能手机。如今，这场盛会也为我们描绘了一幅宏伟蓝图：人工智能技术正以难以置信的速度和深度快速扩展。人工智能已经成为了技术领先企业中的一项主流技术；而且在不久的将来，这项技术就将成为几乎每一种领先的产品和服务的组成部分。当你将顶尖公司领导者、资深科技精英观众、最优秀的面试官这三重身份重合在一起时，会得到什么？答案是 Code 大会。已经是第十三次举办这种独一无二的大会了，大会依旧展示了它一如既往的时效性、启发性和趣味性。尽管大会经历了一系列诸如所有权的更迭、改名、场地变换（包括座位重排）等事件，但 Walt Mossberg 和 Kara Swisher 以及众多支持他们的 Recode 的作者始终在不懈努力着，以期营造一个场所，让我们能够听见（或看见）技术、商业趋势是如何被时下顶尖公司创造、使用或运作的。曾经有许多年代被称为元年（比如 iPod 的出现，甚至Windows 或 Office 的发布），而其他则更多的被关于破坏的争论所淹没（比如网络中立性或音乐发行）。在过去 12 届大会的历程中，或许曾有一些被每个人都提及的主题，但是，我们无法回想起任何一个场景曾在某个技术上汇聚如此一致的前瞻性思考。演讲者们用大量篇幅全面阐述，他们的用户将如何从产品和服务中的智能技术（我们称之为「人工智能」）中受益。为什么这不仅仅是技术发展周期道路上的一个路标？一种简短的回答是，因为作为消费者，我们每天都在手机上「使用」智能。我们必须承认的是，在过去一年中，人工智能已经从理论付诸实践，转变成日常使用。如今对于人工智能的使用不再是纸上谈兵，已成现实。全面解析在聆听演讲者回答关于人工智能在他们各自的企业中将会或正在扮演何种角色的题的过程中，最吸引人的部分就是人工智能技术如何横跨设备、战略以及商业模式。尽管我们每个人可能会对某一个特定案例比较熟悉，演讲者却为我们描绘了一幅宏伟蓝图——人工智能技术正以难以置信的速度和深度快速扩散。Google CEO Sundar Pichai 刚刚结束 Google I/O 大会的 Google CEO Sundar Pichai 带我们领略了人工智能在 Google 的发展史。显然在过去这段时间中，尚未有其他公司在人工智能的钻研和使用上拥有如此的深度和广度。毋庸置疑，人工智能一如既往地定义了谷歌，但直到近几年这一点才被广泛的理解。照片、收件箱、搜索、广告、助手、自动驾驶汽车以及更多项目都被 Pichai 作为例子来阐述 Google 在人工智能方面不间断的投入。如果人工智能本身是一个平台，那么， Google 很有可能是投入最多、最适合成为这个平台领导者的企业。以下是 Walt Mossberg 与 Pichai 对话中有关人工智能的内容：谷歌推出了一系列产品，但其中贯穿一条主题，你给谷歌设定的主题是人工智能，机器学习，这是下一个十年的热点，我们正迈向人工智能，你是怎么看出来的？这是一个很好的总结。我们肯定看到了巨大机遇，看到了拐点。大概三四年前， 我们感觉到了拐点，当时我们将深度学习用于语音识别，还有计算机视觉，都取得了巨大进步。我们花了很长很长时间做这方面的研究，其实很早很早以前，就开始训练我们的算法，在许多特定实例中机器学习真的可以更好地完成很多任务，这都是有原因的。三四年前感觉到拐点后，谷歌内部也经历了一次大的转变，我们开始注重如何将这些技术付诸于外。我们感觉移动端是很大的平台，可以帮助我们实现这个转变。这是一个完全不同的范式。用户一直使用手机，也希望迅速得到帮助。我们就是这样看出拐点的。人工智能方面，微软有 Cortana , FB 有人工智能人工智能, 苹果有siri，亚马逊也有自己的Alexa等，为什么你认为谷歌会比他们做的更好？首先，我们的研究时间更长，看看今天运行规模，无论是其中的计算能力还是我们投入的时间，无论你用什么质量指标给这块市场定基准，比如和谷歌进行各种对话，其回答能力具有内部扩展性，它可以进行 follow on 对话。这些领域展现了谷歌其他公司的真正区别所在。这是我们长时间研究的结果，用户向谷歌问答也很长时间了，语音方面也存在这样的趋势，谷歌的询问体量不逊任何人。无论是使用还是体量规模方面，我们都更好。你是说，你们比微软，苹果等对手要好？你提到的这些公司都是现象级公司。这不是一场《权力游戏》更像是NBA冠军赛。当谈到将机器学习和人工智能以一种有意义的方式带给用户时，还不能自豪地说我们完全做到了，目前我们大家都还处于早期阶段。我们做了各种基准研究，我们大致感觉领先。语音理解方面，比如说，「接通我妻子的电话」，系统最开始要学会谁是我妻子，然后记住。如果你说「接通sweetie」，这个单词语义很多，怎么解决这个问题？理解文本非常难，人类很擅长。计算机很擅长某些事情，不过，情况正在改变，过去两三年，为什么从事这个领域的研究非常让人兴奋，因为我们现在已经开始做一些更加智能的事情，比如理解context,理解所处情景，比如理解谁在问。现在正在解决类似的重要问题。几年前，我拜访过Google Now 的语音团队，还有你，在包装谷歌语音产品方面，你们做的不太好，不像 Siri 和后来的 Cortana，GoogleNow 的语音部分甚至没有自己的名字...部分原因在于，我们认为，这款产品应该因用户不同而有所不同，以某种方式打造专属每个人自己的谷歌。我们思考着如何回答，对用户做出反应，比如一位十岁印尼男孩或者还是你（Walt Mossberg ）这样的身份，我们还不能完全确定什么样的身份可以在所有那些情况中奏效，因此我们希望它用起来有点怪（eccentric），我认为，打造一款能随着时间推移，日益智能化的系统，需要能够理解用户，成为他们的助理，成为朋友，实现这一点，需要时间。我们认为，系统会不断成长和演化，有很多实现的方式，在这么早的阶段，我们还不想把自己局限在一条路上。我们想看它往何处发展。几年前，语音团队告诉我，你们在研究让系统记住对话状态，就像人类对话一样，不用重复对话内容，比如，你问它新西兰有多只羊。谷歌回答说，比如说，4000万只。接着你问，总统是谁？系统会给出新西兰首脑的名字，因为它记得对话领域（domain)。这属于自然语言处理领域。这就是我们可以做多好的例子，比如系统需要理解代词。你说的绵羊的例子只是例子之一，很多时候，你可能谈到电影，第二天说想买电影票，你会希望系统完成这个任务。实现这一点，还需要深入研究。在我看来，智能助力要能真地理解对话，这很难，就像我们接着以前的话题继续聊，实现这个目标，还有很长的路要走，这也是这项研究让人兴奋的地方。人们可以通过很多平台和渠道使用谷歌搜索（不仅仅是安卓系统），有些人认为，其他公司的人工智能助手可能会吞噬谷歌的搜索地盘，你怎么看？在我看来，这就像是 PC 朝移动端的转变。你仍然看到人们在使用 PC，不是取代，而是说 PC 和移动端并存。整体而言，计算在变大，同样，人类对信息的需求也在暴增，因此人们通过很多不同方法去获取信息，我们希望能够帮助他们。这是一个自然演化的过程，前面还有更大的增长，所以，我真的不认为这是一个零和博弈，而是一个重要的拐点。为什么你觉得谷歌的家庭硬件（home hardware）会比亚马逊的 ECHO 更好？下一个五到十年里，真正的对话理解，进行对话，这种技术是我们与对手的区别所在，也是我们一步步计划实现的，硬件不过是这些技术的一个体现。我们想要帮助用户使用这些技术完成工作。很多工作，我们都做在了前面。想想之前的谷歌地图、邮件，都是第一个地图和邮件产品，当然，搜索不是，我们的眼光放得非常长远，也投入了大量金钱和力量。隐私问题，亚马逊的产品在用户家里倾听一切，谷歌产品可能也会给用户这种起鸡皮疙瘩的感觉。人工智能和机器学习可以帮助我们更好地实现隐私控制。Amazon CEO Jeff Bezos 亚马逊 CEO Jeff Bezos 用大量篇幅讲述了人工智能在公司的突破性产品—— Echo 中扮演的角色。从许多方面来说，Echo 已经象征了多重技术的真正潜力——包括语音指令、代理中介、机器学习等，而这些功能都被打包集成在一个极其简单的消费者设备中。此外，Bezos 也概述了 Echo 的基础技术 Alexa 如何既是一个可定制平台又是一个可嵌入技术。开发者可以为 Alexa 构建新的「技能」并使之「学习」以提供新的能力（正如 Mossberg 所说的，Echo 的拥有者每周都会收到邮件，里面详细说明了最新增加的技能）。制造者可以在他们自己的设备中嵌入 Alexa 技术，其中一个被提到的例子是闹钟——这使得一个普通的技术转变为另一个人工智能使能的末端节点。以下是 Walt Mossberg 与 Bezos对话中有关人工智能的内容：亚马逊的家庭助理 Echo 使用 Alexa 作为平台支撑，现在你正逐渐将 Alexa 开放给其他开发者使用，是吗？我们是开放了 Alexa 的语音服务。Alexa 有两个软件开发工具包，一个是 Alexa 语音服务（Alexa Voice Services），你能够通过一套 VPI 将其嵌入到你自己的设备或程序应用中。例如，你可以将它嵌入到你自己制作的闹钟里。另一个开发工具包，是 Alexa 技能包（Alexa Skills Kit），它能让你「教」Alexa 新的技能。目前，其它的大公司也在通过机器学习，加快了在人工智能领域研究的步伐，有些是智能机器人，有些是语音智能。你认为这是继智能手机热潮之后的下一个市场热点吗？是的。我认为再怎么强调人工智能/机器学习在下一个20 年对社会的影响都不为过。但这不意味着智能手机会从人们的视线中消失，智能语音系统并不会取代手机屏幕的地位。只要我们还有眼睛，人们就需要屏幕，并在电子屏幕上进行相关操作。在语音智能这方面，很早的科幻小说里就有各种能够与你自然交谈的智能电脑。我认为，对新的更好的算法的有效结合、计算能力的提升、以及对大型训练数据的使用，是我们能够在机器学习上取得显著进步的主要原因，它们也将为市场提供更多智能产品。那么亚马逊也会逐渐转向以人工智能为重心吗？必须的。这四年来我们一直在背后努力着，光在 Echo 和 Alexa 之后，就有超过一千人的团队在进行研究和维护。但这仅仅是一个开始。我十分激动，因为我认为我们即将踏入一个新的「黄金时代」。不仅仅是大公司，还会有许许多多的创业公司出现，以及许多我们难以预见的新技术。现在大公司更有优势，是因为大公司拥有更大的数据库；但是现在我们所训练的算法，与人类实际上思考的方式是有很大差别的。人脑在使用数据上是极其有效率的：我们无须那么多的数据，就可以总结出复杂事物的规律。同时我们在使用能量上也非常有效率。谈到大数据，隐私保护一直是人们关心的话题，目前也有许多争论进行着，尤其是往往我们收集人们日常活动的信息，仅仅是为了更有针对性地投放广告。对于隐私的问题，你怎么看？从亚马逊的商业操作上来说，我们收集客户的使用数据，是为了向用户推荐他们可能会感兴趣的商品。过去 20年的使用也证明，大部分用户是喜欢看到我们有针对性地推荐商品的。同时，我们在收集和储存数据的时候，会非常明确我们这么做的目的。同时，在必要的情况下，我们会让用户知道，系统是存有他们的数据的。例如，当你再次登录亚马逊的时候，页面会显示「欢迎回来，ＸＸＸ」，这样你知道你在亚马逊上不是匿名的——我们是知道你的名字的。在技术上，我们和苹果使用的是同一种加密协议。但是，即使现在的加密技术已经发展到连开发商本身都无法解密的程度，黑客的攻击也是不可能被根除的。我认为隐私保护问题正是我们这个时代的问题。坏人会越来越先进，那么好人也必须越来越先进。我想我们无法杜绝黑客的入侵，但我们必须不断进步，这就像是一场不停歇的猫追老鼠，我们知道保证永远走在坏人前面就可以了。对于接下来的五年，你有什么新计划吗？我们已经种下了许多种子，接下来我们会继续等待谁会长成参天大树。其中一个我比较看好的项目是Amazon Studios。此外，我们还是会继续发展 Echo, Alexa 和其它自然语言理解项目。事实上，我相信未来出现许多的人工智能代理，就像现在种类繁多的网站和手机应用一样。不同的助理可能会有不同的长处，你不会让一个人工智能完成所有的任务，所以人们可能也会同时使用多个人工智能。对我来说，这是一个非常激动人心的前景，也是我们正在努力的方向。我热爱投身于这样的工作，我们的队伍也特别地酷。（观众）许多大型客机公司都已经在可穿戴设备上投入了许多精力，而亚马逊在这一领域似乎还没有什么动作，对此你是怎么想的？我认为这是一个非常有趣的市场，而同时这也是一个仍处于初步发展阶段的市场，所以，未来会有很多的可能性，我们也会看到许多非常有趣的产品。Elon Musk马斯克分享了一个称作「神经蕾丝（neural lace）」的实验想法，他认为，这或许可以降低人类沦为超智能生物的宠物的风险。「这个似乎是最棒的解决方案，即植入一个AI层，」他说道。「所以想想吧，如果你有自己的表皮系统，皮下组织，以及一个数字层——他们运作良好并与你合而为一。就像你的皮下组织与你的表皮系统共生一样，这个数字层将会与你身体的其他部分共生。」简而言之，这个神经层将会增强我们的输入输出能力，或者处理与交换信息的能力。「我们就是半机械人了，」马斯克说，重复着关于类似手机的设备及诸如因特网的系统实时赋予我们超能力的论点。「但是这种约束就在于输入输出，」他继续道。「我们受到输入输出限制——尤其是输出限制。」尽管我们能够感知并整合巨大的输入信息，我们不能大量炮制同一纬度的信息。我们只能更快的打字和说话。而在神经蕾丝的世界，理论上，允许我们用数字方式连接和交流，以克服生理上的缺陷。马斯克回避了神经蕾丝是通过外科手术亦或种族繁殖的方式植入人体的问题，但是他也暗示了对皮质神经元采用直接接口的必要性，以及这可能会通过连接着神经元的静脉和动脉来实现。马斯克没有提及他已经在研究神经蕾丝，也没有坚持说工作正在进行。「有人会去做的，」他说。「如果没人做，我觉得我就会去做。」另外，虽然没有透露公司名称，但是马斯克承认的确有一家人工智能公司让他担忧。以下是与 Musk 对话中有关人工智能的部分：许多科技行业的人们都相信，人工智能带给人类的未来是光明的，它们能让我们的生活变得更好。在这方面，你似乎有不同的观点，可以详细解释一下吗？人工智能可能带来的未来有很多可能性，我比较关心的是其中不那么乐观的结果。如果我们真的能够创造比我们强大很多的人工智能，那么我们最好能够保证它们带来的影响是积极的。我创立了 OpenAI，它事实上是一个非盈利的组织，在管理中我试图保证，我们研究技术不是以开发产品为目标。很多此类公司没有一种急迫感，而我们的工作节奏是很快的，近期也有很多非常棒的人才加入了我们。OpenAI 建立的目标，是让人工智能更加民主化。Lord Acton 第一次提出「权力腐败」这个概念，他曾说过，「自由是由权力的合理分配组成的，而独裁就是权力的完全集中。」如果我们拥有强大的人工智能，我们需要保证它不会被少数人操控。你觉得那样的未来是什么样的？许多人称之为奇点，因为我们难以预见那到底会是什么样的。你可以将所有的可能性以光谱的形式表现出来——有最好的结果，也有最坏的。OpenAI 在努力做的事情就是，尽我们所能，让人工智能不被少数人操控，从而能将未来引向好的一面。但是将人工智能技术开放之后，你是否担心它也会被用于邪恶的目的？是的，这是一个潜在的问题。所以关键在于，人工智能的能力应该被广泛地分布，例如，让每个人拥有一个人工智能助理。这样，即使有少数个体想要利用人工智能制造麻烦，其它的人工智能能够共同协作，来对抗邪恶的那一个。但是，如果那一个人工智能比其余的要先进上百万倍的话，当它被操控于邪恶的目的时，我们恐怕就拿它无能为力了。所以这是一个实实在在的风险，而最大化避免它发生的关键，就在于民主化。（观众）这是一个奇怪的问题，但我觉得你能给出正确的回答。如果我们真的创造出全面的人工智能，那么它们可能就是一种模拟人类，而这证明，任何足够先进的文明都能够创造一个像我们人类这样的物种。所以，这是否意味着，我们人类本身可能就是另一种生物的模拟？首先，我想能够支撑我们是某种模拟的最有力的论证是这样的：40 年前，苹果开发出了最初级的电脑游戏 Pong——它的组成部分只有两个长方形和一个球；现在，我们已经有像照片一样逼真的 3D模拟游戏。这些游戏拥有成千上万的玩家，并且它们的技术每年都在不断进步，未来，我们还会有虚拟现实等等。不论你假设技术会以怎样的速度发展，我们最终都会达到一个终极阶段，使得人们难以区分游戏和现实。即使你假设技术发展速度为当前的千分之一，或者说我们一万年后才能达到这一步，这点时间从进化的角度看，都根本不算什么。既然我们难以避免地一定会制作出与现实无二的游戏，而且成千上万的人们可以在任何设备上运行这些游戏，而且可能有数十亿的设备在运行这些游戏——那么这样看来，可能有十亿分之一的概率，我们是在某一个游戏中。那你怎么看？我认为我们是那十亿分之一，因为这是唯一的结论。事实上如果这是真的，那么这是件好事，因为一旦文明停止进化，那么这就可能是因为生成文明的东西出现了问题，（这样我们就有办法重启）。所以，如果我们无法创造出模拟人类世界的软件，那么这意味着文明总有一天会消失。（观众）你认为完全自动化的无人车什么时候可以成为现实？例如，一个不会开车的老人家能够让车把她安全带到机场。同时，你认为什么时候人类驾驶汽车会成为严格限制的行为，甚至会成为违法的？我认为自动驾驶汽车很大程度上可以被视为已经解决的问题了。即使是在北京这样的地方？北京确实是路况很复杂的地方，车流量也非常大。但是，事实上，密集的车流反而是非常易于自动驾驶的，因为你不需要考虑超车等等的问题，只需要和周围车辆保持安全距离就可以了。在没有路障、路况良好的高速公路上，使用自动驾驶汽车也是很容易的事情。我认为，让普通人使用无人工操作的汽车，应该在汽车完全自动化的两年内就能实现。但是，政府可能至少还需要一年来通过，因为决策者们需要看到在长距离、各种路况下的大量试驾结果，来证明这是百分之百安全的。同时，我个人希望人们拥有充分的自由，所以我不支持完全禁止人类驾驶汽车，但是，未来的驾驶执照可能会对驾驶人有更高的技术要求。IBM CEO Ginni Romety IBM CEO Ginni Romety 基于 IBM 长久以来在人工智能领域的耕耘成果，概述了是什么使得公司近乎押注在了技术投入上。她讨论了从卫生保健到教育、从商业 IT 到第三方开发者、从云到企业预置等多种场景中 IBM 努力促进和支持人工智能的使用。最有趣的是那些 IBM 努力提供的开源或免费的解决方案——Rometty 说这些行为时有悖于 IBM 的历史的，但对于公司继续前进却是本质而必要的。当我们许多人认为是 IBM 的 Big Blue 或Jeopardy促成了 Watson 时，IBM 关于人工智能的历史却可以追溯到这个领域最初的时候，那时享誉世界的 Watson 实验室是在翻译、语言处理、演讲和手写识别甚至更多方面拥有最早构想的先驱者。尽管困难重重，Rometty 关于技术的展望以及IBM如此深度的参与程度还是给人们留下了深刻的印象。以下是 Kara Swisher 与 Ginni Romety 对话中有关人工智能的部分。你在人工智能领域已经拥有超过十年的名气了。我想在每个人的一生中，都有那么几个需要作出重要决策的时刻，而我们做出了这样的决定。事实上，我一般不称之为人工智能（尽管它好读又好拼），原因是这样的：在 2005 年，我们意识到当时的信息量和数据量在飞速膨胀，已经远远超出了人脑能够处理的范围，我们认为我们需要做些什么，于是我们开始了在这一领域的研究。我们研究的目的，是做出更好的决策，以及解决悬而未决的问题，这也是为什么我们最先选择了健康医疗领域。这一领域仍旧缺乏一个整体的系统，并且其资金浪费是巨大的，我记得是高达 8 兆美元。沃森在 2011 年成为智力竞猜冠军时引发了巨大的反响，而你事实上很早就开始这方面的工作了。你是如何将沃森的技术商业化的？当我们开发出沃森之后，我想：为什么我们不能把它应用到日常生活中呢？因为我已经确确实实预见到，我们的未来无法避免认知人工智能的普及化。在商业应用方面，我们将其应用到健康医疗、教育、金融服务等等行业。前段时间我参观了我们一个客户的工厂，负责人说，一般他们先人工处理任务，然后再让沃森处理一遍，而往往沃森所发现的问题，是工作人员永远无法发现的。我认为，对于人工智能的未来，我们还需要考虑一些方面。对此人们往往会想到，我们需要处理很多无结构的数据，例如之前在大会上谈过的自然语言处理和图像等等。然而，除此之外，我认为我们还需要考虑专业领域的知识，以及理解因果和进行学习的能力。如果你能在某一领域达到这样的水平，一切就会完全不一样了。如果你是一个生态学者，你会如何观察白板上的所有信息和材料，形成你自己的假设，并对其作出评估？而当专业人士需要决策建议的时候，他们需要的不是一个单一的答案，而是一套比较和分析的过程，包括不同选择的利弊和潜在影响等等。例如，一个癌症病人可能非常不希望自己的头发掉光，因为这对他来说很重要。所以，没有绝对正确的答案，而这就是帮助我们处理灰色地带的工具。Facebook COO Sheryl Sandberg , Facebook CTO Mike Schroepfer 或许我们体验到的对于人工智能最频繁的使用，就是在手机上使用 Facebook 。Facebook COO Sheryl Sandberg 和 CTO MikeSchroepfer 概述了在网站、广告以及虚拟现实等领域关于人工智能的诸多努力。有人认为，在人工智能方面，产品功能以及应用研究深度都未得到广泛认可。比如，几周以前，Facebook 向 ICLR 提交了十几页论文。正如下文所说，我们每个人作为「人」的角色（而非「用户」——根据两位演讲者的说法），为公司提供基于人工智能的功能服务做出了巨大贡献，比如，我们觉得十分有价值的照片标记功能。eBay CEO Devin Wenig尽管花费了大量时间区分自己的公司和亚马逊，eBay CEO Devin Wenig 仍详细描述了 eBay 在提升购物体验以及通过使用人工智能消除欺诈方面令人惊叹的成果。Wenig 描绘了通过使用人工智能的方式，公司将会提供一个高度个性化的 eBay ——一个受过调教的、具有高相关度的 eBay。他声称（来源于 Lauren Goode 的后续访谈，来自 Recode 的 Vox 合作伙伴 The Verge）通过使用人工智能技术，欺诈行为已经成为一个「无意义」的数字。Ford CEO Mark Fields福特 CEO Mark Fields 努力使Code 的听众们相信，尽管福特未必拥有人工智能或云技术方面的固有优势，但他们正积极的寻求合作（比如和Pivotal ）并将需要的技能带入到企业内部，以确保福特汽车参与到这一波技术浪潮中。尽管 Fields 没有深度清晰地解读人工智能的远景，但他使用了关于保养维护和导航方面的实例来阐释公司的投入和决心。Bill Gates以下是与盖茨夫妇对话中有关人工智能的内容。人工智能是当下的热门话题，你认为这是否确实是一项重要的话题？它是否有潜在的危险？我认为人工智能确实是当下一件非常重要的事情，实现人工智能可能是所有计算机工程师的梦想。现在，在语音和视觉方面，我们已经开发出了比人类还要强大的系统，所以过去五年里人工智能的发展达到了前所未有的高峰。在接下来的十年内，我们可能会在现实世界的任务上有更多的尝试，例如开车、做家务等等，最终我们会拥有比人工成本更低的机器人，来完成这些体力任务。（梅琳达）同时，我们十分关注女性在计算机行业的发展。在我从计算机专业毕业的时候，34% 的毕业生都是女性，而现在却下降到 17%。但是，我认为在人工智能方面，我们也十分需要大量的女性工程师的加入，来让我们的产品更多样更全面，特别是在类似于健康护理这样的领域。Cisco CEO Chunk Robbins思科 CEO Chunk Robbins 以类似福特的方式谈及了公司数以千计的软件工程师投入到人工智能的研发中，并将集成人工智能到福特现有的产品线中去。他讲述了通过使用人工智能可以带来更好的管理，并帮助理解将会包含数以百万计的各种类型的末端节点的现代网络。为什么是今年？可以肯定地说，Code 大会之前已经见证过了一些颠覆性的变化，举几个例子：数字媒体、移动设备和智能手机。人工智能正被证明是一种不同类型的改变——一个不是让人们抗拒和担忧会破坏其原有业务的变化，而是一个所有人都在拥抱和朝那个方向发展的变化。一路走来，人们关注的有趣问题将不再是哪家公司使用了人工智能，而是哪家公司以全新的方式最大化地利用了人工智能。这一领域还有大量的发明要做，因为即使这项技术正在快速发展，但总归仍旧处在非常早期的起步阶段。例如，微软在两年前的 Code 大会上给出了预览的 Skype Translator 正在使用最现代的深度学习人工智能技术进行打造。今天人工智能是如此的主流，以至于数十位使用该技术明晰其公司执行规划的 CEO 为该技术的愿景中的四个重要改变做出了直接的贡献。这些技术是怎么以这样一种高科技的方式组合到一起的，使从实验室到实际部署的功能成为可能？我认为这是值得反思的。模型的原始计算能力：尽管我们乐于谈论摩尔定律提供了这么大的推动力，但当涉及到人工智能时，推动力则来自应用于并行架构的摩尔定律，而非英特尔的标量的那种。图形处理单元（GPU）上更多晶体管的应用已经成为了人工智能技术的关键推动力。云架构也是推动力中非常重要的组成部分，因为企业再也不需要在它们自己的 GPU 数据中心上构建就能使用人工智能训练模型的力量了，而且还能按需订购所需的规模。用于训练的海量数据容量：所有人都开始了解到更多数据是训练人工智能模型的唯一方法，而且再多数据也无妨（但却很容易陷入数据过少的境地）。只是直到最近，云架构才在保存和获取训练所需的数据量上变得「普遍」和「经济可行」。Facebook 提供了对此的一个最容易想象的视角：其在每天超过 3 亿张照片（大约半个 PB）的基础上训练对人的识别。IBM 使用放射图像的案例为我们理解存储的演进对人工智能的重要性提供了另一个视角。有标签数据的难以置信的可用性：就像数据对训练很重要一样，没有标签（labels）数据也不会很有用。这是我们在技术的使用中扮演了一个重要部分的地方，就像互联网的开放性一样。我们不只是将照片上传到 Facebook 上，我们还标记我们知道的人；而在这么做时，我们就帮助训练了图像识别引擎。对 eBay 来说，说其希望提供一个个性化的商店是不够的，而还需要我们登录并购买商品来为其个性化引擎提供信息。这是在我们过去思考点击流或猜测某人是否是回头客的方式之上的一种进步。除此之外，我们手机中的传感器还提供了运动和位置数据，从而增强了我们所做的一切。清楚而显然的是，所有这些都存在隐私和安全问题，但与此同时，我们每个人在使用这些服务时所得到的个人好处也都是前所未见的。这种数据可用性超越了我个人所产生（和标记）的数据，还包括了因为开放和基于云的解决方案而可用的数据集和 API（例如，来自政府的经济和人口数据），这些数据集和 API 可以被整合为训练模型的一部分。技术基础的开放实现：人工智能解决方案的兴起中最迷人的方面是：如此之多的核心技术在开放的环境中进行开发（通常由许多企业的研究军团主导），或至少在这项技术演化的相对早期阶段以一种开放的方式提供贡献。谷歌的 TensorFlow、Facebook 的 Torch、IBM 的SystemML 和加州大学伯克利分校的 CAFFE 以及一些用于数据的技术（如 Spark）都是公开可用的平台元素。可以很肯定的是，这遵循了类似 HTML/HTTP 的同样模式，这意味着其经济效益将会从系统的其它地方产生（当然是在数据、训练和模型中）。未来如何？在很短的时间跨度内，人工智能实现了一次飞跃，并且很可能已经越过了幻想破灭的低谷。我对此可是很明确的。尽管未来几年出现的一些成果中毫无疑问会有一些让人失望的东西，但同样毫无疑问的是，这样的怀疑将会通过大量受益于人工智能的书写和通信工具进行交流。没有什么能以每个人喜欢的方式做到每个人想要的每一件事。不需认证，很显然人工智能已经成为了技术领先企业中的一项主流技术；而且在不久的将来，这项技术就将成为几乎每一种领先的产品和服务的组成部分。"
"机器学习;;2016 Code大会精彩回顾:描绘机器智能宏伟蓝图导言：如果你错过了上周的 Code 大会，可能会错过许多。这场技术和媒体盛事汇聚了最顶尖的人物，让他们在同一个平台上畅所欲言。机器学习、人工智能、深度模拟、或如 IBM CEO 所坚称的认知计算，已成为几乎每一位 Code 大会发言者的关注焦点。今年，每个人似乎都没有所隐瞒。Code 大会之前已经见证过了一些颠覆性的变化，举几个例子：数字媒体、移动设备和智能手机。如今，这场盛会也为我们描绘了一幅宏伟蓝图：人工智能技术正以难以置信的速度和深度快速扩展。人工智能已经成为了技术领先企业中的一项主流技术；而且在不久的将来，这项技术就将成为几乎每一种领先的产品和服务的组成部分。当你将顶尖公司领导者、资深科技精英观众、最优秀的面试官这三重身份重合在一起时，会得到什么？答案是 Code 大会。已经是第十三次举办这种独一无二的大会了，大会依旧展示了它一如既往的时效性、启发性和趣味性。尽管大会经历了一系列诸如所有权的更迭、改名、场地变换（包括座位重排）等事件，但 Walt Mossberg 和 Kara Swisher 以及众多支持他们的 Recode 的作者始终在不懈努力着，以期营造一个场所，让我们能够听见（或看见）技术、商业趋势是如何被时下顶尖公司创造、使用或运作的。曾经有许多年代被称为元年（比如 iPod 的出现，甚至Windows 或 Office 的发布），而其他则更多的被关于破坏的争论所淹没（比如网络中立性或音乐发行）。在过去 12 届大会的历程中，或许曾有一些被每个人都提及的主题，但是，我们无法回想起任何一个场景曾在某个技术上汇聚如此一致的前瞻性思考。演讲者们用大量篇幅全面阐述，他们的用户将如何从产品和服务中的智能技术（我们称之为「人工智能」）中受益。为什么这不仅仅是技术发展周期道路上的一个路标？一种简短的回答是，因为作为消费者，我们每天都在手机上「使用」智能。我们必须承认的是，在过去一年中，人工智能已经从理论付诸实践，转变成日常使用。如今对于人工智能的使用不再是纸上谈兵，已成现实。全面解析在聆听演讲者回答关于人工智能在他们各自的企业中将会或正在扮演何种角色的题的过程中，最吸引人的部分就是人工智能技术如何横跨设备、战略以及商业模式。尽管我们每个人可能会对某一个特定案例比较熟悉，演讲者却为我们描绘了一幅宏伟蓝图——人工智能技术正以难以置信的速度和深度快速扩散。Google CEO Sundar Pichai 刚刚结束 Google I/O 大会的 Google CEO Sundar Pichai 带我们领略了人工智能在 Google 的发展史。显然在过去这段时间中，尚未有其他公司在人工智能的钻研和使用上拥有如此的深度和广度。毋庸置疑，人工智能一如既往地定义了谷歌，但直到近几年这一点才被广泛的理解。照片、收件箱、搜索、广告、助手、自动驾驶汽车以及更多项目都被 Pichai 作为例子来阐述 Google 在人工智能方面不间断的投入。如果人工智能本身是一个平台，那么， Google 很有可能是投入最多、最适合成为这个平台领导者的企业。以下是 Walt Mossberg 与 Pichai 对话中有关人工智能的内容：谷歌推出了一系列产品，但其中贯穿一条主题，你给谷歌设定的主题是人工智能，机器学习，这是下一个十年的热点，我们正迈向人工智能，你是怎么看出来的？这是一个很好的总结。我们肯定看到了巨大机遇，看到了拐点。大概三四年前， 我们感觉到了拐点，当时我们将深度学习用于语音识别，还有计算机视觉，都取得了巨大进步。我们花了很长很长时间做这方面的研究，其实很早很早以前，就开始训练我们的算法，在许多特定实例中机器学习真的可以更好地完成很多任务，这都是有原因的。三四年前感觉到拐点后，谷歌内部也经历了一次大的转变，我们开始注重如何将这些技术付诸于外。我们感觉移动端是很大的平台，可以帮助我们实现这个转变。这是一个完全不同的范式。用户一直使用手机，也希望迅速得到帮助。我们就是这样看出拐点的。人工智能方面，微软有 Cortana , FB 有人工智能人工智能, 苹果有siri，亚马逊也有自己的Alexa等，为什么你认为谷歌会比他们做的更好？首先，我们的研究时间更长，看看今天运行规模，无论是其中的计算能力还是我们投入的时间，无论你用什么质量指标给这块市场定基准，比如和谷歌进行各种对话，其回答能力具有内部扩展性，它可以进行 follow on 对话。这些领域展现了谷歌其他公司的真正区别所在。这是我们长时间研究的结果，用户向谷歌问答也很长时间了，语音方面也存在这样的趋势，谷歌的询问体量不逊任何人。无论是使用还是体量规模方面，我们都更好。你是说，你们比微软，苹果等对手要好？你提到的这些公司都是现象级公司。这不是一场《权力游戏》更像是NBA冠军赛。当谈到将机器学习和人工智能以一种有意义的方式带给用户时，还不能自豪地说我们完全做到了，目前我们大家都还处于早期阶段。我们做了各种基准研究，我们大致感觉领先。语音理解方面，比如说，「接通我妻子的电话」，系统最开始要学会谁是我妻子，然后记住。如果你说「接通sweetie」，这个单词语义很多，怎么解决这个问题？理解文本非常难，人类很擅长。计算机很擅长某些事情，不过，情况正在改变，过去两三年，为什么从事这个领域的研究非常让人兴奋，因为我们现在已经开始做一些更加智能的事情，比如理解context,理解所处情景，比如理解谁在问。现在正在解决类似的重要问题。几年前，我拜访过Google Now 的语音团队，还有你，在包装谷歌语音产品方面，你们做的不太好，不像 Siri 和后来的 Cortana，GoogleNow 的语音部分甚至没有自己的名字...部分原因在于，我们认为，这款产品应该因用户不同而有所不同，以某种方式打造专属每个人自己的谷歌。我们思考着如何回答，对用户做出反应，比如一位十岁印尼男孩或者还是你（Walt Mossberg ）这样的身份，我们还不能完全确定什么样的身份可以在所有那些情况中奏效，因此我们希望它用起来有点怪（eccentric），我认为，打造一款能随着时间推移，日益智能化的系统，需要能够理解用户，成为他们的助理，成为朋友，实现这一点，需要时间。我们认为，系统会不断成长和演化，有很多实现的方式，在这么早的阶段，我们还不想把自己局限在一条路上。我们想看它往何处发展。几年前，语音团队告诉我，你们在研究让系统记住对话状态，就像人类对话一样，不用重复对话内容，比如，你问它新西兰有多只羊。谷歌回答说，比如说，4000万只。接着你问，总统是谁？系统会给出新西兰首脑的名字，因为它记得对话领域（domain)。这属于自然语言处理领域。这就是我们可以做多好的例子，比如系统需要理解代词。你说的绵羊的例子只是例子之一，很多时候，你可能谈到电影，第二天说想买电影票，你会希望系统完成这个任务。实现这一点，还需要深入研究。在我看来，智能助力要能真地理解对话，这很难，就像我们接着以前的话题继续聊，实现这个目标，还有很长的路要走，这也是这项研究让人兴奋的地方。人们可以通过很多平台和渠道使用谷歌搜索（不仅仅是安卓系统），有些人认为，其他公司的人工智能助手可能会吞噬谷歌的搜索地盘，你怎么看？在我看来，这就像是 PC 朝移动端的转变。你仍然看到人们在使用 PC，不是取代，而是说 PC 和移动端并存。整体而言，计算在变大，同样，人类对信息的需求也在暴增，因此人们通过很多不同方法去获取信息，我们希望能够帮助他们。这是一个自然演化的过程，前面还有更大的增长，所以，我真的不认为这是一个零和博弈，而是一个重要的拐点。为什么你觉得谷歌的家庭硬件（home hardware）会比亚马逊的 ECHO 更好？下一个五到十年里，真正的对话理解，进行对话，这种技术是我们与对手的区别所在，也是我们一步步计划实现的，硬件不过是这些技术的一个体现。我们想要帮助用户使用这些技术完成工作。很多工作，我们都做在了前面。想想之前的谷歌地图、邮件，都是第一个地图和邮件产品，当然，搜索不是，我们的眼光放得非常长远，也投入了大量金钱和力量。隐私问题，亚马逊的产品在用户家里倾听一切，谷歌产品可能也会给用户这种起鸡皮疙瘩的感觉。人工智能和机器学习可以帮助我们更好地实现隐私控制。Amazon CEO Jeff Bezos 亚马逊 CEO Jeff Bezos 用大量篇幅讲述了人工智能在公司的突破性产品—— Echo 中扮演的角色。从许多方面来说，Echo 已经象征了多重技术的真正潜力——包括语音指令、代理中介、机器学习等，而这些功能都被打包集成在一个极其简单的消费者设备中。此外，Bezos 也概述了 Echo 的基础技术 Alexa 如何既是一个可定制平台又是一个可嵌入技术。开发者可以为 Alexa 构建新的「技能」并使之「学习」以提供新的能力（正如 Mossberg 所说的，Echo 的拥有者每周都会收到邮件，里面详细说明了最新增加的技能）。制造者可以在他们自己的设备中嵌入 Alexa 技术，其中一个被提到的例子是闹钟——这使得一个普通的技术转变为另一个人工智能使能的末端节点。以下是 Walt Mossberg 与 Bezos对话中有关人工智能的内容：亚马逊的家庭助理 Echo 使用 Alexa 作为平台支撑，现在你正逐渐将 Alexa 开放给其他开发者使用，是吗？我们是开放了 Alexa 的语音服务。Alexa 有两个软件开发工具包，一个是 Alexa 语音服务（Alexa Voice Services），你能够通过一套 VPI 将其嵌入到你自己的设备或程序应用中。例如，你可以将它嵌入到你自己制作的闹钟里。另一个开发工具包，是 Alexa 技能包（Alexa Skills Kit），它能让你「教」Alexa 新的技能。目前，其它的大公司也在通过机器学习，加快了在人工智能领域研究的步伐，有些是智能机器人，有些是语音智能。你认为这是继智能手机热潮之后的下一个市场热点吗？是的。我认为再怎么强调人工智能/机器学习在下一个20 年对社会的影响都不为过。但这不意味着智能手机会从人们的视线中消失，智能语音系统并不会取代手机屏幕的地位。只要我们还有眼睛，人们就需要屏幕，并在电子屏幕上进行相关操作。在语音智能这方面，很早的科幻小说里就有各种能够与你自然交谈的智能电脑。我认为，对新的更好的算法的有效结合、计算能力的提升、以及对大型训练数据的使用，是我们能够在机器学习上取得显著进步的主要原因，它们也将为市场提供更多智能产品。那么亚马逊也会逐渐转向以人工智能为重心吗？必须的。这四年来我们一直在背后努力着，光在 Echo 和 Alexa 之后，就有超过一千人的团队在进行研究和维护。但这仅仅是一个开始。我十分激动，因为我认为我们即将踏入一个新的「黄金时代」。不仅仅是大公司，还会有许许多多的创业公司出现，以及许多我们难以预见的新技术。现在大公司更有优势，是因为大公司拥有更大的数据库；但是现在我们所训练的算法，与人类实际上思考的方式是有很大差别的。人脑在使用数据上是极其有效率的：我们无须那么多的数据，就可以总结出复杂事物的规律。同时我们在使用能量上也非常有效率。谈到大数据，隐私保护一直是人们关心的话题，目前也有许多争论进行着，尤其是往往我们收集人们日常活动的信息，仅仅是为了更有针对性地投放广告。对于隐私的问题，你怎么看？从亚马逊的商业操作上来说，我们收集客户的使用数据，是为了向用户推荐他们可能会感兴趣的商品。过去 20年的使用也证明，大部分用户是喜欢看到我们有针对性地推荐商品的。同时，我们在收集和储存数据的时候，会非常明确我们这么做的目的。同时，在必要的情况下，我们会让用户知道，系统是存有他们的数据的。例如，当你再次登录亚马逊的时候，页面会显示「欢迎回来，ＸＸＸ」，这样你知道你在亚马逊上不是匿名的——我们是知道你的名字的。在技术上，我们和苹果使用的是同一种加密协议。但是，即使现在的加密技术已经发展到连开发商本身都无法解密的程度，黑客的攻击也是不可能被根除的。我认为隐私保护问题正是我们这个时代的问题。坏人会越来越先进，那么好人也必须越来越先进。我想我们无法杜绝黑客的入侵，但我们必须不断进步，这就像是一场不停歇的猫追老鼠，我们知道保证永远走在坏人前面就可以了。对于接下来的五年，你有什么新计划吗？我们已经种下了许多种子，接下来我们会继续等待谁会长成参天大树。其中一个我比较看好的项目是Amazon Studios。此外，我们还是会继续发展 Echo, Alexa 和其它自然语言理解项目。事实上，我相信未来出现许多的人工智能代理，就像现在种类繁多的网站和手机应用一样。不同的助理可能会有不同的长处，你不会让一个人工智能完成所有的任务，所以人们可能也会同时使用多个人工智能。对我来说，这是一个非常激动人心的前景，也是我们正在努力的方向。我热爱投身于这样的工作，我们的队伍也特别地酷。（观众）许多大型客机公司都已经在可穿戴设备上投入了许多精力，而亚马逊在这一领域似乎还没有什么动作，对此你是怎么想的？我认为这是一个非常有趣的市场，而同时这也是一个仍处于初步发展阶段的市场，所以，未来会有很多的可能性，我们也会看到许多非常有趣的产品。Elon Musk马斯克分享了一个称作「神经蕾丝（neural lace）」的实验想法，他认为，这或许可以降低人类沦为超智能生物的宠物的风险。「这个似乎是最棒的解决方案，即植入一个AI层，」他说道。「所以想想吧，如果你有自己的表皮系统，皮下组织，以及一个数字层——他们运作良好并与你合而为一。就像你的皮下组织与你的表皮系统共生一样，这个数字层将会与你身体的其他部分共生。」简而言之，这个神经层将会增强我们的输入输出能力，或者处理与交换信息的能力。「我们就是半机械人了，」马斯克说，重复着关于类似手机的设备及诸如因特网的系统实时赋予我们超能力的论点。「但是这种约束就在于输入输出，」他继续道。「我们受到输入输出限制——尤其是输出限制。」尽管我们能够感知并整合巨大的输入信息，我们不能大量炮制同一纬度的信息。我们只能更快的打字和说话。而在神经蕾丝的世界，理论上，允许我们用数字方式连接和交流，以克服生理上的缺陷。马斯克回避了神经蕾丝是通过外科手术亦或种族繁殖的方式植入人体的问题，但是他也暗示了对皮质神经元采用直接接口的必要性，以及这可能会通过连接着神经元的静脉和动脉来实现。马斯克没有提及他已经在研究神经蕾丝，也没有坚持说工作正在进行。「有人会去做的，」他说。「如果没人做，我觉得我就会去做。」另外，虽然没有透露公司名称，但是马斯克承认的确有一家人工智能公司让他担忧。以下是与 Musk 对话中有关人工智能的部分：许多科技行业的人们都相信，人工智能带给人类的未来是光明的，它们能让我们的生活变得更好。在这方面，你似乎有不同的观点，可以详细解释一下吗？人工智能可能带来的未来有很多可能性，我比较关心的是其中不那么乐观的结果。如果我们真的能够创造比我们强大很多的人工智能，那么我们最好能够保证它们带来的影响是积极的。我创立了 OpenAI，它事实上是一个非盈利的组织，在管理中我试图保证，我们研究技术不是以开发产品为目标。很多此类公司没有一种急迫感，而我们的工作节奏是很快的，近期也有很多非常棒的人才加入了我们。OpenAI 建立的目标，是让人工智能更加民主化。Lord Acton 第一次提出「权力腐败」这个概念，他曾说过，「自由是由权力的合理分配组成的，而独裁就是权力的完全集中。」如果我们拥有强大的人工智能，我们需要保证它不会被少数人操控。你觉得那样的未来是什么样的？许多人称之为奇点，因为我们难以预见那到底会是什么样的。你可以将所有的可能性以光谱的形式表现出来——有最好的结果，也有最坏的。OpenAI 在努力做的事情就是，尽我们所能，让人工智能不被少数人操控，从而能将未来引向好的一面。但是将人工智能技术开放之后，你是否担心它也会被用于邪恶的目的？是的，这是一个潜在的问题。所以关键在于，人工智能的能力应该被广泛地分布，例如，让每个人拥有一个人工智能助理。这样，即使有少数个体想要利用人工智能制造麻烦，其它的人工智能能够共同协作，来对抗邪恶的那一个。但是，如果那一个人工智能比其余的要先进上百万倍的话，当它被操控于邪恶的目的时，我们恐怕就拿它无能为力了。所以这是一个实实在在的风险，而最大化避免它发生的关键，就在于民主化。（观众）这是一个奇怪的问题，但我觉得你能给出正确的回答。如果我们真的创造出全面的人工智能，那么它们可能就是一种模拟人类，而这证明，任何足够先进的文明都能够创造一个像我们人类这样的物种。所以，这是否意味着，我们人类本身可能就是另一种生物的模拟？首先，我想能够支撑我们是某种模拟的最有力的论证是这样的：40 年前，苹果开发出了最初级的电脑游戏 Pong——它的组成部分只有两个长方形和一个球；现在，我们已经有像照片一样逼真的 3D模拟游戏。这些游戏拥有成千上万的玩家，并且它们的技术每年都在不断进步，未来，我们还会有虚拟现实等等。不论你假设技术会以怎样的速度发展，我们最终都会达到一个终极阶段，使得人们难以区分游戏和现实。即使你假设技术发展速度为当前的千分之一，或者说我们一万年后才能达到这一步，这点时间从进化的角度看，都根本不算什么。既然我们难以避免地一定会制作出与现实无二的游戏，而且成千上万的人们可以在任何设备上运行这些游戏，而且可能有数十亿的设备在运行这些游戏——那么这样看来，可能有十亿分之一的概率，我们是在某一个游戏中。那你怎么看？我认为我们是那十亿分之一，因为这是唯一的结论。事实上如果这是真的，那么这是件好事，因为一旦文明停止进化，那么这就可能是因为生成文明的东西出现了问题，（这样我们就有办法重启）。所以，如果我们无法创造出模拟人类世界的软件，那么这意味着文明总有一天会消失。（观众）你认为完全自动化的无人车什么时候可以成为现实？例如，一个不会开车的老人家能够让车把她安全带到机场。同时，你认为什么时候人类驾驶汽车会成为严格限制的行为，甚至会成为违法的？我认为自动驾驶汽车很大程度上可以被视为已经解决的问题了。即使是在北京这样的地方？北京确实是路况很复杂的地方，车流量也非常大。但是，事实上，密集的车流反而是非常易于自动驾驶的，因为你不需要考虑超车等等的问题，只需要和周围车辆保持安全距离就可以了。在没有路障、路况良好的高速公路上，使用自动驾驶汽车也是很容易的事情。我认为，让普通人使用无人工操作的汽车，应该在汽车完全自动化的两年内就能实现。但是，政府可能至少还需要一年来通过，因为决策者们需要看到在长距离、各种路况下的大量试驾结果，来证明这是百分之百安全的。同时，我个人希望人们拥有充分的自由，所以我不支持完全禁止人类驾驶汽车，但是，未来的驾驶执照可能会对驾驶人有更高的技术要求。IBM CEO Ginni Romety IBM CEO Ginni Romety 基于 IBM 长久以来在人工智能领域的耕耘成果，概述了是什么使得公司近乎押注在了技术投入上。她讨论了从卫生保健到教育、从商业 IT 到第三方开发者、从云到企业预置等多种场景中 IBM 努力促进和支持人工智能的使用。最有趣的是那些 IBM 努力提供的开源或免费的解决方案——Rometty 说这些行为时有悖于 IBM 的历史的，但对于公司继续前进却是本质而必要的。当我们许多人认为是 IBM 的 Big Blue 或Jeopardy促成了 Watson 时，IBM 关于人工智能的历史却可以追溯到这个领域最初的时候，那时享誉世界的 Watson 实验室是在翻译、语言处理、演讲和手写识别甚至更多方面拥有最早构想的先驱者。尽管困难重重，Rometty 关于技术的展望以及IBM如此深度的参与程度还是给人们留下了深刻的印象。以下是 Kara Swisher 与 Ginni Romety 对话中有关人工智能的部分。你在人工智能领域已经拥有超过十年的名气了。我想在每个人的一生中，都有那么几个需要作出重要决策的时刻，而我们做出了这样的决定。事实上，我一般不称之为人工智能（尽管它好读又好拼），原因是这样的：在 2005 年，我们意识到当时的信息量和数据量在飞速膨胀，已经远远超出了人脑能够处理的范围，我们认为我们需要做些什么，于是我们开始了在这一领域的研究。我们研究的目的，是做出更好的决策，以及解决悬而未决的问题，这也是为什么我们最先选择了健康医疗领域。这一领域仍旧缺乏一个整体的系统，并且其资金浪费是巨大的，我记得是高达 8 兆美元。沃森在 2011 年成为智力竞猜冠军时引发了巨大的反响，而你事实上很早就开始这方面的工作了。你是如何将沃森的技术商业化的？当我们开发出沃森之后，我想：为什么我们不能把它应用到日常生活中呢？因为我已经确确实实预见到，我们的未来无法避免认知人工智能的普及化。在商业应用方面，我们将其应用到健康医疗、教育、金融服务等等行业。前段时间我参观了我们一个客户的工厂，负责人说，一般他们先人工处理任务，然后再让沃森处理一遍，而往往沃森所发现的问题，是工作人员永远无法发现的。我认为，对于人工智能的未来，我们还需要考虑一些方面。对此人们往往会想到，我们需要处理很多无结构的数据，例如之前在大会上谈过的自然语言处理和图像等等。然而，除此之外，我认为我们还需要考虑专业领域的知识，以及理解因果和进行学习的能力。如果你能在某一领域达到这样的水平，一切就会完全不一样了。如果你是一个生态学者，你会如何观察白板上的所有信息和材料，形成你自己的假设，并对其作出评估？而当专业人士需要决策建议的时候，他们需要的不是一个单一的答案，而是一套比较和分析的过程，包括不同选择的利弊和潜在影响等等。例如，一个癌症病人可能非常不希望自己的头发掉光，因为这对他来说很重要。所以，没有绝对正确的答案，而这就是帮助我们处理灰色地带的工具。Facebook COO Sheryl Sandberg , Facebook CTO Mike Schroepfer 或许我们体验到的对于人工智能最频繁的使用，就是在手机上使用 Facebook 。Facebook COO Sheryl Sandberg 和 CTO MikeSchroepfer 概述了在网站、广告以及虚拟现实等领域关于人工智能的诸多努力。有人认为，在人工智能方面，产品功能以及应用研究深度都未得到广泛认可。比如，几周以前，Facebook 向 ICLR 提交了十几页论文。正如下文所说，我们每个人作为「人」的角色（而非「用户」——根据两位演讲者的说法），为公司提供基于人工智能的功能服务做出了巨大贡献，比如，我们觉得十分有价值的照片标记功能。eBay CEO Devin Wenig尽管花费了大量时间区分自己的公司和亚马逊，eBay CEO Devin Wenig 仍详细描述了 eBay 在提升购物体验以及通过使用人工智能消除欺诈方面令人惊叹的成果。Wenig 描绘了通过使用人工智能的方式，公司将会提供一个高度个性化的 eBay ——一个受过调教的、具有高相关度的 eBay。他声称（来源于 Lauren Goode 的后续访谈，来自 Recode 的 Vox 合作伙伴 The Verge）通过使用人工智能技术，欺诈行为已经成为一个「无意义」的数字。Ford CEO Mark Fields福特 CEO Mark Fields 努力使Code 的听众们相信，尽管福特未必拥有人工智能或云技术方面的固有优势，但他们正积极的寻求合作（比如和Pivotal ）并将需要的技能带入到企业内部，以确保福特汽车参与到这一波技术浪潮中。尽管 Fields 没有深度清晰地解读人工智能的远景，但他使用了关于保养维护和导航方面的实例来阐释公司的投入和决心。Bill Gates以下是与盖茨夫妇对话中有关人工智能的内容。人工智能是当下的热门话题，你认为这是否确实是一项重要的话题？它是否有潜在的危险？我认为人工智能确实是当下一件非常重要的事情，实现人工智能可能是所有计算机工程师的梦想。现在，在语音和视觉方面，我们已经开发出了比人类还要强大的系统，所以过去五年里人工智能的发展达到了前所未有的高峰。在接下来的十年内，我们可能会在现实世界的任务上有更多的尝试，例如开车、做家务等等，最终我们会拥有比人工成本更低的机器人，来完成这些体力任务。（梅琳达）同时，我们十分关注女性在计算机行业的发展。在我从计算机专业毕业的时候，34% 的毕业生都是女性，而现在却下降到 17%。但是，我认为在人工智能方面，我们也十分需要大量的女性工程师的加入，来让我们的产品更多样更全面，特别是在类似于健康护理这样的领域。Cisco CEO Chunk Robbins思科 CEO Chunk Robbins 以类似福特的方式谈及了公司数以千计的软件工程师投入到人工智能的研发中，并将集成人工智能到福特现有的产品线中去。他讲述了通过使用人工智能可以带来更好的管理，并帮助理解将会包含数以百万计的各种类型的末端节点的现代网络。为什么是今年？可以肯定地说，Code 大会之前已经见证过了一些颠覆性的变化，举几个例子：数字媒体、移动设备和智能手机。人工智能正被证明是一种不同类型的改变——一个不是让人们抗拒和担忧会破坏其原有业务的变化，而是一个所有人都在拥抱和朝那个方向发展的变化。一路走来，人们关注的有趣问题将不再是哪家公司使用了人工智能，而是哪家公司以全新的方式最大化地利用了人工智能。这一领域还有大量的发明要做，因为即使这项技术正在快速发展，但总归仍旧处在非常早期的起步阶段。例如，微软在两年前的 Code 大会上给出了预览的 Skype Translator 正在使用最现代的深度学习人工智能技术进行打造。今天人工智能是如此的主流，以至于数十位使用该技术明晰其公司执行规划的 CEO 为该技术的愿景中的四个重要改变做出了直接的贡献。这些技术是怎么以这样一种高科技的方式组合到一起的，使从实验室到实际部署的功能成为可能？我认为这是值得反思的。模型的原始计算能力：尽管我们乐于谈论摩尔定律提供了这么大的推动力，但当涉及到人工智能时，推动力则来自应用于并行架构的摩尔定律，而非英特尔的标量的那种。图形处理单元（GPU）上更多晶体管的应用已经成为了人工智能技术的关键推动力。云架构也是推动力中非常重要的组成部分，因为企业再也不需要在它们自己的 GPU 数据中心上构建就能使用人工智能训练模型的力量了，而且还能按需订购所需的规模。用于训练的海量数据容量：所有人都开始了解到更多数据是训练人工智能模型的唯一方法，而且再多数据也无妨（但却很容易陷入数据过少的境地）。只是直到最近，云架构才在保存和获取训练所需的数据量上变得「普遍」和「经济可行」。Facebook 提供了对此的一个最容易想象的视角：其在每天超过 3 亿张照片（大约半个 PB）的基础上训练对人的识别。IBM 使用放射图像的案例为我们理解存储的演进对人工智能的重要性提供了另一个视角。有标签数据的难以置信的可用性：就像数据对训练很重要一样，没有标签（labels）数据也不会很有用。这是我们在技术的使用中扮演了一个重要部分的地方，就像互联网的开放性一样。我们不只是将照片上传到 Facebook 上，我们还标记我们知道的人；而在这么做时，我们就帮助训练了图像识别引擎。对 eBay 来说，说其希望提供一个个性化的商店是不够的，而还需要我们登录并购买商品来为其个性化引擎提供信息。这是在我们过去思考点击流或猜测某人是否是回头客的方式之上的一种进步。除此之外，我们手机中的传感器还提供了运动和位置数据，从而增强了我们所做的一切。清楚而显然的是，所有这些都存在隐私和安全问题，但与此同时，我们每个人在使用这些服务时所得到的个人好处也都是前所未见的。这种数据可用性超越了我个人所产生（和标记）的数据，还包括了因为开放和基于云的解决方案而可用的数据集和 API（例如，来自政府的经济和人口数据），这些数据集和 API 可以被整合为训练模型的一部分。技术基础的开放实现：人工智能解决方案的兴起中最迷人的方面是：如此之多的核心技术在开放的环境中进行开发（通常由许多企业的研究军团主导），或至少在这项技术演化的相对早期阶段以一种开放的方式提供贡献。谷歌的 TensorFlow、Facebook 的 Torch、IBM 的SystemML 和加州大学伯克利分校的 CAFFE 以及一些用于数据的技术（如 Spark）都是公开可用的平台元素。可以很肯定的是，这遵循了类似 HTML/HTTP 的同样模式，这意味着其经济效益将会从系统的其它地方产生（当然是在数据、训练和模型中）。未来如何？在很短的时间跨度内，人工智能实现了一次飞跃，并且很可能已经越过了幻想破灭的低谷。我对此可是很明确的。尽管未来几年出现的一些成果中毫无疑问会有一些让人失望的东西，但同样毫无疑问的是，这样的怀疑将会通过大量受益于人工智能的书写和通信工具进行交流。没有什么能以每个人喜欢的方式做到每个人想要的每一件事。不需认证，很显然人工智能已经成为了技术领先企业中的一项主流技术；而且在不久的将来，这项技术就将成为几乎每一种领先的产品和服务的组成部分。"
"入门;;从GPU、TPU到FPGA及其它：一文读懂神经网络硬件平台战局在如今深度学习大爆发的时代，相关的硬件平台也在百花齐放，既有英伟达和谷歌这样的科技巨头，也有地平线机器人和 Graphcore 等创业公司——它们都各自提出了自己的解决方案。近日，多家公司的技术顾问 Matt Hurd 在其博客上发表了一篇全面评点各种神经网络硬件平台的长文，机器之心对本文进行了编译介绍。这是我几周前做的一个传统的 90 年代风格的性别识别神经网络的很好的隐藏节点。一个简单的性别识别器网络中的 90 年代风格的隐藏节点图像我的硕士项目是一种类似级联相关（cascade correlation）的神经网络 Multi-rate Optimising Order Statistic Equaliser（MOOSE：多速率优化顺序统计均衡器），可用于日内的 Bund（国库债券产品）交易。MOOSE 曾经是为获取高速的 LEO 卫星信号（McCaw 的 Teledesic）而设计的一点成果，后来在从 LIFFE 迁移到 DTB 时将目标转向了 Bund。作为一家投资银行的职业交易员，我可以购买很好的工具。我有那时候世界上最快的计算机：一个 IBM MicroChannel dual Pentium Pro 200MHz 处理器外加带有几 MB RAM 的 SCSI。在 1994 年那会儿，将 800,000 个数据点输入到我的 C++ stream/dag 处理器中看上去就像是黑魔法。有限差分方法让我可以做许多 O(1) 增量线性回归这样的运算，以获得 1000 倍的加速。那时候这看起来已经很好了。现在，你的手机都能嘲笑我的大方向。那时候，在神经网络领域有很多研究。倒不是说它有过人的生产力，只是因为有用。读到 Lindsay Fortado 和 Robin Wigglesworth 的 FT 文章《Machine learning set to shake up equity hedge funds》中 Eric Schmidt 关于机器学习和交易的看法，还真有点让人高兴：Eric Schmidt 是谷歌母公司 Alphabet 的执行董事长，他上周对一众对冲基金经理说他相信在 50 年内，所有交易都会有计算机解读数据和市场信号。「我期待出现在交易方面做机器学习的创业公司，看看我描述的这种模式识别能否比数据分析专家的传统线性回归算法做得更好。」他补充说，「我这个行业内的许多人都认为这注定将成为一种新的交易形式。」老朋友 Eric，我在 90 年代早期就已经算是迟到了，你真是有点后知后觉。好吧，现在情况已经不同了。我喜欢思考它，并喜欢将神经网络的这次新复兴称为感知时代（The Age of Perception）。这并不是智能，只是擅长模式而已。它仍然无力应对语言歧义。它还要一些时间才能理解基本的价值和概念，从而形成深刻的金融理解。深度学习既被夸大了，也被低估了。这不是智能，但会有助于帮我们实现智能。一些人将其夸大为将会给我们带来替代人的类人机器的人工智能突破。我们仍然还受困于常识以及用于推理的简单文本中的歧义。我们还有很长的路要走。相对简单的规划算法和启发式方法以及基于神奇的深度学习的视觉、声音、文本、雷达等等的感知能力将会带来深刻的影响，就像是每个人和他们的狗现在理解的那样。所以我叫它「感知时代」。就好像是我们口袋里的超级计算机突然有了眼睛，并且快速适应了真实世界所带来的闪光性致盲。深度学习将会带来巨大的影响，并且将会改变这颗行星上全人类的生活方式。但我们低估了其对我们的危险。不，我们不会和能激起或挑战我们最深刻的思想的深度图灵对话者约会——还不会。这将不可避免地到来，但在可见的未来里还不可见。借助语音、文本和 Watson 那样的数据库的智能代理可以实现非常先进的 Eliza，但不会更先进了。自动化运输、食物生产、建筑、协助家事将会极大地改变人们的生活方式和不动产的价值。除了这些泛泛之谈，本文的目的是收集一些关于芯片的思想见解——它们驱动着当前的神经网络革命。其中很多见解都不是最为激动人心的，但这对我来说是一个有用的锻炼。神经网络硬件与 20 年前相比，今天的神经处理方法并没有很大不同。深度更多的是一个品牌，而不是一项差异。激活函数已经得到了简化，以更好地适配硬件。主要的成功之处在于我们有了更多数据，对如何初始化权重、处理许多层、并行化和提升鲁棒性也有了更好的理解，其中要用到像是 dropout 这样的技术。1980 年的 Neocognitron 架构与今天的深度学习器或 CNN 并没有显著差异，但 Yann LeCun 让它具备了学习能力。在 90 年代那会儿也有很多神经硬件平台，比如 CNAPS（1990），它带有 64 个处理单元和 256kB 内存，可以在 8/16 位条件下达到 1.6 GCPS 的速度（CPS 是指每秒连接次数/ connections per second）或在 1 位条件下达到 12.8 GCPS 的速度。你可以在《神经硬件概述（Overview of neural hardware）》[Heemskerk, 1995, draft] 中读到 Synapse-1、CNAPS、SNAP、CNS Connectionist Supercomputer、Hitachi WSI、My-Neupower、LNeuro 1.0、UTAK1、GNU（通用神经单元/General Neural Unit）Implementation、UCL、Mantra 1、Biologically-Inspired Emulator、INPG Architecture、BACHUS 和 ZISC036。阅读地址：https://pdfs.semanticscholar.org/5841/73aa4886f87da4501571957c2b14a8fb9069.pdf好吧，东西还真多，但实际上还排除了软件和加速器板/CPU 组合，比如 ANZA plus、SAIC SIGMA-1、NT6000、Balboa 860 协处理器、Ni1000 识别加速器硬件（英特尔）、IBM NEP、NBC、Neuro Turbo I、Neuro Turbo II、WISARD、Mark II & IV、Sandy/8、GCN（索尼）、Topsi、BSP400（400 微处理器）、DREAM Machine、RAP、COKOS、REMAP、通用并行神经计算机（General Purpose Parallel Neurocomputer）、TI NETSIM 和 GeNet。另外还有一些模拟和混合模拟的实现，包括英特尔的电气式可训练模拟神经网络（801770NX）。你懂我要表达的意思了，那时候的东西还真是多。这在 1994 年迎来了一次爆发：乐观的摩尔定律告诉我们，TeraCPS 即将实现：「未来十年，微电子很可能将继续主导神经网络实现的领域。如果进展和过去进步得一样快，那就意味着神经计算机的性能将会增长大约 2 个数量级。因此，神经计算机将会接近 TeraCPS（10^12 CPS）的性能。由 100 万个节点（每个节点有大约 1000 个输入）组成的网络可以达到大脑的计算速度（100-1000 Hz）。这将能为实验合理巨大的网络提供良好的机会。」由于 Minsky 和 Papert 对隐藏层的不正确简单概括，打击了 Rosenblatt 的感知器梦想并最终导致了他不幸的死亡，神经网络研究遭遇了第一个冬天，研究资金被残酷地撤销了。1995 年，又出现了另一次神经网络冬天，尽管那时候我其实并不知道。作为温水锅里的一只青蛙，我没有注意到正在加热。第二个冬天的主要原因是缺乏激动人心的进展，让人们普遍感到无聊了。到了 2012 年，多亏了 Geoffrey Hinton 的冬季生存技能，多伦多大学基于 AlexNet 开发的 SuperVision 在 ImageNet 处理上实现了极大的提升，第二个神经网络冬天也由此终结了。之后谷歌的 LeNet Inception 模型在 2014 年打破了它的记录。所以据我估计，感知时代始于 2012 年。将它记在你的日历里面吧，五年已经过去了。谷歌在几千台普通机器上进行了出色的并行 CPU 有损更新研究。吴恩达教授和他的朋友们让数十台 GPU 就能完成数千台 CPU 的工作，从而让规模化成为了可能。因此，我们从需要很好的资助的神经处理前景中解放了出来。好吧，或多或少吧，现在最先进的网络有时候需要数千台 GPU 或专用芯片。更多数据和更多处理能力是其中的关键。让我们进入本文的重点，列出一些感知时代大数据之战中的一些关键平台：英伟达的 GPU这一家是很难被战胜的。来自大型视频处理市场的收益驱动着巨大的规模经济。新款英伟达 V100 带有一种新的 Tensor Core 架构，速度可达 15 TFlops（单精度/SP）或 120 TFlops（浮点精度，其中带有 FP16 的乘法和 FP32 的累加或加法，非常适合机器学习）。英伟达在它们的 DGX-1 中装入了 8 块计算卡，速度可达 960 Tensor TFlops.AMD 的 GPU在机器学习领域，AMD 一直是英伟达的追赶者。即将发布的 AMD Radeon Instinct MI25 有希望达到 12.3 TFlops（SP）或 24.6 TFlops（FP16）。如果你把英伟达的 Tensor Core 也算进来，则 AMD 完全无力竞争。英伟达设备的带宽 900GB/s 也是 AMD 484GB/s 的两倍。谷歌的 TPU谷歌原来的 TPU 相比于 GPU 有很大的领先，并帮助 DeepMind 的 AlphaGo 赢得了与李世石的围棋大战。据描述，原来的 700 MHz TPU 有 95 TFlops 的 8 位计算能力或 23 TFlops 的 16 位计算能力，同时功耗仅有 40W。这可比当时的 GPU 快得多，但现在落后于英伟达的 V100；但在单位功耗的计算能力上，TPU 并没落后。据称新的 TPU2 是一款带有 4 块芯片的 TPU 设备，速度可达到 180 TFlops 左右。每块芯片的性能都翻倍了，达到了 45 TFlops 的 16 位计算能力。你可以看到与英伟达 V100 的差距正在变小。你没法买到 TPU 或 TPU2。谷歌正在通过它们的云提供这些 TPU 服务，包含 64 台设备的 TPU pod 速度高达 11.5 PetaFlops。TPU2 上巨大的散热片说明了一些原因，但市场正在发生变化——从单独的设备转向了设备的组合以及将这些组合以云的形式提供。Wave ComputingWave 之父澳洲人 CTO Chris Nicol 博士的成果斐然，领导开发了 Wave 的 Compute Appliance 中的异步数据流处理器（asynchronous data flow processor）。几年前，Metamako 的创始人 Charles Thomas 在加州简单介绍了我和 Chris 认识。他们俩都曾在 NICTA 研究过无时钟异步。这两人都很出色。我不确定 Wave 的设备最早是不是针对机器学习设计的，但在他们的 3RU appliance 上运行 TensorFlow 的速度可以达到 2.9 PetaOPS/s，这实在了不起。Wave 将他们的处理器称为 DPU，一个 appliance 有 16 个 DPU。Wave 使用了他们称之为粗粒度可重构阵列（CGRA：Coarse Grained Reconfigurable Array）的处理元素。我还不清楚 2.9 PetaOPS/s 的速度对应多少位宽。根据他们的白皮书，其 ALU 可以执行 1 位、8 位、16 位和 32 位计算：「算术单元是分区的。它们可以并行执行 8 位运算（完美适用于 DNN 推理）以及 16 位和 32 位运算（或以上的任何组合）。也可以执行一些 64 位运算，而且可以使用软件扩展到任意精度。」关于其 appliance 中的 16 个 DPU，还有一些额外的信息：「Wave Computing DPU 是一种包含了 16384 PE 的 SoC，它们被配置成了一个 32×32 集群的 CGRA。它包含了 4 个 HMC（Hybrid Memory Cube）第二代接口、一个 PCIe 第三代 16 通道接口和一个用于 SoC 资源管理的嵌入式 32 位 RISC 微控制器。这款 Wave DPU 可以在没有主机 CPU 时自动执行。」对于 TensoFlow 指令：「Wave DNN Library 团队为 TensorFlow 等工作流程中所使用的常见 DNN 函数创建了预编译的可重新定位的 kernel。它们可以被组合到 Agent 中并且可以被实例化成机器，以构建大型的张量数据流图和 DNN kernel。」「……一个可与 TensorFlow、CNTK、Caffe 和 MXNet 等机器学习工作流程交互的 session 管理器，可作为用于训练和推理的工作器过程（worker process）。这些工作流程可为工作器过程提供张量的数据流图。在运行时，Wave 的 session 管理器会分析数据流图并将这些软件智能体放置到 DPU 芯片中，然后将它们连接起来以构建数据流图。这些软件智能体会被分配用于输入缓冲和本地存储的全局内存区域。CGRA kernel 的静态本质和分布式的内存架构可以让一个性能模型准确估计智能体的延迟。session 管理器可使用该性能模型来在智能体之间插入 FIFO 缓冲，这有助于 DPU 中通信和计算的重叠。这个可变智能体支持穿过整个图的数据流的软件流程，从而可以进一步增加并发性和性能。这个 session 管理器可以监控数据流图运行时的性能（通过监控卡顿、缓冲区下溢出和/或上溢出），并且可以动态地调节 FIFO 缓冲区的大小以实现吞吐量的最大化。在附加了 DPU 的处理器中，有一个分布式运行时管理系统会在运行时安装和卸载数据流图的一些部分，从而平衡计算和内存的使用量。这种在数据流计算机中的数据流图上的运行时重新配置还是有史以来第一次。」是的，我也觉得非常酷。这个平台的惊人之处是它在架构方面比 FPGA 更加粗粒度，因此灵活性更低，但却很可能表现更好。非常有意思。KnuEdge 的 KnuPath在 2016 年 6 月时我在 Twitter 上谈过 KnuPath。那以后他们的产品页面却失踪了。我不确定他们要把那 1 亿美元如何用到他们的 MIMD 架构上。那时候他们向我这样描述的：每个 ASIC 中有 256 个微型 DSP（即 tDSP）核以及一个 ARM 控制器，适用于 35W 包络中的稀疏矩阵处理。它的性能未知，但他们把自己的芯片与一款当时的英伟达芯片进行了比较，那时候他们说实现了 2.5 倍的性能。我们知道英伟达现在凭借 Tensor 内核已经提速了十倍以上，所以 KnuEdge 还要努力才能跟上节奏。MIMD 或 DSP 方法必须要得到非常好的效果才能在这一领域占据一席之地。时间会给我们答案。英特尔的 NervanaNervana Systems 曾经除了开发他们的 Nervana Engine ASIC，还开发着一种 GPU/软件方法，后来英特尔收购了这家公司。性能比较方面还不清楚。英特尔也在规划通过一个 Knights Crest 项目将其集成到 Phi 平台中。NextPlatform 认为其 2017 年在 28nm 节点上的目标是在某个位宽的运算速度达到 55 TOPS/s。英特尔还安排一个 NervanaCon，将在 12 月份举办，所以那时候我们也许将能看到他们的第一批成果。地平线机器人这家中国创业公司正在研发一种大脑处理单元（BPU：Brain Processing Unit）。余凯博士是正规军出身，他曾是百度深度学习研究院的负责人。今年早些时候，一段 YouTube 视频演示了基于 Arria 10 FPGA 的 BPU 仿真：https://youtu.be/GI9U9lUFaDo。目前关于这一平台的公开消息还很少。EyerissEyeriss 是 MIT 的一个项目，开发出了一款具有出色原始性能表现的 64nm ASIC。在 AlexNet 上，这款芯片的速度大约是英伟达 TK1 的一半。其优势在于借助于其行固定（row stationary）方法，仅需要一个 278mW 的可重新配置加速器就能实现这样中规中矩的性能。赞。Graphcore去年 Graphcore 拿到了 3000 万美元的 A 轮融资，以开发他们的智能处理单元（IPU： Intelligence Processing Unit）。他们的网站还缺少细节，只是给出了一些亮眼的事实，比如多于 14000 个独立的处理器线程和大于 100 倍的内存带宽。根据 NextPlatform 报道的零星信息，其在一块芯片上具有多于 1000 个真实内核，并且采用了定制的互连方式。它的 PCIe 板具有一个 16 个处理器的元件。听起来似乎是数据流。抛开公关的言论，这个团队确实有很强的背景，而且投资者也不并不傻，所以就拭目以待吧。TenstorrentTenstorrent 是加拿大多伦多的一家小创业公司，它宣称在深度学习的效率上实现了一个数量级的提升，和大多数公司一样，还没有什么公开的细节，但该公司入选了 Cognitive 300 榜单。CerebrasCerebras 值得一提，因为它得到了 Benchmark 的支持，而且其创始人是 SeaMicro 的 CEO。它似乎已经融资 2500 万美元了，而且仍然处于隐身模式。ThinciThinci 正在美国萨克拉门托开发视觉处理器，并且在印度也有员工。他们宣称即将推出他们的第一款硅芯片 Thinci-tc500，而且已经开始进行标准评测和赢得客户了。但除了「一切都并行处理」之外，我们所知甚少。KonikuKoniku 的网站正在倒计时，现在还有 20 几天。我已经等不及了。他们没有融到多少钱，而看过它们在福布斯上的这个视频之后（https://goo.gl/VA1PJx），你很可能也不得不相信他们，但你也无法预料究竟会如何。利用生物细胞肯定是不一样的。听起来就像是一个科研项目，但他们这样说：「我们是一个企业。我们不是一个科研项目。」下周将在维也纳的 Pioneers Festival 上发表演讲的 Agabi 这样说，「今天有些需求是硅所不能满足的，而我们可以通过我们的系统提供。」Koniku 提供的核心是所谓的神经元壳（neuron-shell），这家创业公司称其内部可以控制神经元彼此通信的方式，加上一种正在申请专利的电极，就可以在神经元上读写信息。所有这些都可以装在一个 iPad 大小的设备里，他们还希望能在 2018 年之前将其减小到一枚五美分硬币大小。AdaptevaAdapteva 是我最喜欢的一家小技术公司，正如你在之前文章《Adapteva tapes out Epiphany-V：一款 1024 核 64 位 RISC 处理器》：https://goo.gl/6ZH7JP。去年年底时 Andreas Olofsson 拿出了他的 1024 核芯片，我们都等着看它的性能表现。Epiphany-V 有用于深度学习的新指令，我们必须要看看这种带有 64MB 片上内存的更少内存控制器的设计是否具有合适的扩展能力。Andrea 的设计和构建的出色效率可能能让我们真正负担得起这种芯片，所以让我们希望它有良好的表现吧。KnowmKnown 研究的是 Anti-Hebbian and Hebbian（AHaH）可塑性和忆阻器。这里有篇覆盖这一主题的论文《AHaH 计算：从 Metastable Switches 到 Attractors 到机器学》：https://doi.org/10.1371/journal.pone.0085175。这对我来说有点太高级了。简单看了看，我看不出这项技术和胡言乱语的区别，但看起来确实科学味道十足。我需要亲眼看到才能相信。神经忆阻式处理器（neuromemristive processor）的思想是很有趣的。我早上确实需要一个好的流行术语。MythicMythic 的一款电池驱动的神经芯片具有低 50 倍的功耗。目前还看不到太多真正的细节。这款芯片大约纽扣大小，但大多数芯片不都这样吗？「Mythic 的平台能在纽扣大小的芯片上提供桌面 GPU 的性能。」也许这又是一款适合无人机和手机的芯片，很可能被用在手机中，也可能被排除在外。高通手机显然是机器学习硬件的一大用武之地。我们希望能够识别狗的品种、花朵、树叶、癌症痣、翻译标识、理解口语等等。我们口袋里的超级计算机愿意用上它能得到的所有帮助，以便能迈入感知时代。高通一直以来都在鼓捣机器学习，推出了 Zeroth SDK 和 Snapdragon 神经处理引擎（NPE）。这种 NPE 显然在高通所用的 Hexagon DSP 上效果良好。Hexagon DSP 已经远远不止是一个非常广泛的并行平台了，Yann LeCun 已经证实高通和 Facebook 正在合作开发一种更好的方法，参见 Wired 的文章《业界 | 谷歌 TPU 之后还有高通，人工智能芯片竞赛已经展开》：「最近，高通已经开始制造执行神经网络的专用芯片，这条消息来自 LeCun，因为 Facebook 正帮助高通开发机器学习相关技术，所以他对高通的计划很了解；高通技术副总裁 Jeff Gehlhaar 证实了这个项目，他说：『在原型设计和开发方面，我们还有很长的路要走。』」也许我们很快就会看到 Kryo CPU、Adreno GPU、Hexagon DSP 和 Hexagon Vector Extensions 之外的其它东西。对于这一领域的创业公司来说，和高通的机器学习竞争将会艰难无比。Pezy-SC 和 Pezy-SC2这两者是 Pezy 开发的 1024 核和 2048 核处理器。Pezy-SC 1024 核芯片可以驱动 2015 年 Green500 超级计算机榜单的前三名的系统。Pezy-SC2 是现在已经开始提供的后续芯片，我在 6 月份也做过关于它的演讲，但相关细节还很少，不过仍然很吸引人：「PEZY-SC2 HPC Brick：单个机箱中有 32 个带有 64GB DDR4 DIMM（2.1 PetaFLOPS（DP））的 Pezy-SC2 模块卡，速度可达 6.4 Tb/s.」不知道 2048 个 MIMD MIPS Warrior 64 位核的机器能做到什么？在 6 月份的 2017 年 Green500 榜单中，一个英伟达 P100 系统拿到了头把交椅，而排名第 7 的是一个 Pezy-SC2 系统。所以看起来这款芯片还活着，但相关细节却很少。Motoaki Saito（齊藤元章）当然值得一看。Kalray尽管做了很多承诺，但 Kalray 的芯片还没有超过 256 核，我在 2015 年的一篇文章就谈到：https://goo.gl/pxqn7Z。Kalray 宣传自己的产品说是适合嵌入式自动驾驶汽车应用，但我觉得其目前产品架构的形式并不是一种完美的 CNN 平台。Kalray 有一个 Kalray Neural Network（KaNN）软件包并且宣称有比 GPU 更好的效率，在芯片上能实现高达 1 TFlop/s 的速度。随着即将到来的产品更新，Kalray 的神经网络财富可能还会提升，就在这个月 Kalray 完成了新一轮 2600 万美元的融资。他们新的 Coolidge 处理器预计将在 2018 年年中上市，它会带有 80 或 160 个核，另外还有 80 或 160 个为视觉和深度学习优化的协处理器。这在他们的多于 1000 核的方法上有了很大的改变，而我认为这是最明智的。IBM TrueNorthTrueNorth 是 IBM 的神经形态 CMOS ASIC，是与 DARPA 的 StNAPSE 项目一起开发的。这是一种单个芯片设计上的多核处理器网络，具有 4096 个核，每个核模拟 256 个可编程的硅「神经元」，总共就超过了 100 万个神经元。另外，每个神经元还有 256 个可编程的「突触」，信号可以在这些突触之间传递。因此，可编程突触的总是超过了 2.68 亿（2^28）。在基本构建模块方面，它的晶体管数量为 54 亿。因为存储、计算和通信都是在这 4096 个神经突触核中各自处理的，所以 TrueNorth 避开了冯·诺依曼架构的瓶颈，而且能效非常高，功耗为 70 mW，大约是传统微处理器的功率密度的万分之一（来自维基百科）。IBM 之前还在批评脉冲神经网络（spiking neural network）无法适配深度学习，现在 IBM 开发了一种在 TureNorth 上运行 CNN 的新算法。这些神经元不会在每个周期内都放电，脉冲神经网络中的神经元必须逐渐积累它们的电势，然后才能放电……深度学习专家普遍认为脉冲神经网络在深度学习上没有效率——至少和卷积神经网络比起来是这样。Facebook 人工智能研究院主任和深度学习先驱 Yann LeCun 曾经批评过 IBM 的 TureNorth 芯片，因为它主要支持脉冲神经网络…………这种神经形态芯片不会太激动人心，因为它们注重的脉冲神经网络在深度学习领域并不受欢迎。为了让 TrueNorth 芯片很好地契合深度学习，IBM 不得不开发了一种新算法，让卷积神经网络可以在这款神经形态计算硬件上很好地运行。这种组合方法实现了 IBM 所谓的「接近当前最佳」的分类准确度，实验涉及了视觉和语音挑战赛方面的 8 个数据集。在最好的情况下，它们的准确度达到了 65% 到 97%。当只使用一块 TrueNorth 芯片时，它只在这 8 个数据集中的 1 个数据集上超越了当前最佳的准确度。但如果使用多达 8 块芯片，IBM 的研究者可以极大提升这款硬件在深度学习挑战上的准确度。这使 TureNorth 可以在其中 3 个数据集上媲美或超越当前最佳的准确度。这个 TureNorth 测试也做到了每秒处理 1200 到 2600 视频帧。这意味着单个 TureNorth 芯片就能实时检测来自多达 100 个同时工作的相机的数据的模式……（来自 IEEE Spectrum）TrueNorth 的功率效率非常出色，所以非常值得考虑。Brainchip 的脉冲神经元自适应处理器（SNAP：Spiking Neuron Adaptive Processor）SNAP 不能做深度学习，这只是一个源于好奇心的项目，还没有实际落地成 CNN 工程解决方案，至少现在还没有。如果你想探索这条路，IBM 的随机相变神经元（stochastic phase-change neurons）似乎更有意思。苹果的神经引擎（Neural Engine）到底会不会有？彭博社报道称这会是一款次级处理器，但没有什么细节信息。对苹果来说，这不仅是一个重要领域，而且也有助于与高通的竞争。其它1. 寒武纪（Cambricon）——中国科学院在芯片上投入的 140 万美元。它是一个用于神经网络的指令集架构，具有数据级的并行、自定义向量/矩阵指令、片上 scratchpad memory。宣称速度是 x86 CPU 的 91 倍，是 K40M 的 3 倍，且峰值功率仅有 1%，即 1.695W。参阅这两篇论文：Cambricon-X：一个用于稀疏神经网络的加速器：http://cslt.riit.tsinghua.edu.cn/mediawiki/images/f/f1/Cambricon-X.pdfCambricon：一个用于神经网络的指令集架构：http://dl.acm.org/citation.cfm?id=30011792. 前谷歌员工创立的 Groq Inc.，也许是另一种 TPU？3. Aimotive：https://aimotive.com/4. Deep Vision 正在开发用于深度学习的低功耗芯片，也许这两篇创始人的论文能提供一点线索：Convolution Engine: Balancing Efficiency &amp; Flexibility in Specialized Computing [2013]：http://csl.stanford.edu/~christos/publications/2013.convolution.isca.pdfConvolution Engine: Balancing Efficiency and Flexibility in Specialized Computing [2015]：http://csl.stanford.edu/~christos/publications/2015.convolution_engine.cacm.pdf5. DeepScale&nbsp;6. Reduced Energy Microsystems 正在开发用于 CNN 推理的低功耗异步芯片。据 TechCrunch 报道，REM 是 Y Combinator 第一个 ASIC 领域的风险投资。7. Leapmind 也很忙。FPGA微软已经站队 FPGA 了。Wired 这篇文章说得很好：《深度 |《连线》长文揭秘微软 Project Catapult：人工智能时代押注 FPGA》。「Bing 占据着世界上 20% 的桌面搜索市场和 6% 的移动手机市场，在 Bing 上，这个芯片能帮助 Bing 适应新品种的人工智能：深度神经网络。」我对这种方法也有些兴趣。赛灵思和英特尔（收购了 Altera）的 FPGA 是很强大的引擎。赛灵思自然宣称他们的 FPGA 是对 INT8 最好的，他们的一份白皮书里面包含了下面的幻灯片：这两家供应商都很支持使用他们的 FPGA 做机器学习：Xilinx - Acceleration Zone：https://goo.gl/KheG5WIntel FPGA OpenCL（https://goo.gl/S62fMA）和 Solutions（https://goo.gl/zkYyXB）尽管 FPGA 单位功耗的性能是很出色的，但这些供应商的更大型的芯片的售价却长时间以来都高得吓人。赛灵思的 VU9P 在 Avnet 上的售价超过 5 万美元。寻找价格和能力之间的平衡点是 FPGA 的主要难题。FPGA 方法的一大优势是可以用来制定一些非常出色的架构决策。比如如果因为 HBM，你想在板上 DRAM 压缩并且实时地解压，然后希望借此改善你的内存的浮点数据流，如果你足够努力，你能找到解决方案。参阅《用于基于 FGPA 的高性能计算的浮点数值数据流的带宽压缩（Bandwidth Compression of Floating-Point Numerical Data Streams for FPGA-Based High-Performance Computing）》：http://dl.acm.org/citation.cfm?id=3053688。这种动态的架构敏捷性（architectural agility）很困难，几乎无法用其它任何方法实现。架构选择太多可能也是一个问题，但我自己还是挺喜欢这个问题的。这篇论文很不错《使用 TILT 减少 Soft Scalar CPU 和定制硬件之间的性能差距（Reducing the Performance Gap between Soft Scalar CPUs and Custom Hardware with TILT）》：http://dl.acm.org/citation.cfm?id=3079757，其中研究了定制硬件和 FPGA 处理器（带有基于 FPGA 的水平微编码的计算引擎）的性能差距，这让人想起了古老的 DISC 和很多个月前的离散指令集计算机（discrete instruction set computer）。谁是赢家？在这样的竞争中，预测谁是赢家是傻瓜的举动。高通凭借其手机市场的主导地位，可以轻松进入赢家榜单。苹果不管做什么都会成功的。英伟达的 V100 有 Tensor 单元，也基本上赢了。我不确定我能否看到谷歌的 TPU 在无休止的长期硅谷竞赛中幸存下来，尽管目前它的性能很出色。我很喜欢 FPGA 方法，但我也不禁想他们应该以远远更低的价格发布 DNN 版本，这样才不会被大众漠视。英特尔和 AMD 会做自己的协处理器。因为所有主要玩家都参战了，所以其中很多都会支持标准的工具包，比如 TensorFlow，这样我们就不必太在意规格了，关心基准就可以了。在更小型的玩家中，我很喜欢而且支持的是 Adapteva 方法，我认为他们的内存架构可能并不适合 DNN。我希望我错了。Wave Computing 可能是我继 FPGA 之后最喜欢的方法。他们的整个异步数据流方法是非常棒的。REM 似乎也在做类似的事情；但我认为他们可能太迟了。Wave Computing 能否在面对所有竞争对手时继续保持领先吗？也许只要他们的异步 CGRA 有一个本质优势，就可以。尽管我不确定他们是否只要 DNN 上的成功，因为他们的技术有更加广泛的应用能力。神经形态脉冲处理器也许现在暂时可以忽略，但也可以保持关注，因为它们有很大的功耗优势。量子计算反正会让所有这些都稍逊风骚。IBM 的 TrueNorth 可能会是个例外，因为它不仅能执行脉冲网络，还能有效地运行 DNN。&nbsp;原文链接：https://meanderful.blogspot.jp/2017/06/fpgas-and-ai-processors-dnn-and-cnn-for.html​"
"硬件;;从GPU、TPU到FPGA及其它：一文读懂神经网络硬件平台战局在如今深度学习大爆发的时代，相关的硬件平台也在百花齐放，既有英伟达和谷歌这样的科技巨头，也有地平线机器人和 Graphcore 等创业公司——它们都各自提出了自己的解决方案。近日，多家公司的技术顾问 Matt Hurd 在其博客上发表了一篇全面评点各种神经网络硬件平台的长文，机器之心对本文进行了编译介绍。这是我几周前做的一个传统的 90 年代风格的性别识别神经网络的很好的隐藏节点。一个简单的性别识别器网络中的 90 年代风格的隐藏节点图像我的硕士项目是一种类似级联相关（cascade correlation）的神经网络 Multi-rate Optimising Order Statistic Equaliser（MOOSE：多速率优化顺序统计均衡器），可用于日内的 Bund（国库债券产品）交易。MOOSE 曾经是为获取高速的 LEO 卫星信号（McCaw 的 Teledesic）而设计的一点成果，后来在从 LIFFE 迁移到 DTB 时将目标转向了 Bund。作为一家投资银行的职业交易员，我可以购买很好的工具。我有那时候世界上最快的计算机：一个 IBM MicroChannel dual Pentium Pro 200MHz 处理器外加带有几 MB RAM 的 SCSI。在 1994 年那会儿，将 800,000 个数据点输入到我的 C++ stream/dag 处理器中看上去就像是黑魔法。有限差分方法让我可以做许多 O(1) 增量线性回归这样的运算，以获得 1000 倍的加速。那时候这看起来已经很好了。现在，你的手机都能嘲笑我的大方向。那时候，在神经网络领域有很多研究。倒不是说它有过人的生产力，只是因为有用。读到 Lindsay Fortado 和 Robin Wigglesworth 的 FT 文章《Machine learning set to shake up equity hedge funds》中 Eric Schmidt 关于机器学习和交易的看法，还真有点让人高兴：Eric Schmidt 是谷歌母公司 Alphabet 的执行董事长，他上周对一众对冲基金经理说他相信在 50 年内，所有交易都会有计算机解读数据和市场信号。「我期待出现在交易方面做机器学习的创业公司，看看我描述的这种模式识别能否比数据分析专家的传统线性回归算法做得更好。」他补充说，「我这个行业内的许多人都认为这注定将成为一种新的交易形式。」老朋友 Eric，我在 90 年代早期就已经算是迟到了，你真是有点后知后觉。好吧，现在情况已经不同了。我喜欢思考它，并喜欢将神经网络的这次新复兴称为感知时代（The Age of Perception）。这并不是智能，只是擅长模式而已。它仍然无力应对语言歧义。它还要一些时间才能理解基本的价值和概念，从而形成深刻的金融理解。深度学习既被夸大了，也被低估了。这不是智能，但会有助于帮我们实现智能。一些人将其夸大为将会给我们带来替代人的类人机器的人工智能突破。我们仍然还受困于常识以及用于推理的简单文本中的歧义。我们还有很长的路要走。相对简单的规划算法和启发式方法以及基于神奇的深度学习的视觉、声音、文本、雷达等等的感知能力将会带来深刻的影响，就像是每个人和他们的狗现在理解的那样。所以我叫它「感知时代」。就好像是我们口袋里的超级计算机突然有了眼睛，并且快速适应了真实世界所带来的闪光性致盲。深度学习将会带来巨大的影响，并且将会改变这颗行星上全人类的生活方式。但我们低估了其对我们的危险。不，我们不会和能激起或挑战我们最深刻的思想的深度图灵对话者约会——还不会。这将不可避免地到来，但在可见的未来里还不可见。借助语音、文本和 Watson 那样的数据库的智能代理可以实现非常先进的 Eliza，但不会更先进了。自动化运输、食物生产、建筑、协助家事将会极大地改变人们的生活方式和不动产的价值。除了这些泛泛之谈，本文的目的是收集一些关于芯片的思想见解——它们驱动着当前的神经网络革命。其中很多见解都不是最为激动人心的，但这对我来说是一个有用的锻炼。神经网络硬件与 20 年前相比，今天的神经处理方法并没有很大不同。深度更多的是一个品牌，而不是一项差异。激活函数已经得到了简化，以更好地适配硬件。主要的成功之处在于我们有了更多数据，对如何初始化权重、处理许多层、并行化和提升鲁棒性也有了更好的理解，其中要用到像是 dropout 这样的技术。1980 年的 Neocognitron 架构与今天的深度学习器或 CNN 并没有显著差异，但 Yann LeCun 让它具备了学习能力。在 90 年代那会儿也有很多神经硬件平台，比如 CNAPS（1990），它带有 64 个处理单元和 256kB 内存，可以在 8/16 位条件下达到 1.6 GCPS 的速度（CPS 是指每秒连接次数/ connections per second）或在 1 位条件下达到 12.8 GCPS 的速度。你可以在《神经硬件概述（Overview of neural hardware）》[Heemskerk, 1995, draft] 中读到 Synapse-1、CNAPS、SNAP、CNS Connectionist Supercomputer、Hitachi WSI、My-Neupower、LNeuro 1.0、UTAK1、GNU（通用神经单元/General Neural Unit）Implementation、UCL、Mantra 1、Biologically-Inspired Emulator、INPG Architecture、BACHUS 和 ZISC036。阅读地址：https://pdfs.semanticscholar.org/5841/73aa4886f87da4501571957c2b14a8fb9069.pdf好吧，东西还真多，但实际上还排除了软件和加速器板/CPU 组合，比如 ANZA plus、SAIC SIGMA-1、NT6000、Balboa 860 协处理器、Ni1000 识别加速器硬件（英特尔）、IBM NEP、NBC、Neuro Turbo I、Neuro Turbo II、WISARD、Mark II & IV、Sandy/8、GCN（索尼）、Topsi、BSP400（400 微处理器）、DREAM Machine、RAP、COKOS、REMAP、通用并行神经计算机（General Purpose Parallel Neurocomputer）、TI NETSIM 和 GeNet。另外还有一些模拟和混合模拟的实现，包括英特尔的电气式可训练模拟神经网络（801770NX）。你懂我要表达的意思了，那时候的东西还真是多。这在 1994 年迎来了一次爆发：乐观的摩尔定律告诉我们，TeraCPS 即将实现：「未来十年，微电子很可能将继续主导神经网络实现的领域。如果进展和过去进步得一样快，那就意味着神经计算机的性能将会增长大约 2 个数量级。因此，神经计算机将会接近 TeraCPS（10^12 CPS）的性能。由 100 万个节点（每个节点有大约 1000 个输入）组成的网络可以达到大脑的计算速度（100-1000 Hz）。这将能为实验合理巨大的网络提供良好的机会。」由于 Minsky 和 Papert 对隐藏层的不正确简单概括，打击了 Rosenblatt 的感知器梦想并最终导致了他不幸的死亡，神经网络研究遭遇了第一个冬天，研究资金被残酷地撤销了。1995 年，又出现了另一次神经网络冬天，尽管那时候我其实并不知道。作为温水锅里的一只青蛙，我没有注意到正在加热。第二个冬天的主要原因是缺乏激动人心的进展，让人们普遍感到无聊了。到了 2012 年，多亏了 Geoffrey Hinton 的冬季生存技能，多伦多大学基于 AlexNet 开发的 SuperVision 在 ImageNet 处理上实现了极大的提升，第二个神经网络冬天也由此终结了。之后谷歌的 LeNet Inception 模型在 2014 年打破了它的记录。所以据我估计，感知时代始于 2012 年。将它记在你的日历里面吧，五年已经过去了。谷歌在几千台普通机器上进行了出色的并行 CPU 有损更新研究。吴恩达教授和他的朋友们让数十台 GPU 就能完成数千台 CPU 的工作，从而让规模化成为了可能。因此，我们从需要很好的资助的神经处理前景中解放了出来。好吧，或多或少吧，现在最先进的网络有时候需要数千台 GPU 或专用芯片。更多数据和更多处理能力是其中的关键。让我们进入本文的重点，列出一些感知时代大数据之战中的一些关键平台：英伟达的 GPU这一家是很难被战胜的。来自大型视频处理市场的收益驱动着巨大的规模经济。新款英伟达 V100 带有一种新的 Tensor Core 架构，速度可达 15 TFlops（单精度/SP）或 120 TFlops（浮点精度，其中带有 FP16 的乘法和 FP32 的累加或加法，非常适合机器学习）。英伟达在它们的 DGX-1 中装入了 8 块计算卡，速度可达 960 Tensor TFlops.AMD 的 GPU在机器学习领域，AMD 一直是英伟达的追赶者。即将发布的 AMD Radeon Instinct MI25 有希望达到 12.3 TFlops（SP）或 24.6 TFlops（FP16）。如果你把英伟达的 Tensor Core 也算进来，则 AMD 完全无力竞争。英伟达设备的带宽 900GB/s 也是 AMD 484GB/s 的两倍。谷歌的 TPU谷歌原来的 TPU 相比于 GPU 有很大的领先，并帮助 DeepMind 的 AlphaGo 赢得了与李世石的围棋大战。据描述，原来的 700 MHz TPU 有 95 TFlops 的 8 位计算能力或 23 TFlops 的 16 位计算能力，同时功耗仅有 40W。这可比当时的 GPU 快得多，但现在落后于英伟达的 V100；但在单位功耗的计算能力上，TPU 并没落后。据称新的 TPU2 是一款带有 4 块芯片的 TPU 设备，速度可达到 180 TFlops 左右。每块芯片的性能都翻倍了，达到了 45 TFlops 的 16 位计算能力。你可以看到与英伟达 V100 的差距正在变小。你没法买到 TPU 或 TPU2。谷歌正在通过它们的云提供这些 TPU 服务，包含 64 台设备的 TPU pod 速度高达 11.5 PetaFlops。TPU2 上巨大的散热片说明了一些原因，但市场正在发生变化——从单独的设备转向了设备的组合以及将这些组合以云的形式提供。Wave ComputingWave 之父澳洲人 CTO Chris Nicol 博士的成果斐然，领导开发了 Wave 的 Compute Appliance 中的异步数据流处理器（asynchronous data flow processor）。几年前，Metamako 的创始人 Charles Thomas 在加州简单介绍了我和 Chris 认识。他们俩都曾在 NICTA 研究过无时钟异步。这两人都很出色。我不确定 Wave 的设备最早是不是针对机器学习设计的，但在他们的 3RU appliance 上运行 TensorFlow 的速度可以达到 2.9 PetaOPS/s，这实在了不起。Wave 将他们的处理器称为 DPU，一个 appliance 有 16 个 DPU。Wave 使用了他们称之为粗粒度可重构阵列（CGRA：Coarse Grained Reconfigurable Array）的处理元素。我还不清楚 2.9 PetaOPS/s 的速度对应多少位宽。根据他们的白皮书，其 ALU 可以执行 1 位、8 位、16 位和 32 位计算：「算术单元是分区的。它们可以并行执行 8 位运算（完美适用于 DNN 推理）以及 16 位和 32 位运算（或以上的任何组合）。也可以执行一些 64 位运算，而且可以使用软件扩展到任意精度。」关于其 appliance 中的 16 个 DPU，还有一些额外的信息：「Wave Computing DPU 是一种包含了 16384 PE 的 SoC，它们被配置成了一个 32×32 集群的 CGRA。它包含了 4 个 HMC（Hybrid Memory Cube）第二代接口、一个 PCIe 第三代 16 通道接口和一个用于 SoC 资源管理的嵌入式 32 位 RISC 微控制器。这款 Wave DPU 可以在没有主机 CPU 时自动执行。」对于 TensoFlow 指令：「Wave DNN Library 团队为 TensorFlow 等工作流程中所使用的常见 DNN 函数创建了预编译的可重新定位的 kernel。它们可以被组合到 Agent 中并且可以被实例化成机器，以构建大型的张量数据流图和 DNN kernel。」「……一个可与 TensorFlow、CNTK、Caffe 和 MXNet 等机器学习工作流程交互的 session 管理器，可作为用于训练和推理的工作器过程（worker process）。这些工作流程可为工作器过程提供张量的数据流图。在运行时，Wave 的 session 管理器会分析数据流图并将这些软件智能体放置到 DPU 芯片中，然后将它们连接起来以构建数据流图。这些软件智能体会被分配用于输入缓冲和本地存储的全局内存区域。CGRA kernel 的静态本质和分布式的内存架构可以让一个性能模型准确估计智能体的延迟。session 管理器可使用该性能模型来在智能体之间插入 FIFO 缓冲，这有助于 DPU 中通信和计算的重叠。这个可变智能体支持穿过整个图的数据流的软件流程，从而可以进一步增加并发性和性能。这个 session 管理器可以监控数据流图运行时的性能（通过监控卡顿、缓冲区下溢出和/或上溢出），并且可以动态地调节 FIFO 缓冲区的大小以实现吞吐量的最大化。在附加了 DPU 的处理器中，有一个分布式运行时管理系统会在运行时安装和卸载数据流图的一些部分，从而平衡计算和内存的使用量。这种在数据流计算机中的数据流图上的运行时重新配置还是有史以来第一次。」是的，我也觉得非常酷。这个平台的惊人之处是它在架构方面比 FPGA 更加粗粒度，因此灵活性更低，但却很可能表现更好。非常有意思。KnuEdge 的 KnuPath在 2016 年 6 月时我在 Twitter 上谈过 KnuPath。那以后他们的产品页面却失踪了。我不确定他们要把那 1 亿美元如何用到他们的 MIMD 架构上。那时候他们向我这样描述的：每个 ASIC 中有 256 个微型 DSP（即 tDSP）核以及一个 ARM 控制器，适用于 35W 包络中的稀疏矩阵处理。它的性能未知，但他们把自己的芯片与一款当时的英伟达芯片进行了比较，那时候他们说实现了 2.5 倍的性能。我们知道英伟达现在凭借 Tensor 内核已经提速了十倍以上，所以 KnuEdge 还要努力才能跟上节奏。MIMD 或 DSP 方法必须要得到非常好的效果才能在这一领域占据一席之地。时间会给我们答案。英特尔的 NervanaNervana Systems 曾经除了开发他们的 Nervana Engine ASIC，还开发着一种 GPU/软件方法，后来英特尔收购了这家公司。性能比较方面还不清楚。英特尔也在规划通过一个 Knights Crest 项目将其集成到 Phi 平台中。NextPlatform 认为其 2017 年在 28nm 节点上的目标是在某个位宽的运算速度达到 55 TOPS/s。英特尔还安排一个 NervanaCon，将在 12 月份举办，所以那时候我们也许将能看到他们的第一批成果。地平线机器人这家中国创业公司正在研发一种大脑处理单元（BPU：Brain Processing Unit）。余凯博士是正规军出身，他曾是百度深度学习研究院的负责人。今年早些时候，一段 YouTube 视频演示了基于 Arria 10 FPGA 的 BPU 仿真：https://youtu.be/GI9U9lUFaDo。目前关于这一平台的公开消息还很少。EyerissEyeriss 是 MIT 的一个项目，开发出了一款具有出色原始性能表现的 64nm ASIC。在 AlexNet 上，这款芯片的速度大约是英伟达 TK1 的一半。其优势在于借助于其行固定（row stationary）方法，仅需要一个 278mW 的可重新配置加速器就能实现这样中规中矩的性能。赞。Graphcore去年 Graphcore 拿到了 3000 万美元的 A 轮融资，以开发他们的智能处理单元（IPU： Intelligence Processing Unit）。他们的网站还缺少细节，只是给出了一些亮眼的事实，比如多于 14000 个独立的处理器线程和大于 100 倍的内存带宽。根据 NextPlatform 报道的零星信息，其在一块芯片上具有多于 1000 个真实内核，并且采用了定制的互连方式。它的 PCIe 板具有一个 16 个处理器的元件。听起来似乎是数据流。抛开公关的言论，这个团队确实有很强的背景，而且投资者也不并不傻，所以就拭目以待吧。TenstorrentTenstorrent 是加拿大多伦多的一家小创业公司，它宣称在深度学习的效率上实现了一个数量级的提升，和大多数公司一样，还没有什么公开的细节，但该公司入选了 Cognitive 300 榜单。CerebrasCerebras 值得一提，因为它得到了 Benchmark 的支持，而且其创始人是 SeaMicro 的 CEO。它似乎已经融资 2500 万美元了，而且仍然处于隐身模式。ThinciThinci 正在美国萨克拉门托开发视觉处理器，并且在印度也有员工。他们宣称即将推出他们的第一款硅芯片 Thinci-tc500，而且已经开始进行标准评测和赢得客户了。但除了「一切都并行处理」之外，我们所知甚少。KonikuKoniku 的网站正在倒计时，现在还有 20 几天。我已经等不及了。他们没有融到多少钱，而看过它们在福布斯上的这个视频之后（https://goo.gl/VA1PJx），你很可能也不得不相信他们，但你也无法预料究竟会如何。利用生物细胞肯定是不一样的。听起来就像是一个科研项目，但他们这样说：「我们是一个企业。我们不是一个科研项目。」下周将在维也纳的 Pioneers Festival 上发表演讲的 Agabi 这样说，「今天有些需求是硅所不能满足的，而我们可以通过我们的系统提供。」Koniku 提供的核心是所谓的神经元壳（neuron-shell），这家创业公司称其内部可以控制神经元彼此通信的方式，加上一种正在申请专利的电极，就可以在神经元上读写信息。所有这些都可以装在一个 iPad 大小的设备里，他们还希望能在 2018 年之前将其减小到一枚五美分硬币大小。AdaptevaAdapteva 是我最喜欢的一家小技术公司，正如你在之前文章《Adapteva tapes out Epiphany-V：一款 1024 核 64 位 RISC 处理器》：https://goo.gl/6ZH7JP。去年年底时 Andreas Olofsson 拿出了他的 1024 核芯片，我们都等着看它的性能表现。Epiphany-V 有用于深度学习的新指令，我们必须要看看这种带有 64MB 片上内存的更少内存控制器的设计是否具有合适的扩展能力。Andrea 的设计和构建的出色效率可能能让我们真正负担得起这种芯片，所以让我们希望它有良好的表现吧。KnowmKnown 研究的是 Anti-Hebbian and Hebbian（AHaH）可塑性和忆阻器。这里有篇覆盖这一主题的论文《AHaH 计算：从 Metastable Switches 到 Attractors 到机器学》：https://doi.org/10.1371/journal.pone.0085175。这对我来说有点太高级了。简单看了看，我看不出这项技术和胡言乱语的区别，但看起来确实科学味道十足。我需要亲眼看到才能相信。神经忆阻式处理器（neuromemristive processor）的思想是很有趣的。我早上确实需要一个好的流行术语。MythicMythic 的一款电池驱动的神经芯片具有低 50 倍的功耗。目前还看不到太多真正的细节。这款芯片大约纽扣大小，但大多数芯片不都这样吗？「Mythic 的平台能在纽扣大小的芯片上提供桌面 GPU 的性能。」也许这又是一款适合无人机和手机的芯片，很可能被用在手机中，也可能被排除在外。高通手机显然是机器学习硬件的一大用武之地。我们希望能够识别狗的品种、花朵、树叶、癌症痣、翻译标识、理解口语等等。我们口袋里的超级计算机愿意用上它能得到的所有帮助，以便能迈入感知时代。高通一直以来都在鼓捣机器学习，推出了 Zeroth SDK 和 Snapdragon 神经处理引擎（NPE）。这种 NPE 显然在高通所用的 Hexagon DSP 上效果良好。Hexagon DSP 已经远远不止是一个非常广泛的并行平台了，Yann LeCun 已经证实高通和 Facebook 正在合作开发一种更好的方法，参见 Wired 的文章《业界 | 谷歌 TPU 之后还有高通，人工智能芯片竞赛已经展开》：「最近，高通已经开始制造执行神经网络的专用芯片，这条消息来自 LeCun，因为 Facebook 正帮助高通开发机器学习相关技术，所以他对高通的计划很了解；高通技术副总裁 Jeff Gehlhaar 证实了这个项目，他说：『在原型设计和开发方面，我们还有很长的路要走。』」也许我们很快就会看到 Kryo CPU、Adreno GPU、Hexagon DSP 和 Hexagon Vector Extensions 之外的其它东西。对于这一领域的创业公司来说，和高通的机器学习竞争将会艰难无比。Pezy-SC 和 Pezy-SC2这两者是 Pezy 开发的 1024 核和 2048 核处理器。Pezy-SC 1024 核芯片可以驱动 2015 年 Green500 超级计算机榜单的前三名的系统。Pezy-SC2 是现在已经开始提供的后续芯片，我在 6 月份也做过关于它的演讲，但相关细节还很少，不过仍然很吸引人：「PEZY-SC2 HPC Brick：单个机箱中有 32 个带有 64GB DDR4 DIMM（2.1 PetaFLOPS（DP））的 Pezy-SC2 模块卡，速度可达 6.4 Tb/s.」不知道 2048 个 MIMD MIPS Warrior 64 位核的机器能做到什么？在 6 月份的 2017 年 Green500 榜单中，一个英伟达 P100 系统拿到了头把交椅，而排名第 7 的是一个 Pezy-SC2 系统。所以看起来这款芯片还活着，但相关细节却很少。Motoaki Saito（齊藤元章）当然值得一看。Kalray尽管做了很多承诺，但 Kalray 的芯片还没有超过 256 核，我在 2015 年的一篇文章就谈到：https://goo.gl/pxqn7Z。Kalray 宣传自己的产品说是适合嵌入式自动驾驶汽车应用，但我觉得其目前产品架构的形式并不是一种完美的 CNN 平台。Kalray 有一个 Kalray Neural Network（KaNN）软件包并且宣称有比 GPU 更好的效率，在芯片上能实现高达 1 TFlop/s 的速度。随着即将到来的产品更新，Kalray 的神经网络财富可能还会提升，就在这个月 Kalray 完成了新一轮 2600 万美元的融资。他们新的 Coolidge 处理器预计将在 2018 年年中上市，它会带有 80 或 160 个核，另外还有 80 或 160 个为视觉和深度学习优化的协处理器。这在他们的多于 1000 核的方法上有了很大的改变，而我认为这是最明智的。IBM TrueNorthTrueNorth 是 IBM 的神经形态 CMOS ASIC，是与 DARPA 的 StNAPSE 项目一起开发的。这是一种单个芯片设计上的多核处理器网络，具有 4096 个核，每个核模拟 256 个可编程的硅「神经元」，总共就超过了 100 万个神经元。另外，每个神经元还有 256 个可编程的「突触」，信号可以在这些突触之间传递。因此，可编程突触的总是超过了 2.68 亿（2^28）。在基本构建模块方面，它的晶体管数量为 54 亿。因为存储、计算和通信都是在这 4096 个神经突触核中各自处理的，所以 TrueNorth 避开了冯·诺依曼架构的瓶颈，而且能效非常高，功耗为 70 mW，大约是传统微处理器的功率密度的万分之一（来自维基百科）。IBM 之前还在批评脉冲神经网络（spiking neural network）无法适配深度学习，现在 IBM 开发了一种在 TureNorth 上运行 CNN 的新算法。这些神经元不会在每个周期内都放电，脉冲神经网络中的神经元必须逐渐积累它们的电势，然后才能放电……深度学习专家普遍认为脉冲神经网络在深度学习上没有效率——至少和卷积神经网络比起来是这样。Facebook 人工智能研究院主任和深度学习先驱 Yann LeCun 曾经批评过 IBM 的 TureNorth 芯片，因为它主要支持脉冲神经网络…………这种神经形态芯片不会太激动人心，因为它们注重的脉冲神经网络在深度学习领域并不受欢迎。为了让 TrueNorth 芯片很好地契合深度学习，IBM 不得不开发了一种新算法，让卷积神经网络可以在这款神经形态计算硬件上很好地运行。这种组合方法实现了 IBM 所谓的「接近当前最佳」的分类准确度，实验涉及了视觉和语音挑战赛方面的 8 个数据集。在最好的情况下，它们的准确度达到了 65% 到 97%。当只使用一块 TrueNorth 芯片时，它只在这 8 个数据集中的 1 个数据集上超越了当前最佳的准确度。但如果使用多达 8 块芯片，IBM 的研究者可以极大提升这款硬件在深度学习挑战上的准确度。这使 TureNorth 可以在其中 3 个数据集上媲美或超越当前最佳的准确度。这个 TureNorth 测试也做到了每秒处理 1200 到 2600 视频帧。这意味着单个 TureNorth 芯片就能实时检测来自多达 100 个同时工作的相机的数据的模式……（来自 IEEE Spectrum）TrueNorth 的功率效率非常出色，所以非常值得考虑。Brainchip 的脉冲神经元自适应处理器（SNAP：Spiking Neuron Adaptive Processor）SNAP 不能做深度学习，这只是一个源于好奇心的项目，还没有实际落地成 CNN 工程解决方案，至少现在还没有。如果你想探索这条路，IBM 的随机相变神经元（stochastic phase-change neurons）似乎更有意思。苹果的神经引擎（Neural Engine）到底会不会有？彭博社报道称这会是一款次级处理器，但没有什么细节信息。对苹果来说，这不仅是一个重要领域，而且也有助于与高通的竞争。其它1. 寒武纪（Cambricon）——中国科学院在芯片上投入的 140 万美元。它是一个用于神经网络的指令集架构，具有数据级的并行、自定义向量/矩阵指令、片上 scratchpad memory。宣称速度是 x86 CPU 的 91 倍，是 K40M 的 3 倍，且峰值功率仅有 1%，即 1.695W。参阅这两篇论文：Cambricon-X：一个用于稀疏神经网络的加速器：http://cslt.riit.tsinghua.edu.cn/mediawiki/images/f/f1/Cambricon-X.pdfCambricon：一个用于神经网络的指令集架构：http://dl.acm.org/citation.cfm?id=30011792. 前谷歌员工创立的 Groq Inc.，也许是另一种 TPU？3. Aimotive：https://aimotive.com/4. Deep Vision 正在开发用于深度学习的低功耗芯片，也许这两篇创始人的论文能提供一点线索：Convolution Engine: Balancing Efficiency &amp; Flexibility in Specialized Computing [2013]：http://csl.stanford.edu/~christos/publications/2013.convolution.isca.pdfConvolution Engine: Balancing Efficiency and Flexibility in Specialized Computing [2015]：http://csl.stanford.edu/~christos/publications/2015.convolution_engine.cacm.pdf5. DeepScale&nbsp;6. Reduced Energy Microsystems 正在开发用于 CNN 推理的低功耗异步芯片。据 TechCrunch 报道，REM 是 Y Combinator 第一个 ASIC 领域的风险投资。7. Leapmind 也很忙。FPGA微软已经站队 FPGA 了。Wired 这篇文章说得很好：《深度 |《连线》长文揭秘微软 Project Catapult：人工智能时代押注 FPGA》。「Bing 占据着世界上 20% 的桌面搜索市场和 6% 的移动手机市场，在 Bing 上，这个芯片能帮助 Bing 适应新品种的人工智能：深度神经网络。」我对这种方法也有些兴趣。赛灵思和英特尔（收购了 Altera）的 FPGA 是很强大的引擎。赛灵思自然宣称他们的 FPGA 是对 INT8 最好的，他们的一份白皮书里面包含了下面的幻灯片：这两家供应商都很支持使用他们的 FPGA 做机器学习：Xilinx - Acceleration Zone：https://goo.gl/KheG5WIntel FPGA OpenCL（https://goo.gl/S62fMA）和 Solutions（https://goo.gl/zkYyXB）尽管 FPGA 单位功耗的性能是很出色的，但这些供应商的更大型的芯片的售价却长时间以来都高得吓人。赛灵思的 VU9P 在 Avnet 上的售价超过 5 万美元。寻找价格和能力之间的平衡点是 FPGA 的主要难题。FPGA 方法的一大优势是可以用来制定一些非常出色的架构决策。比如如果因为 HBM，你想在板上 DRAM 压缩并且实时地解压，然后希望借此改善你的内存的浮点数据流，如果你足够努力，你能找到解决方案。参阅《用于基于 FGPA 的高性能计算的浮点数值数据流的带宽压缩（Bandwidth Compression of Floating-Point Numerical Data Streams for FPGA-Based High-Performance Computing）》：http://dl.acm.org/citation.cfm?id=3053688。这种动态的架构敏捷性（architectural agility）很困难，几乎无法用其它任何方法实现。架构选择太多可能也是一个问题，但我自己还是挺喜欢这个问题的。这篇论文很不错《使用 TILT 减少 Soft Scalar CPU 和定制硬件之间的性能差距（Reducing the Performance Gap between Soft Scalar CPUs and Custom Hardware with TILT）》：http://dl.acm.org/citation.cfm?id=3079757，其中研究了定制硬件和 FPGA 处理器（带有基于 FPGA 的水平微编码的计算引擎）的性能差距，这让人想起了古老的 DISC 和很多个月前的离散指令集计算机（discrete instruction set computer）。谁是赢家？在这样的竞争中，预测谁是赢家是傻瓜的举动。高通凭借其手机市场的主导地位，可以轻松进入赢家榜单。苹果不管做什么都会成功的。英伟达的 V100 有 Tensor 单元，也基本上赢了。我不确定我能否看到谷歌的 TPU 在无休止的长期硅谷竞赛中幸存下来，尽管目前它的性能很出色。我很喜欢 FPGA 方法，但我也不禁想他们应该以远远更低的价格发布 DNN 版本，这样才不会被大众漠视。英特尔和 AMD 会做自己的协处理器。因为所有主要玩家都参战了，所以其中很多都会支持标准的工具包，比如 TensorFlow，这样我们就不必太在意规格了，关心基准就可以了。在更小型的玩家中，我很喜欢而且支持的是 Adapteva 方法，我认为他们的内存架构可能并不适合 DNN。我希望我错了。Wave Computing 可能是我继 FPGA 之后最喜欢的方法。他们的整个异步数据流方法是非常棒的。REM 似乎也在做类似的事情；但我认为他们可能太迟了。Wave Computing 能否在面对所有竞争对手时继续保持领先吗？也许只要他们的异步 CGRA 有一个本质优势，就可以。尽管我不确定他们是否只要 DNN 上的成功，因为他们的技术有更加广泛的应用能力。神经形态脉冲处理器也许现在暂时可以忽略，但也可以保持关注，因为它们有很大的功耗优势。量子计算反正会让所有这些都稍逊风骚。IBM 的 TrueNorth 可能会是个例外，因为它不仅能执行脉冲网络，还能有效地运行 DNN。&nbsp;原文链接：https://meanderful.blogspot.jp/2017/06/fpgas-and-ai-processors-dnn-and-cnn-for.html​"
"GPU;;从GPU、TPU到FPGA及其它：一文读懂神经网络硬件平台战局在如今深度学习大爆发的时代，相关的硬件平台也在百花齐放，既有英伟达和谷歌这样的科技巨头，也有地平线机器人和 Graphcore 等创业公司——它们都各自提出了自己的解决方案。近日，多家公司的技术顾问 Matt Hurd 在其博客上发表了一篇全面评点各种神经网络硬件平台的长文，机器之心对本文进行了编译介绍。这是我几周前做的一个传统的 90 年代风格的性别识别神经网络的很好的隐藏节点。一个简单的性别识别器网络中的 90 年代风格的隐藏节点图像我的硕士项目是一种类似级联相关（cascade correlation）的神经网络 Multi-rate Optimising Order Statistic Equaliser（MOOSE：多速率优化顺序统计均衡器），可用于日内的 Bund（国库债券产品）交易。MOOSE 曾经是为获取高速的 LEO 卫星信号（McCaw 的 Teledesic）而设计的一点成果，后来在从 LIFFE 迁移到 DTB 时将目标转向了 Bund。作为一家投资银行的职业交易员，我可以购买很好的工具。我有那时候世界上最快的计算机：一个 IBM MicroChannel dual Pentium Pro 200MHz 处理器外加带有几 MB RAM 的 SCSI。在 1994 年那会儿，将 800,000 个数据点输入到我的 C++ stream/dag 处理器中看上去就像是黑魔法。有限差分方法让我可以做许多 O(1) 增量线性回归这样的运算，以获得 1000 倍的加速。那时候这看起来已经很好了。现在，你的手机都能嘲笑我的大方向。那时候，在神经网络领域有很多研究。倒不是说它有过人的生产力，只是因为有用。读到 Lindsay Fortado 和 Robin Wigglesworth 的 FT 文章《Machine learning set to shake up equity hedge funds》中 Eric Schmidt 关于机器学习和交易的看法，还真有点让人高兴：Eric Schmidt 是谷歌母公司 Alphabet 的执行董事长，他上周对一众对冲基金经理说他相信在 50 年内，所有交易都会有计算机解读数据和市场信号。「我期待出现在交易方面做机器学习的创业公司，看看我描述的这种模式识别能否比数据分析专家的传统线性回归算法做得更好。」他补充说，「我这个行业内的许多人都认为这注定将成为一种新的交易形式。」老朋友 Eric，我在 90 年代早期就已经算是迟到了，你真是有点后知后觉。好吧，现在情况已经不同了。我喜欢思考它，并喜欢将神经网络的这次新复兴称为感知时代（The Age of Perception）。这并不是智能，只是擅长模式而已。它仍然无力应对语言歧义。它还要一些时间才能理解基本的价值和概念，从而形成深刻的金融理解。深度学习既被夸大了，也被低估了。这不是智能，但会有助于帮我们实现智能。一些人将其夸大为将会给我们带来替代人的类人机器的人工智能突破。我们仍然还受困于常识以及用于推理的简单文本中的歧义。我们还有很长的路要走。相对简单的规划算法和启发式方法以及基于神奇的深度学习的视觉、声音、文本、雷达等等的感知能力将会带来深刻的影响，就像是每个人和他们的狗现在理解的那样。所以我叫它「感知时代」。就好像是我们口袋里的超级计算机突然有了眼睛，并且快速适应了真实世界所带来的闪光性致盲。深度学习将会带来巨大的影响，并且将会改变这颗行星上全人类的生活方式。但我们低估了其对我们的危险。不，我们不会和能激起或挑战我们最深刻的思想的深度图灵对话者约会——还不会。这将不可避免地到来，但在可见的未来里还不可见。借助语音、文本和 Watson 那样的数据库的智能代理可以实现非常先进的 Eliza，但不会更先进了。自动化运输、食物生产、建筑、协助家事将会极大地改变人们的生活方式和不动产的价值。除了这些泛泛之谈，本文的目的是收集一些关于芯片的思想见解——它们驱动着当前的神经网络革命。其中很多见解都不是最为激动人心的，但这对我来说是一个有用的锻炼。神经网络硬件与 20 年前相比，今天的神经处理方法并没有很大不同。深度更多的是一个品牌，而不是一项差异。激活函数已经得到了简化，以更好地适配硬件。主要的成功之处在于我们有了更多数据，对如何初始化权重、处理许多层、并行化和提升鲁棒性也有了更好的理解，其中要用到像是 dropout 这样的技术。1980 年的 Neocognitron 架构与今天的深度学习器或 CNN 并没有显著差异，但 Yann LeCun 让它具备了学习能力。在 90 年代那会儿也有很多神经硬件平台，比如 CNAPS（1990），它带有 64 个处理单元和 256kB 内存，可以在 8/16 位条件下达到 1.6 GCPS 的速度（CPS 是指每秒连接次数/ connections per second）或在 1 位条件下达到 12.8 GCPS 的速度。你可以在《神经硬件概述（Overview of neural hardware）》[Heemskerk, 1995, draft] 中读到 Synapse-1、CNAPS、SNAP、CNS Connectionist Supercomputer、Hitachi WSI、My-Neupower、LNeuro 1.0、UTAK1、GNU（通用神经单元/General Neural Unit）Implementation、UCL、Mantra 1、Biologically-Inspired Emulator、INPG Architecture、BACHUS 和 ZISC036。阅读地址：https://pdfs.semanticscholar.org/5841/73aa4886f87da4501571957c2b14a8fb9069.pdf好吧，东西还真多，但实际上还排除了软件和加速器板/CPU 组合，比如 ANZA plus、SAIC SIGMA-1、NT6000、Balboa 860 协处理器、Ni1000 识别加速器硬件（英特尔）、IBM NEP、NBC、Neuro Turbo I、Neuro Turbo II、WISARD、Mark II & IV、Sandy/8、GCN（索尼）、Topsi、BSP400（400 微处理器）、DREAM Machine、RAP、COKOS、REMAP、通用并行神经计算机（General Purpose Parallel Neurocomputer）、TI NETSIM 和 GeNet。另外还有一些模拟和混合模拟的实现，包括英特尔的电气式可训练模拟神经网络（801770NX）。你懂我要表达的意思了，那时候的东西还真是多。这在 1994 年迎来了一次爆发：乐观的摩尔定律告诉我们，TeraCPS 即将实现：「未来十年，微电子很可能将继续主导神经网络实现的领域。如果进展和过去进步得一样快，那就意味着神经计算机的性能将会增长大约 2 个数量级。因此，神经计算机将会接近 TeraCPS（10^12 CPS）的性能。由 100 万个节点（每个节点有大约 1000 个输入）组成的网络可以达到大脑的计算速度（100-1000 Hz）。这将能为实验合理巨大的网络提供良好的机会。」由于 Minsky 和 Papert 对隐藏层的不正确简单概括，打击了 Rosenblatt 的感知器梦想并最终导致了他不幸的死亡，神经网络研究遭遇了第一个冬天，研究资金被残酷地撤销了。1995 年，又出现了另一次神经网络冬天，尽管那时候我其实并不知道。作为温水锅里的一只青蛙，我没有注意到正在加热。第二个冬天的主要原因是缺乏激动人心的进展，让人们普遍感到无聊了。到了 2012 年，多亏了 Geoffrey Hinton 的冬季生存技能，多伦多大学基于 AlexNet 开发的 SuperVision 在 ImageNet 处理上实现了极大的提升，第二个神经网络冬天也由此终结了。之后谷歌的 LeNet Inception 模型在 2014 年打破了它的记录。所以据我估计，感知时代始于 2012 年。将它记在你的日历里面吧，五年已经过去了。谷歌在几千台普通机器上进行了出色的并行 CPU 有损更新研究。吴恩达教授和他的朋友们让数十台 GPU 就能完成数千台 CPU 的工作，从而让规模化成为了可能。因此，我们从需要很好的资助的神经处理前景中解放了出来。好吧，或多或少吧，现在最先进的网络有时候需要数千台 GPU 或专用芯片。更多数据和更多处理能力是其中的关键。让我们进入本文的重点，列出一些感知时代大数据之战中的一些关键平台：英伟达的 GPU这一家是很难被战胜的。来自大型视频处理市场的收益驱动着巨大的规模经济。新款英伟达 V100 带有一种新的 Tensor Core 架构，速度可达 15 TFlops（单精度/SP）或 120 TFlops（浮点精度，其中带有 FP16 的乘法和 FP32 的累加或加法，非常适合机器学习）。英伟达在它们的 DGX-1 中装入了 8 块计算卡，速度可达 960 Tensor TFlops.AMD 的 GPU在机器学习领域，AMD 一直是英伟达的追赶者。即将发布的 AMD Radeon Instinct MI25 有希望达到 12.3 TFlops（SP）或 24.6 TFlops（FP16）。如果你把英伟达的 Tensor Core 也算进来，则 AMD 完全无力竞争。英伟达设备的带宽 900GB/s 也是 AMD 484GB/s 的两倍。谷歌的 TPU谷歌原来的 TPU 相比于 GPU 有很大的领先，并帮助 DeepMind 的 AlphaGo 赢得了与李世石的围棋大战。据描述，原来的 700 MHz TPU 有 95 TFlops 的 8 位计算能力或 23 TFlops 的 16 位计算能力，同时功耗仅有 40W。这可比当时的 GPU 快得多，但现在落后于英伟达的 V100；但在单位功耗的计算能力上，TPU 并没落后。据称新的 TPU2 是一款带有 4 块芯片的 TPU 设备，速度可达到 180 TFlops 左右。每块芯片的性能都翻倍了，达到了 45 TFlops 的 16 位计算能力。你可以看到与英伟达 V100 的差距正在变小。你没法买到 TPU 或 TPU2。谷歌正在通过它们的云提供这些 TPU 服务，包含 64 台设备的 TPU pod 速度高达 11.5 PetaFlops。TPU2 上巨大的散热片说明了一些原因，但市场正在发生变化——从单独的设备转向了设备的组合以及将这些组合以云的形式提供。Wave ComputingWave 之父澳洲人 CTO Chris Nicol 博士的成果斐然，领导开发了 Wave 的 Compute Appliance 中的异步数据流处理器（asynchronous data flow processor）。几年前，Metamako 的创始人 Charles Thomas 在加州简单介绍了我和 Chris 认识。他们俩都曾在 NICTA 研究过无时钟异步。这两人都很出色。我不确定 Wave 的设备最早是不是针对机器学习设计的，但在他们的 3RU appliance 上运行 TensorFlow 的速度可以达到 2.9 PetaOPS/s，这实在了不起。Wave 将他们的处理器称为 DPU，一个 appliance 有 16 个 DPU。Wave 使用了他们称之为粗粒度可重构阵列（CGRA：Coarse Grained Reconfigurable Array）的处理元素。我还不清楚 2.9 PetaOPS/s 的速度对应多少位宽。根据他们的白皮书，其 ALU 可以执行 1 位、8 位、16 位和 32 位计算：「算术单元是分区的。它们可以并行执行 8 位运算（完美适用于 DNN 推理）以及 16 位和 32 位运算（或以上的任何组合）。也可以执行一些 64 位运算，而且可以使用软件扩展到任意精度。」关于其 appliance 中的 16 个 DPU，还有一些额外的信息：「Wave Computing DPU 是一种包含了 16384 PE 的 SoC，它们被配置成了一个 32×32 集群的 CGRA。它包含了 4 个 HMC（Hybrid Memory Cube）第二代接口、一个 PCIe 第三代 16 通道接口和一个用于 SoC 资源管理的嵌入式 32 位 RISC 微控制器。这款 Wave DPU 可以在没有主机 CPU 时自动执行。」对于 TensoFlow 指令：「Wave DNN Library 团队为 TensorFlow 等工作流程中所使用的常见 DNN 函数创建了预编译的可重新定位的 kernel。它们可以被组合到 Agent 中并且可以被实例化成机器，以构建大型的张量数据流图和 DNN kernel。」「……一个可与 TensorFlow、CNTK、Caffe 和 MXNet 等机器学习工作流程交互的 session 管理器，可作为用于训练和推理的工作器过程（worker process）。这些工作流程可为工作器过程提供张量的数据流图。在运行时，Wave 的 session 管理器会分析数据流图并将这些软件智能体放置到 DPU 芯片中，然后将它们连接起来以构建数据流图。这些软件智能体会被分配用于输入缓冲和本地存储的全局内存区域。CGRA kernel 的静态本质和分布式的内存架构可以让一个性能模型准确估计智能体的延迟。session 管理器可使用该性能模型来在智能体之间插入 FIFO 缓冲，这有助于 DPU 中通信和计算的重叠。这个可变智能体支持穿过整个图的数据流的软件流程，从而可以进一步增加并发性和性能。这个 session 管理器可以监控数据流图运行时的性能（通过监控卡顿、缓冲区下溢出和/或上溢出），并且可以动态地调节 FIFO 缓冲区的大小以实现吞吐量的最大化。在附加了 DPU 的处理器中，有一个分布式运行时管理系统会在运行时安装和卸载数据流图的一些部分，从而平衡计算和内存的使用量。这种在数据流计算机中的数据流图上的运行时重新配置还是有史以来第一次。」是的，我也觉得非常酷。这个平台的惊人之处是它在架构方面比 FPGA 更加粗粒度，因此灵活性更低，但却很可能表现更好。非常有意思。KnuEdge 的 KnuPath在 2016 年 6 月时我在 Twitter 上谈过 KnuPath。那以后他们的产品页面却失踪了。我不确定他们要把那 1 亿美元如何用到他们的 MIMD 架构上。那时候他们向我这样描述的：每个 ASIC 中有 256 个微型 DSP（即 tDSP）核以及一个 ARM 控制器，适用于 35W 包络中的稀疏矩阵处理。它的性能未知，但他们把自己的芯片与一款当时的英伟达芯片进行了比较，那时候他们说实现了 2.5 倍的性能。我们知道英伟达现在凭借 Tensor 内核已经提速了十倍以上，所以 KnuEdge 还要努力才能跟上节奏。MIMD 或 DSP 方法必须要得到非常好的效果才能在这一领域占据一席之地。时间会给我们答案。英特尔的 NervanaNervana Systems 曾经除了开发他们的 Nervana Engine ASIC，还开发着一种 GPU/软件方法，后来英特尔收购了这家公司。性能比较方面还不清楚。英特尔也在规划通过一个 Knights Crest 项目将其集成到 Phi 平台中。NextPlatform 认为其 2017 年在 28nm 节点上的目标是在某个位宽的运算速度达到 55 TOPS/s。英特尔还安排一个 NervanaCon，将在 12 月份举办，所以那时候我们也许将能看到他们的第一批成果。地平线机器人这家中国创业公司正在研发一种大脑处理单元（BPU：Brain Processing Unit）。余凯博士是正规军出身，他曾是百度深度学习研究院的负责人。今年早些时候，一段 YouTube 视频演示了基于 Arria 10 FPGA 的 BPU 仿真：https://youtu.be/GI9U9lUFaDo。目前关于这一平台的公开消息还很少。EyerissEyeriss 是 MIT 的一个项目，开发出了一款具有出色原始性能表现的 64nm ASIC。在 AlexNet 上，这款芯片的速度大约是英伟达 TK1 的一半。其优势在于借助于其行固定（row stationary）方法，仅需要一个 278mW 的可重新配置加速器就能实现这样中规中矩的性能。赞。Graphcore去年 Graphcore 拿到了 3000 万美元的 A 轮融资，以开发他们的智能处理单元（IPU： Intelligence Processing Unit）。他们的网站还缺少细节，只是给出了一些亮眼的事实，比如多于 14000 个独立的处理器线程和大于 100 倍的内存带宽。根据 NextPlatform 报道的零星信息，其在一块芯片上具有多于 1000 个真实内核，并且采用了定制的互连方式。它的 PCIe 板具有一个 16 个处理器的元件。听起来似乎是数据流。抛开公关的言论，这个团队确实有很强的背景，而且投资者也不并不傻，所以就拭目以待吧。TenstorrentTenstorrent 是加拿大多伦多的一家小创业公司，它宣称在深度学习的效率上实现了一个数量级的提升，和大多数公司一样，还没有什么公开的细节，但该公司入选了 Cognitive 300 榜单。CerebrasCerebras 值得一提，因为它得到了 Benchmark 的支持，而且其创始人是 SeaMicro 的 CEO。它似乎已经融资 2500 万美元了，而且仍然处于隐身模式。ThinciThinci 正在美国萨克拉门托开发视觉处理器，并且在印度也有员工。他们宣称即将推出他们的第一款硅芯片 Thinci-tc500，而且已经开始进行标准评测和赢得客户了。但除了「一切都并行处理」之外，我们所知甚少。KonikuKoniku 的网站正在倒计时，现在还有 20 几天。我已经等不及了。他们没有融到多少钱，而看过它们在福布斯上的这个视频之后（https://goo.gl/VA1PJx），你很可能也不得不相信他们，但你也无法预料究竟会如何。利用生物细胞肯定是不一样的。听起来就像是一个科研项目，但他们这样说：「我们是一个企业。我们不是一个科研项目。」下周将在维也纳的 Pioneers Festival 上发表演讲的 Agabi 这样说，「今天有些需求是硅所不能满足的，而我们可以通过我们的系统提供。」Koniku 提供的核心是所谓的神经元壳（neuron-shell），这家创业公司称其内部可以控制神经元彼此通信的方式，加上一种正在申请专利的电极，就可以在神经元上读写信息。所有这些都可以装在一个 iPad 大小的设备里，他们还希望能在 2018 年之前将其减小到一枚五美分硬币大小。AdaptevaAdapteva 是我最喜欢的一家小技术公司，正如你在之前文章《Adapteva tapes out Epiphany-V：一款 1024 核 64 位 RISC 处理器》：https://goo.gl/6ZH7JP。去年年底时 Andreas Olofsson 拿出了他的 1024 核芯片，我们都等着看它的性能表现。Epiphany-V 有用于深度学习的新指令，我们必须要看看这种带有 64MB 片上内存的更少内存控制器的设计是否具有合适的扩展能力。Andrea 的设计和构建的出色效率可能能让我们真正负担得起这种芯片，所以让我们希望它有良好的表现吧。KnowmKnown 研究的是 Anti-Hebbian and Hebbian（AHaH）可塑性和忆阻器。这里有篇覆盖这一主题的论文《AHaH 计算：从 Metastable Switches 到 Attractors 到机器学》：https://doi.org/10.1371/journal.pone.0085175。这对我来说有点太高级了。简单看了看，我看不出这项技术和胡言乱语的区别，但看起来确实科学味道十足。我需要亲眼看到才能相信。神经忆阻式处理器（neuromemristive processor）的思想是很有趣的。我早上确实需要一个好的流行术语。MythicMythic 的一款电池驱动的神经芯片具有低 50 倍的功耗。目前还看不到太多真正的细节。这款芯片大约纽扣大小，但大多数芯片不都这样吗？「Mythic 的平台能在纽扣大小的芯片上提供桌面 GPU 的性能。」也许这又是一款适合无人机和手机的芯片，很可能被用在手机中，也可能被排除在外。高通手机显然是机器学习硬件的一大用武之地。我们希望能够识别狗的品种、花朵、树叶、癌症痣、翻译标识、理解口语等等。我们口袋里的超级计算机愿意用上它能得到的所有帮助，以便能迈入感知时代。高通一直以来都在鼓捣机器学习，推出了 Zeroth SDK 和 Snapdragon 神经处理引擎（NPE）。这种 NPE 显然在高通所用的 Hexagon DSP 上效果良好。Hexagon DSP 已经远远不止是一个非常广泛的并行平台了，Yann LeCun 已经证实高通和 Facebook 正在合作开发一种更好的方法，参见 Wired 的文章《业界 | 谷歌 TPU 之后还有高通，人工智能芯片竞赛已经展开》：「最近，高通已经开始制造执行神经网络的专用芯片，这条消息来自 LeCun，因为 Facebook 正帮助高通开发机器学习相关技术，所以他对高通的计划很了解；高通技术副总裁 Jeff Gehlhaar 证实了这个项目，他说：『在原型设计和开发方面，我们还有很长的路要走。』」也许我们很快就会看到 Kryo CPU、Adreno GPU、Hexagon DSP 和 Hexagon Vector Extensions 之外的其它东西。对于这一领域的创业公司来说，和高通的机器学习竞争将会艰难无比。Pezy-SC 和 Pezy-SC2这两者是 Pezy 开发的 1024 核和 2048 核处理器。Pezy-SC 1024 核芯片可以驱动 2015 年 Green500 超级计算机榜单的前三名的系统。Pezy-SC2 是现在已经开始提供的后续芯片，我在 6 月份也做过关于它的演讲，但相关细节还很少，不过仍然很吸引人：「PEZY-SC2 HPC Brick：单个机箱中有 32 个带有 64GB DDR4 DIMM（2.1 PetaFLOPS（DP））的 Pezy-SC2 模块卡，速度可达 6.4 Tb/s.」不知道 2048 个 MIMD MIPS Warrior 64 位核的机器能做到什么？在 6 月份的 2017 年 Green500 榜单中，一个英伟达 P100 系统拿到了头把交椅，而排名第 7 的是一个 Pezy-SC2 系统。所以看起来这款芯片还活着，但相关细节却很少。Motoaki Saito（齊藤元章）当然值得一看。Kalray尽管做了很多承诺，但 Kalray 的芯片还没有超过 256 核，我在 2015 年的一篇文章就谈到：https://goo.gl/pxqn7Z。Kalray 宣传自己的产品说是适合嵌入式自动驾驶汽车应用，但我觉得其目前产品架构的形式并不是一种完美的 CNN 平台。Kalray 有一个 Kalray Neural Network（KaNN）软件包并且宣称有比 GPU 更好的效率，在芯片上能实现高达 1 TFlop/s 的速度。随着即将到来的产品更新，Kalray 的神经网络财富可能还会提升，就在这个月 Kalray 完成了新一轮 2600 万美元的融资。他们新的 Coolidge 处理器预计将在 2018 年年中上市，它会带有 80 或 160 个核，另外还有 80 或 160 个为视觉和深度学习优化的协处理器。这在他们的多于 1000 核的方法上有了很大的改变，而我认为这是最明智的。IBM TrueNorthTrueNorth 是 IBM 的神经形态 CMOS ASIC，是与 DARPA 的 StNAPSE 项目一起开发的。这是一种单个芯片设计上的多核处理器网络，具有 4096 个核，每个核模拟 256 个可编程的硅「神经元」，总共就超过了 100 万个神经元。另外，每个神经元还有 256 个可编程的「突触」，信号可以在这些突触之间传递。因此，可编程突触的总是超过了 2.68 亿（2^28）。在基本构建模块方面，它的晶体管数量为 54 亿。因为存储、计算和通信都是在这 4096 个神经突触核中各自处理的，所以 TrueNorth 避开了冯·诺依曼架构的瓶颈，而且能效非常高，功耗为 70 mW，大约是传统微处理器的功率密度的万分之一（来自维基百科）。IBM 之前还在批评脉冲神经网络（spiking neural network）无法适配深度学习，现在 IBM 开发了一种在 TureNorth 上运行 CNN 的新算法。这些神经元不会在每个周期内都放电，脉冲神经网络中的神经元必须逐渐积累它们的电势，然后才能放电……深度学习专家普遍认为脉冲神经网络在深度学习上没有效率——至少和卷积神经网络比起来是这样。Facebook 人工智能研究院主任和深度学习先驱 Yann LeCun 曾经批评过 IBM 的 TureNorth 芯片，因为它主要支持脉冲神经网络…………这种神经形态芯片不会太激动人心，因为它们注重的脉冲神经网络在深度学习领域并不受欢迎。为了让 TrueNorth 芯片很好地契合深度学习，IBM 不得不开发了一种新算法，让卷积神经网络可以在这款神经形态计算硬件上很好地运行。这种组合方法实现了 IBM 所谓的「接近当前最佳」的分类准确度，实验涉及了视觉和语音挑战赛方面的 8 个数据集。在最好的情况下，它们的准确度达到了 65% 到 97%。当只使用一块 TrueNorth 芯片时，它只在这 8 个数据集中的 1 个数据集上超越了当前最佳的准确度。但如果使用多达 8 块芯片，IBM 的研究者可以极大提升这款硬件在深度学习挑战上的准确度。这使 TureNorth 可以在其中 3 个数据集上媲美或超越当前最佳的准确度。这个 TureNorth 测试也做到了每秒处理 1200 到 2600 视频帧。这意味着单个 TureNorth 芯片就能实时检测来自多达 100 个同时工作的相机的数据的模式……（来自 IEEE Spectrum）TrueNorth 的功率效率非常出色，所以非常值得考虑。Brainchip 的脉冲神经元自适应处理器（SNAP：Spiking Neuron Adaptive Processor）SNAP 不能做深度学习，这只是一个源于好奇心的项目，还没有实际落地成 CNN 工程解决方案，至少现在还没有。如果你想探索这条路，IBM 的随机相变神经元（stochastic phase-change neurons）似乎更有意思。苹果的神经引擎（Neural Engine）到底会不会有？彭博社报道称这会是一款次级处理器，但没有什么细节信息。对苹果来说，这不仅是一个重要领域，而且也有助于与高通的竞争。其它1. 寒武纪（Cambricon）——中国科学院在芯片上投入的 140 万美元。它是一个用于神经网络的指令集架构，具有数据级的并行、自定义向量/矩阵指令、片上 scratchpad memory。宣称速度是 x86 CPU 的 91 倍，是 K40M 的 3 倍，且峰值功率仅有 1%，即 1.695W。参阅这两篇论文：Cambricon-X：一个用于稀疏神经网络的加速器：http://cslt.riit.tsinghua.edu.cn/mediawiki/images/f/f1/Cambricon-X.pdfCambricon：一个用于神经网络的指令集架构：http://dl.acm.org/citation.cfm?id=30011792. 前谷歌员工创立的 Groq Inc.，也许是另一种 TPU？3. Aimotive：https://aimotive.com/4. Deep Vision 正在开发用于深度学习的低功耗芯片，也许这两篇创始人的论文能提供一点线索：Convolution Engine: Balancing Efficiency &amp; Flexibility in Specialized Computing [2013]：http://csl.stanford.edu/~christos/publications/2013.convolution.isca.pdfConvolution Engine: Balancing Efficiency and Flexibility in Specialized Computing [2015]：http://csl.stanford.edu/~christos/publications/2015.convolution_engine.cacm.pdf5. DeepScale&nbsp;6. Reduced Energy Microsystems 正在开发用于 CNN 推理的低功耗异步芯片。据 TechCrunch 报道，REM 是 Y Combinator 第一个 ASIC 领域的风险投资。7. Leapmind 也很忙。FPGA微软已经站队 FPGA 了。Wired 这篇文章说得很好：《深度 |《连线》长文揭秘微软 Project Catapult：人工智能时代押注 FPGA》。「Bing 占据着世界上 20% 的桌面搜索市场和 6% 的移动手机市场，在 Bing 上，这个芯片能帮助 Bing 适应新品种的人工智能：深度神经网络。」我对这种方法也有些兴趣。赛灵思和英特尔（收购了 Altera）的 FPGA 是很强大的引擎。赛灵思自然宣称他们的 FPGA 是对 INT8 最好的，他们的一份白皮书里面包含了下面的幻灯片：这两家供应商都很支持使用他们的 FPGA 做机器学习：Xilinx - Acceleration Zone：https://goo.gl/KheG5WIntel FPGA OpenCL（https://goo.gl/S62fMA）和 Solutions（https://goo.gl/zkYyXB）尽管 FPGA 单位功耗的性能是很出色的，但这些供应商的更大型的芯片的售价却长时间以来都高得吓人。赛灵思的 VU9P 在 Avnet 上的售价超过 5 万美元。寻找价格和能力之间的平衡点是 FPGA 的主要难题。FPGA 方法的一大优势是可以用来制定一些非常出色的架构决策。比如如果因为 HBM，你想在板上 DRAM 压缩并且实时地解压，然后希望借此改善你的内存的浮点数据流，如果你足够努力，你能找到解决方案。参阅《用于基于 FGPA 的高性能计算的浮点数值数据流的带宽压缩（Bandwidth Compression of Floating-Point Numerical Data Streams for FPGA-Based High-Performance Computing）》：http://dl.acm.org/citation.cfm?id=3053688。这种动态的架构敏捷性（architectural agility）很困难，几乎无法用其它任何方法实现。架构选择太多可能也是一个问题，但我自己还是挺喜欢这个问题的。这篇论文很不错《使用 TILT 减少 Soft Scalar CPU 和定制硬件之间的性能差距（Reducing the Performance Gap between Soft Scalar CPUs and Custom Hardware with TILT）》：http://dl.acm.org/citation.cfm?id=3079757，其中研究了定制硬件和 FPGA 处理器（带有基于 FPGA 的水平微编码的计算引擎）的性能差距，这让人想起了古老的 DISC 和很多个月前的离散指令集计算机（discrete instruction set computer）。谁是赢家？在这样的竞争中，预测谁是赢家是傻瓜的举动。高通凭借其手机市场的主导地位，可以轻松进入赢家榜单。苹果不管做什么都会成功的。英伟达的 V100 有 Tensor 单元，也基本上赢了。我不确定我能否看到谷歌的 TPU 在无休止的长期硅谷竞赛中幸存下来，尽管目前它的性能很出色。我很喜欢 FPGA 方法，但我也不禁想他们应该以远远更低的价格发布 DNN 版本，这样才不会被大众漠视。英特尔和 AMD 会做自己的协处理器。因为所有主要玩家都参战了，所以其中很多都会支持标准的工具包，比如 TensorFlow，这样我们就不必太在意规格了，关心基准就可以了。在更小型的玩家中，我很喜欢而且支持的是 Adapteva 方法，我认为他们的内存架构可能并不适合 DNN。我希望我错了。Wave Computing 可能是我继 FPGA 之后最喜欢的方法。他们的整个异步数据流方法是非常棒的。REM 似乎也在做类似的事情；但我认为他们可能太迟了。Wave Computing 能否在面对所有竞争对手时继续保持领先吗？也许只要他们的异步 CGRA 有一个本质优势，就可以。尽管我不确定他们是否只要 DNN 上的成功，因为他们的技术有更加广泛的应用能力。神经形态脉冲处理器也许现在暂时可以忽略，但也可以保持关注，因为它们有很大的功耗优势。量子计算反正会让所有这些都稍逊风骚。IBM 的 TrueNorth 可能会是个例外，因为它不仅能执行脉冲网络，还能有效地运行 DNN。&nbsp;原文链接：https://meanderful.blogspot.jp/2017/06/fpgas-and-ai-processors-dnn-and-cnn-for.html​"
"CPU;;从GPU、TPU到FPGA及其它：一文读懂神经网络硬件平台战局在如今深度学习大爆发的时代，相关的硬件平台也在百花齐放，既有英伟达和谷歌这样的科技巨头，也有地平线机器人和 Graphcore 等创业公司——它们都各自提出了自己的解决方案。近日，多家公司的技术顾问 Matt Hurd 在其博客上发表了一篇全面评点各种神经网络硬件平台的长文，机器之心对本文进行了编译介绍。这是我几周前做的一个传统的 90 年代风格的性别识别神经网络的很好的隐藏节点。一个简单的性别识别器网络中的 90 年代风格的隐藏节点图像我的硕士项目是一种类似级联相关（cascade correlation）的神经网络 Multi-rate Optimising Order Statistic Equaliser（MOOSE：多速率优化顺序统计均衡器），可用于日内的 Bund（国库债券产品）交易。MOOSE 曾经是为获取高速的 LEO 卫星信号（McCaw 的 Teledesic）而设计的一点成果，后来在从 LIFFE 迁移到 DTB 时将目标转向了 Bund。作为一家投资银行的职业交易员，我可以购买很好的工具。我有那时候世界上最快的计算机：一个 IBM MicroChannel dual Pentium Pro 200MHz 处理器外加带有几 MB RAM 的 SCSI。在 1994 年那会儿，将 800,000 个数据点输入到我的 C++ stream/dag 处理器中看上去就像是黑魔法。有限差分方法让我可以做许多 O(1) 增量线性回归这样的运算，以获得 1000 倍的加速。那时候这看起来已经很好了。现在，你的手机都能嘲笑我的大方向。那时候，在神经网络领域有很多研究。倒不是说它有过人的生产力，只是因为有用。读到 Lindsay Fortado 和 Robin Wigglesworth 的 FT 文章《Machine learning set to shake up equity hedge funds》中 Eric Schmidt 关于机器学习和交易的看法，还真有点让人高兴：Eric Schmidt 是谷歌母公司 Alphabet 的执行董事长，他上周对一众对冲基金经理说他相信在 50 年内，所有交易都会有计算机解读数据和市场信号。「我期待出现在交易方面做机器学习的创业公司，看看我描述的这种模式识别能否比数据分析专家的传统线性回归算法做得更好。」他补充说，「我这个行业内的许多人都认为这注定将成为一种新的交易形式。」老朋友 Eric，我在 90 年代早期就已经算是迟到了，你真是有点后知后觉。好吧，现在情况已经不同了。我喜欢思考它，并喜欢将神经网络的这次新复兴称为感知时代（The Age of Perception）。这并不是智能，只是擅长模式而已。它仍然无力应对语言歧义。它还要一些时间才能理解基本的价值和概念，从而形成深刻的金融理解。深度学习既被夸大了，也被低估了。这不是智能，但会有助于帮我们实现智能。一些人将其夸大为将会给我们带来替代人的类人机器的人工智能突破。我们仍然还受困于常识以及用于推理的简单文本中的歧义。我们还有很长的路要走。相对简单的规划算法和启发式方法以及基于神奇的深度学习的视觉、声音、文本、雷达等等的感知能力将会带来深刻的影响，就像是每个人和他们的狗现在理解的那样。所以我叫它「感知时代」。就好像是我们口袋里的超级计算机突然有了眼睛，并且快速适应了真实世界所带来的闪光性致盲。深度学习将会带来巨大的影响，并且将会改变这颗行星上全人类的生活方式。但我们低估了其对我们的危险。不，我们不会和能激起或挑战我们最深刻的思想的深度图灵对话者约会——还不会。这将不可避免地到来，但在可见的未来里还不可见。借助语音、文本和 Watson 那样的数据库的智能代理可以实现非常先进的 Eliza，但不会更先进了。自动化运输、食物生产、建筑、协助家事将会极大地改变人们的生活方式和不动产的价值。除了这些泛泛之谈，本文的目的是收集一些关于芯片的思想见解——它们驱动着当前的神经网络革命。其中很多见解都不是最为激动人心的，但这对我来说是一个有用的锻炼。神经网络硬件与 20 年前相比，今天的神经处理方法并没有很大不同。深度更多的是一个品牌，而不是一项差异。激活函数已经得到了简化，以更好地适配硬件。主要的成功之处在于我们有了更多数据，对如何初始化权重、处理许多层、并行化和提升鲁棒性也有了更好的理解，其中要用到像是 dropout 这样的技术。1980 年的 Neocognitron 架构与今天的深度学习器或 CNN 并没有显著差异，但 Yann LeCun 让它具备了学习能力。在 90 年代那会儿也有很多神经硬件平台，比如 CNAPS（1990），它带有 64 个处理单元和 256kB 内存，可以在 8/16 位条件下达到 1.6 GCPS 的速度（CPS 是指每秒连接次数/ connections per second）或在 1 位条件下达到 12.8 GCPS 的速度。你可以在《神经硬件概述（Overview of neural hardware）》[Heemskerk, 1995, draft] 中读到 Synapse-1、CNAPS、SNAP、CNS Connectionist Supercomputer、Hitachi WSI、My-Neupower、LNeuro 1.0、UTAK1、GNU（通用神经单元/General Neural Unit）Implementation、UCL、Mantra 1、Biologically-Inspired Emulator、INPG Architecture、BACHUS 和 ZISC036。阅读地址：https://pdfs.semanticscholar.org/5841/73aa4886f87da4501571957c2b14a8fb9069.pdf好吧，东西还真多，但实际上还排除了软件和加速器板/CPU 组合，比如 ANZA plus、SAIC SIGMA-1、NT6000、Balboa 860 协处理器、Ni1000 识别加速器硬件（英特尔）、IBM NEP、NBC、Neuro Turbo I、Neuro Turbo II、WISARD、Mark II & IV、Sandy/8、GCN（索尼）、Topsi、BSP400（400 微处理器）、DREAM Machine、RAP、COKOS、REMAP、通用并行神经计算机（General Purpose Parallel Neurocomputer）、TI NETSIM 和 GeNet。另外还有一些模拟和混合模拟的实现，包括英特尔的电气式可训练模拟神经网络（801770NX）。你懂我要表达的意思了，那时候的东西还真是多。这在 1994 年迎来了一次爆发：乐观的摩尔定律告诉我们，TeraCPS 即将实现：「未来十年，微电子很可能将继续主导神经网络实现的领域。如果进展和过去进步得一样快，那就意味着神经计算机的性能将会增长大约 2 个数量级。因此，神经计算机将会接近 TeraCPS（10^12 CPS）的性能。由 100 万个节点（每个节点有大约 1000 个输入）组成的网络可以达到大脑的计算速度（100-1000 Hz）。这将能为实验合理巨大的网络提供良好的机会。」由于 Minsky 和 Papert 对隐藏层的不正确简单概括，打击了 Rosenblatt 的感知器梦想并最终导致了他不幸的死亡，神经网络研究遭遇了第一个冬天，研究资金被残酷地撤销了。1995 年，又出现了另一次神经网络冬天，尽管那时候我其实并不知道。作为温水锅里的一只青蛙，我没有注意到正在加热。第二个冬天的主要原因是缺乏激动人心的进展，让人们普遍感到无聊了。到了 2012 年，多亏了 Geoffrey Hinton 的冬季生存技能，多伦多大学基于 AlexNet 开发的 SuperVision 在 ImageNet 处理上实现了极大的提升，第二个神经网络冬天也由此终结了。之后谷歌的 LeNet Inception 模型在 2014 年打破了它的记录。所以据我估计，感知时代始于 2012 年。将它记在你的日历里面吧，五年已经过去了。谷歌在几千台普通机器上进行了出色的并行 CPU 有损更新研究。吴恩达教授和他的朋友们让数十台 GPU 就能完成数千台 CPU 的工作，从而让规模化成为了可能。因此，我们从需要很好的资助的神经处理前景中解放了出来。好吧，或多或少吧，现在最先进的网络有时候需要数千台 GPU 或专用芯片。更多数据和更多处理能力是其中的关键。让我们进入本文的重点，列出一些感知时代大数据之战中的一些关键平台：英伟达的 GPU这一家是很难被战胜的。来自大型视频处理市场的收益驱动着巨大的规模经济。新款英伟达 V100 带有一种新的 Tensor Core 架构，速度可达 15 TFlops（单精度/SP）或 120 TFlops（浮点精度，其中带有 FP16 的乘法和 FP32 的累加或加法，非常适合机器学习）。英伟达在它们的 DGX-1 中装入了 8 块计算卡，速度可达 960 Tensor TFlops.AMD 的 GPU在机器学习领域，AMD 一直是英伟达的追赶者。即将发布的 AMD Radeon Instinct MI25 有希望达到 12.3 TFlops（SP）或 24.6 TFlops（FP16）。如果你把英伟达的 Tensor Core 也算进来，则 AMD 完全无力竞争。英伟达设备的带宽 900GB/s 也是 AMD 484GB/s 的两倍。谷歌的 TPU谷歌原来的 TPU 相比于 GPU 有很大的领先，并帮助 DeepMind 的 AlphaGo 赢得了与李世石的围棋大战。据描述，原来的 700 MHz TPU 有 95 TFlops 的 8 位计算能力或 23 TFlops 的 16 位计算能力，同时功耗仅有 40W。这可比当时的 GPU 快得多，但现在落后于英伟达的 V100；但在单位功耗的计算能力上，TPU 并没落后。据称新的 TPU2 是一款带有 4 块芯片的 TPU 设备，速度可达到 180 TFlops 左右。每块芯片的性能都翻倍了，达到了 45 TFlops 的 16 位计算能力。你可以看到与英伟达 V100 的差距正在变小。你没法买到 TPU 或 TPU2。谷歌正在通过它们的云提供这些 TPU 服务，包含 64 台设备的 TPU pod 速度高达 11.5 PetaFlops。TPU2 上巨大的散热片说明了一些原因，但市场正在发生变化——从单独的设备转向了设备的组合以及将这些组合以云的形式提供。Wave ComputingWave 之父澳洲人 CTO Chris Nicol 博士的成果斐然，领导开发了 Wave 的 Compute Appliance 中的异步数据流处理器（asynchronous data flow processor）。几年前，Metamako 的创始人 Charles Thomas 在加州简单介绍了我和 Chris 认识。他们俩都曾在 NICTA 研究过无时钟异步。这两人都很出色。我不确定 Wave 的设备最早是不是针对机器学习设计的，但在他们的 3RU appliance 上运行 TensorFlow 的速度可以达到 2.9 PetaOPS/s，这实在了不起。Wave 将他们的处理器称为 DPU，一个 appliance 有 16 个 DPU。Wave 使用了他们称之为粗粒度可重构阵列（CGRA：Coarse Grained Reconfigurable Array）的处理元素。我还不清楚 2.9 PetaOPS/s 的速度对应多少位宽。根据他们的白皮书，其 ALU 可以执行 1 位、8 位、16 位和 32 位计算：「算术单元是分区的。它们可以并行执行 8 位运算（完美适用于 DNN 推理）以及 16 位和 32 位运算（或以上的任何组合）。也可以执行一些 64 位运算，而且可以使用软件扩展到任意精度。」关于其 appliance 中的 16 个 DPU，还有一些额外的信息：「Wave Computing DPU 是一种包含了 16384 PE 的 SoC，它们被配置成了一个 32×32 集群的 CGRA。它包含了 4 个 HMC（Hybrid Memory Cube）第二代接口、一个 PCIe 第三代 16 通道接口和一个用于 SoC 资源管理的嵌入式 32 位 RISC 微控制器。这款 Wave DPU 可以在没有主机 CPU 时自动执行。」对于 TensoFlow 指令：「Wave DNN Library 团队为 TensorFlow 等工作流程中所使用的常见 DNN 函数创建了预编译的可重新定位的 kernel。它们可以被组合到 Agent 中并且可以被实例化成机器，以构建大型的张量数据流图和 DNN kernel。」「……一个可与 TensorFlow、CNTK、Caffe 和 MXNet 等机器学习工作流程交互的 session 管理器，可作为用于训练和推理的工作器过程（worker process）。这些工作流程可为工作器过程提供张量的数据流图。在运行时，Wave 的 session 管理器会分析数据流图并将这些软件智能体放置到 DPU 芯片中，然后将它们连接起来以构建数据流图。这些软件智能体会被分配用于输入缓冲和本地存储的全局内存区域。CGRA kernel 的静态本质和分布式的内存架构可以让一个性能模型准确估计智能体的延迟。session 管理器可使用该性能模型来在智能体之间插入 FIFO 缓冲，这有助于 DPU 中通信和计算的重叠。这个可变智能体支持穿过整个图的数据流的软件流程，从而可以进一步增加并发性和性能。这个 session 管理器可以监控数据流图运行时的性能（通过监控卡顿、缓冲区下溢出和/或上溢出），并且可以动态地调节 FIFO 缓冲区的大小以实现吞吐量的最大化。在附加了 DPU 的处理器中，有一个分布式运行时管理系统会在运行时安装和卸载数据流图的一些部分，从而平衡计算和内存的使用量。这种在数据流计算机中的数据流图上的运行时重新配置还是有史以来第一次。」是的，我也觉得非常酷。这个平台的惊人之处是它在架构方面比 FPGA 更加粗粒度，因此灵活性更低，但却很可能表现更好。非常有意思。KnuEdge 的 KnuPath在 2016 年 6 月时我在 Twitter 上谈过 KnuPath。那以后他们的产品页面却失踪了。我不确定他们要把那 1 亿美元如何用到他们的 MIMD 架构上。那时候他们向我这样描述的：每个 ASIC 中有 256 个微型 DSP（即 tDSP）核以及一个 ARM 控制器，适用于 35W 包络中的稀疏矩阵处理。它的性能未知，但他们把自己的芯片与一款当时的英伟达芯片进行了比较，那时候他们说实现了 2.5 倍的性能。我们知道英伟达现在凭借 Tensor 内核已经提速了十倍以上，所以 KnuEdge 还要努力才能跟上节奏。MIMD 或 DSP 方法必须要得到非常好的效果才能在这一领域占据一席之地。时间会给我们答案。英特尔的 NervanaNervana Systems 曾经除了开发他们的 Nervana Engine ASIC，还开发着一种 GPU/软件方法，后来英特尔收购了这家公司。性能比较方面还不清楚。英特尔也在规划通过一个 Knights Crest 项目将其集成到 Phi 平台中。NextPlatform 认为其 2017 年在 28nm 节点上的目标是在某个位宽的运算速度达到 55 TOPS/s。英特尔还安排一个 NervanaCon，将在 12 月份举办，所以那时候我们也许将能看到他们的第一批成果。地平线机器人这家中国创业公司正在研发一种大脑处理单元（BPU：Brain Processing Unit）。余凯博士是正规军出身，他曾是百度深度学习研究院的负责人。今年早些时候，一段 YouTube 视频演示了基于 Arria 10 FPGA 的 BPU 仿真：https://youtu.be/GI9U9lUFaDo。目前关于这一平台的公开消息还很少。EyerissEyeriss 是 MIT 的一个项目，开发出了一款具有出色原始性能表现的 64nm ASIC。在 AlexNet 上，这款芯片的速度大约是英伟达 TK1 的一半。其优势在于借助于其行固定（row stationary）方法，仅需要一个 278mW 的可重新配置加速器就能实现这样中规中矩的性能。赞。Graphcore去年 Graphcore 拿到了 3000 万美元的 A 轮融资，以开发他们的智能处理单元（IPU： Intelligence Processing Unit）。他们的网站还缺少细节，只是给出了一些亮眼的事实，比如多于 14000 个独立的处理器线程和大于 100 倍的内存带宽。根据 NextPlatform 报道的零星信息，其在一块芯片上具有多于 1000 个真实内核，并且采用了定制的互连方式。它的 PCIe 板具有一个 16 个处理器的元件。听起来似乎是数据流。抛开公关的言论，这个团队确实有很强的背景，而且投资者也不并不傻，所以就拭目以待吧。TenstorrentTenstorrent 是加拿大多伦多的一家小创业公司，它宣称在深度学习的效率上实现了一个数量级的提升，和大多数公司一样，还没有什么公开的细节，但该公司入选了 Cognitive 300 榜单。CerebrasCerebras 值得一提，因为它得到了 Benchmark 的支持，而且其创始人是 SeaMicro 的 CEO。它似乎已经融资 2500 万美元了，而且仍然处于隐身模式。ThinciThinci 正在美国萨克拉门托开发视觉处理器，并且在印度也有员工。他们宣称即将推出他们的第一款硅芯片 Thinci-tc500，而且已经开始进行标准评测和赢得客户了。但除了「一切都并行处理」之外，我们所知甚少。KonikuKoniku 的网站正在倒计时，现在还有 20 几天。我已经等不及了。他们没有融到多少钱，而看过它们在福布斯上的这个视频之后（https://goo.gl/VA1PJx），你很可能也不得不相信他们，但你也无法预料究竟会如何。利用生物细胞肯定是不一样的。听起来就像是一个科研项目，但他们这样说：「我们是一个企业。我们不是一个科研项目。」下周将在维也纳的 Pioneers Festival 上发表演讲的 Agabi 这样说，「今天有些需求是硅所不能满足的，而我们可以通过我们的系统提供。」Koniku 提供的核心是所谓的神经元壳（neuron-shell），这家创业公司称其内部可以控制神经元彼此通信的方式，加上一种正在申请专利的电极，就可以在神经元上读写信息。所有这些都可以装在一个 iPad 大小的设备里，他们还希望能在 2018 年之前将其减小到一枚五美分硬币大小。AdaptevaAdapteva 是我最喜欢的一家小技术公司，正如你在之前文章《Adapteva tapes out Epiphany-V：一款 1024 核 64 位 RISC 处理器》：https://goo.gl/6ZH7JP。去年年底时 Andreas Olofsson 拿出了他的 1024 核芯片，我们都等着看它的性能表现。Epiphany-V 有用于深度学习的新指令，我们必须要看看这种带有 64MB 片上内存的更少内存控制器的设计是否具有合适的扩展能力。Andrea 的设计和构建的出色效率可能能让我们真正负担得起这种芯片，所以让我们希望它有良好的表现吧。KnowmKnown 研究的是 Anti-Hebbian and Hebbian（AHaH）可塑性和忆阻器。这里有篇覆盖这一主题的论文《AHaH 计算：从 Metastable Switches 到 Attractors 到机器学》：https://doi.org/10.1371/journal.pone.0085175。这对我来说有点太高级了。简单看了看，我看不出这项技术和胡言乱语的区别，但看起来确实科学味道十足。我需要亲眼看到才能相信。神经忆阻式处理器（neuromemristive processor）的思想是很有趣的。我早上确实需要一个好的流行术语。MythicMythic 的一款电池驱动的神经芯片具有低 50 倍的功耗。目前还看不到太多真正的细节。这款芯片大约纽扣大小，但大多数芯片不都这样吗？「Mythic 的平台能在纽扣大小的芯片上提供桌面 GPU 的性能。」也许这又是一款适合无人机和手机的芯片，很可能被用在手机中，也可能被排除在外。高通手机显然是机器学习硬件的一大用武之地。我们希望能够识别狗的品种、花朵、树叶、癌症痣、翻译标识、理解口语等等。我们口袋里的超级计算机愿意用上它能得到的所有帮助，以便能迈入感知时代。高通一直以来都在鼓捣机器学习，推出了 Zeroth SDK 和 Snapdragon 神经处理引擎（NPE）。这种 NPE 显然在高通所用的 Hexagon DSP 上效果良好。Hexagon DSP 已经远远不止是一个非常广泛的并行平台了，Yann LeCun 已经证实高通和 Facebook 正在合作开发一种更好的方法，参见 Wired 的文章《业界 | 谷歌 TPU 之后还有高通，人工智能芯片竞赛已经展开》：「最近，高通已经开始制造执行神经网络的专用芯片，这条消息来自 LeCun，因为 Facebook 正帮助高通开发机器学习相关技术，所以他对高通的计划很了解；高通技术副总裁 Jeff Gehlhaar 证实了这个项目，他说：『在原型设计和开发方面，我们还有很长的路要走。』」也许我们很快就会看到 Kryo CPU、Adreno GPU、Hexagon DSP 和 Hexagon Vector Extensions 之外的其它东西。对于这一领域的创业公司来说，和高通的机器学习竞争将会艰难无比。Pezy-SC 和 Pezy-SC2这两者是 Pezy 开发的 1024 核和 2048 核处理器。Pezy-SC 1024 核芯片可以驱动 2015 年 Green500 超级计算机榜单的前三名的系统。Pezy-SC2 是现在已经开始提供的后续芯片，我在 6 月份也做过关于它的演讲，但相关细节还很少，不过仍然很吸引人：「PEZY-SC2 HPC Brick：单个机箱中有 32 个带有 64GB DDR4 DIMM（2.1 PetaFLOPS（DP））的 Pezy-SC2 模块卡，速度可达 6.4 Tb/s.」不知道 2048 个 MIMD MIPS Warrior 64 位核的机器能做到什么？在 6 月份的 2017 年 Green500 榜单中，一个英伟达 P100 系统拿到了头把交椅，而排名第 7 的是一个 Pezy-SC2 系统。所以看起来这款芯片还活着，但相关细节却很少。Motoaki Saito（齊藤元章）当然值得一看。Kalray尽管做了很多承诺，但 Kalray 的芯片还没有超过 256 核，我在 2015 年的一篇文章就谈到：https://goo.gl/pxqn7Z。Kalray 宣传自己的产品说是适合嵌入式自动驾驶汽车应用，但我觉得其目前产品架构的形式并不是一种完美的 CNN 平台。Kalray 有一个 Kalray Neural Network（KaNN）软件包并且宣称有比 GPU 更好的效率，在芯片上能实现高达 1 TFlop/s 的速度。随着即将到来的产品更新，Kalray 的神经网络财富可能还会提升，就在这个月 Kalray 完成了新一轮 2600 万美元的融资。他们新的 Coolidge 处理器预计将在 2018 年年中上市，它会带有 80 或 160 个核，另外还有 80 或 160 个为视觉和深度学习优化的协处理器。这在他们的多于 1000 核的方法上有了很大的改变，而我认为这是最明智的。IBM TrueNorthTrueNorth 是 IBM 的神经形态 CMOS ASIC，是与 DARPA 的 StNAPSE 项目一起开发的。这是一种单个芯片设计上的多核处理器网络，具有 4096 个核，每个核模拟 256 个可编程的硅「神经元」，总共就超过了 100 万个神经元。另外，每个神经元还有 256 个可编程的「突触」，信号可以在这些突触之间传递。因此，可编程突触的总是超过了 2.68 亿（2^28）。在基本构建模块方面，它的晶体管数量为 54 亿。因为存储、计算和通信都是在这 4096 个神经突触核中各自处理的，所以 TrueNorth 避开了冯·诺依曼架构的瓶颈，而且能效非常高，功耗为 70 mW，大约是传统微处理器的功率密度的万分之一（来自维基百科）。IBM 之前还在批评脉冲神经网络（spiking neural network）无法适配深度学习，现在 IBM 开发了一种在 TureNorth 上运行 CNN 的新算法。这些神经元不会在每个周期内都放电，脉冲神经网络中的神经元必须逐渐积累它们的电势，然后才能放电……深度学习专家普遍认为脉冲神经网络在深度学习上没有效率——至少和卷积神经网络比起来是这样。Facebook 人工智能研究院主任和深度学习先驱 Yann LeCun 曾经批评过 IBM 的 TureNorth 芯片，因为它主要支持脉冲神经网络…………这种神经形态芯片不会太激动人心，因为它们注重的脉冲神经网络在深度学习领域并不受欢迎。为了让 TrueNorth 芯片很好地契合深度学习，IBM 不得不开发了一种新算法，让卷积神经网络可以在这款神经形态计算硬件上很好地运行。这种组合方法实现了 IBM 所谓的「接近当前最佳」的分类准确度，实验涉及了视觉和语音挑战赛方面的 8 个数据集。在最好的情况下，它们的准确度达到了 65% 到 97%。当只使用一块 TrueNorth 芯片时，它只在这 8 个数据集中的 1 个数据集上超越了当前最佳的准确度。但如果使用多达 8 块芯片，IBM 的研究者可以极大提升这款硬件在深度学习挑战上的准确度。这使 TureNorth 可以在其中 3 个数据集上媲美或超越当前最佳的准确度。这个 TureNorth 测试也做到了每秒处理 1200 到 2600 视频帧。这意味着单个 TureNorth 芯片就能实时检测来自多达 100 个同时工作的相机的数据的模式……（来自 IEEE Spectrum）TrueNorth 的功率效率非常出色，所以非常值得考虑。Brainchip 的脉冲神经元自适应处理器（SNAP：Spiking Neuron Adaptive Processor）SNAP 不能做深度学习，这只是一个源于好奇心的项目，还没有实际落地成 CNN 工程解决方案，至少现在还没有。如果你想探索这条路，IBM 的随机相变神经元（stochastic phase-change neurons）似乎更有意思。苹果的神经引擎（Neural Engine）到底会不会有？彭博社报道称这会是一款次级处理器，但没有什么细节信息。对苹果来说，这不仅是一个重要领域，而且也有助于与高通的竞争。其它1. 寒武纪（Cambricon）——中国科学院在芯片上投入的 140 万美元。它是一个用于神经网络的指令集架构，具有数据级的并行、自定义向量/矩阵指令、片上 scratchpad memory。宣称速度是 x86 CPU 的 91 倍，是 K40M 的 3 倍，且峰值功率仅有 1%，即 1.695W。参阅这两篇论文：Cambricon-X：一个用于稀疏神经网络的加速器：http://cslt.riit.tsinghua.edu.cn/mediawiki/images/f/f1/Cambricon-X.pdfCambricon：一个用于神经网络的指令集架构：http://dl.acm.org/citation.cfm?id=30011792. 前谷歌员工创立的 Groq Inc.，也许是另一种 TPU？3. Aimotive：https://aimotive.com/4. Deep Vision 正在开发用于深度学习的低功耗芯片，也许这两篇创始人的论文能提供一点线索：Convolution Engine: Balancing Efficiency &amp; Flexibility in Specialized Computing [2013]：http://csl.stanford.edu/~christos/publications/2013.convolution.isca.pdfConvolution Engine: Balancing Efficiency and Flexibility in Specialized Computing [2015]：http://csl.stanford.edu/~christos/publications/2015.convolution_engine.cacm.pdf5. DeepScale&nbsp;6. Reduced Energy Microsystems 正在开发用于 CNN 推理的低功耗异步芯片。据 TechCrunch 报道，REM 是 Y Combinator 第一个 ASIC 领域的风险投资。7. Leapmind 也很忙。FPGA微软已经站队 FPGA 了。Wired 这篇文章说得很好：《深度 |《连线》长文揭秘微软 Project Catapult：人工智能时代押注 FPGA》。「Bing 占据着世界上 20% 的桌面搜索市场和 6% 的移动手机市场，在 Bing 上，这个芯片能帮助 Bing 适应新品种的人工智能：深度神经网络。」我对这种方法也有些兴趣。赛灵思和英特尔（收购了 Altera）的 FPGA 是很强大的引擎。赛灵思自然宣称他们的 FPGA 是对 INT8 最好的，他们的一份白皮书里面包含了下面的幻灯片：这两家供应商都很支持使用他们的 FPGA 做机器学习：Xilinx - Acceleration Zone：https://goo.gl/KheG5WIntel FPGA OpenCL（https://goo.gl/S62fMA）和 Solutions（https://goo.gl/zkYyXB）尽管 FPGA 单位功耗的性能是很出色的，但这些供应商的更大型的芯片的售价却长时间以来都高得吓人。赛灵思的 VU9P 在 Avnet 上的售价超过 5 万美元。寻找价格和能力之间的平衡点是 FPGA 的主要难题。FPGA 方法的一大优势是可以用来制定一些非常出色的架构决策。比如如果因为 HBM，你想在板上 DRAM 压缩并且实时地解压，然后希望借此改善你的内存的浮点数据流，如果你足够努力，你能找到解决方案。参阅《用于基于 FGPA 的高性能计算的浮点数值数据流的带宽压缩（Bandwidth Compression of Floating-Point Numerical Data Streams for FPGA-Based High-Performance Computing）》：http://dl.acm.org/citation.cfm?id=3053688。这种动态的架构敏捷性（architectural agility）很困难，几乎无法用其它任何方法实现。架构选择太多可能也是一个问题，但我自己还是挺喜欢这个问题的。这篇论文很不错《使用 TILT 减少 Soft Scalar CPU 和定制硬件之间的性能差距（Reducing the Performance Gap between Soft Scalar CPUs and Custom Hardware with TILT）》：http://dl.acm.org/citation.cfm?id=3079757，其中研究了定制硬件和 FPGA 处理器（带有基于 FPGA 的水平微编码的计算引擎）的性能差距，这让人想起了古老的 DISC 和很多个月前的离散指令集计算机（discrete instruction set computer）。谁是赢家？在这样的竞争中，预测谁是赢家是傻瓜的举动。高通凭借其手机市场的主导地位，可以轻松进入赢家榜单。苹果不管做什么都会成功的。英伟达的 V100 有 Tensor 单元，也基本上赢了。我不确定我能否看到谷歌的 TPU 在无休止的长期硅谷竞赛中幸存下来，尽管目前它的性能很出色。我很喜欢 FPGA 方法，但我也不禁想他们应该以远远更低的价格发布 DNN 版本，这样才不会被大众漠视。英特尔和 AMD 会做自己的协处理器。因为所有主要玩家都参战了，所以其中很多都会支持标准的工具包，比如 TensorFlow，这样我们就不必太在意规格了，关心基准就可以了。在更小型的玩家中，我很喜欢而且支持的是 Adapteva 方法，我认为他们的内存架构可能并不适合 DNN。我希望我错了。Wave Computing 可能是我继 FPGA 之后最喜欢的方法。他们的整个异步数据流方法是非常棒的。REM 似乎也在做类似的事情；但我认为他们可能太迟了。Wave Computing 能否在面对所有竞争对手时继续保持领先吗？也许只要他们的异步 CGRA 有一个本质优势，就可以。尽管我不确定他们是否只要 DNN 上的成功，因为他们的技术有更加广泛的应用能力。神经形态脉冲处理器也许现在暂时可以忽略，但也可以保持关注，因为它们有很大的功耗优势。量子计算反正会让所有这些都稍逊风骚。IBM 的 TrueNorth 可能会是个例外，因为它不仅能执行脉冲网络，还能有效地运行 DNN。&nbsp;原文链接：https://meanderful.blogspot.jp/2017/06/fpgas-and-ai-processors-dnn-and-cnn-for.html​"
"FPGA;;从GPU、TPU到FPGA及其它：一文读懂神经网络硬件平台战局在如今深度学习大爆发的时代，相关的硬件平台也在百花齐放，既有英伟达和谷歌这样的科技巨头，也有地平线机器人和 Graphcore 等创业公司——它们都各自提出了自己的解决方案。近日，多家公司的技术顾问 Matt Hurd 在其博客上发表了一篇全面评点各种神经网络硬件平台的长文，机器之心对本文进行了编译介绍。这是我几周前做的一个传统的 90 年代风格的性别识别神经网络的很好的隐藏节点。一个简单的性别识别器网络中的 90 年代风格的隐藏节点图像我的硕士项目是一种类似级联相关（cascade correlation）的神经网络 Multi-rate Optimising Order Statistic Equaliser（MOOSE：多速率优化顺序统计均衡器），可用于日内的 Bund（国库债券产品）交易。MOOSE 曾经是为获取高速的 LEO 卫星信号（McCaw 的 Teledesic）而设计的一点成果，后来在从 LIFFE 迁移到 DTB 时将目标转向了 Bund。作为一家投资银行的职业交易员，我可以购买很好的工具。我有那时候世界上最快的计算机：一个 IBM MicroChannel dual Pentium Pro 200MHz 处理器外加带有几 MB RAM 的 SCSI。在 1994 年那会儿，将 800,000 个数据点输入到我的 C++ stream/dag 处理器中看上去就像是黑魔法。有限差分方法让我可以做许多 O(1) 增量线性回归这样的运算，以获得 1000 倍的加速。那时候这看起来已经很好了。现在，你的手机都能嘲笑我的大方向。那时候，在神经网络领域有很多研究。倒不是说它有过人的生产力，只是因为有用。读到 Lindsay Fortado 和 Robin Wigglesworth 的 FT 文章《Machine learning set to shake up equity hedge funds》中 Eric Schmidt 关于机器学习和交易的看法，还真有点让人高兴：Eric Schmidt 是谷歌母公司 Alphabet 的执行董事长，他上周对一众对冲基金经理说他相信在 50 年内，所有交易都会有计算机解读数据和市场信号。「我期待出现在交易方面做机器学习的创业公司，看看我描述的这种模式识别能否比数据分析专家的传统线性回归算法做得更好。」他补充说，「我这个行业内的许多人都认为这注定将成为一种新的交易形式。」老朋友 Eric，我在 90 年代早期就已经算是迟到了，你真是有点后知后觉。好吧，现在情况已经不同了。我喜欢思考它，并喜欢将神经网络的这次新复兴称为感知时代（The Age of Perception）。这并不是智能，只是擅长模式而已。它仍然无力应对语言歧义。它还要一些时间才能理解基本的价值和概念，从而形成深刻的金融理解。深度学习既被夸大了，也被低估了。这不是智能，但会有助于帮我们实现智能。一些人将其夸大为将会给我们带来替代人的类人机器的人工智能突破。我们仍然还受困于常识以及用于推理的简单文本中的歧义。我们还有很长的路要走。相对简单的规划算法和启发式方法以及基于神奇的深度学习的视觉、声音、文本、雷达等等的感知能力将会带来深刻的影响，就像是每个人和他们的狗现在理解的那样。所以我叫它「感知时代」。就好像是我们口袋里的超级计算机突然有了眼睛，并且快速适应了真实世界所带来的闪光性致盲。深度学习将会带来巨大的影响，并且将会改变这颗行星上全人类的生活方式。但我们低估了其对我们的危险。不，我们不会和能激起或挑战我们最深刻的思想的深度图灵对话者约会——还不会。这将不可避免地到来，但在可见的未来里还不可见。借助语音、文本和 Watson 那样的数据库的智能代理可以实现非常先进的 Eliza，但不会更先进了。自动化运输、食物生产、建筑、协助家事将会极大地改变人们的生活方式和不动产的价值。除了这些泛泛之谈，本文的目的是收集一些关于芯片的思想见解——它们驱动着当前的神经网络革命。其中很多见解都不是最为激动人心的，但这对我来说是一个有用的锻炼。神经网络硬件与 20 年前相比，今天的神经处理方法并没有很大不同。深度更多的是一个品牌，而不是一项差异。激活函数已经得到了简化，以更好地适配硬件。主要的成功之处在于我们有了更多数据，对如何初始化权重、处理许多层、并行化和提升鲁棒性也有了更好的理解，其中要用到像是 dropout 这样的技术。1980 年的 Neocognitron 架构与今天的深度学习器或 CNN 并没有显著差异，但 Yann LeCun 让它具备了学习能力。在 90 年代那会儿也有很多神经硬件平台，比如 CNAPS（1990），它带有 64 个处理单元和 256kB 内存，可以在 8/16 位条件下达到 1.6 GCPS 的速度（CPS 是指每秒连接次数/ connections per second）或在 1 位条件下达到 12.8 GCPS 的速度。你可以在《神经硬件概述（Overview of neural hardware）》[Heemskerk, 1995, draft] 中读到 Synapse-1、CNAPS、SNAP、CNS Connectionist Supercomputer、Hitachi WSI、My-Neupower、LNeuro 1.0、UTAK1、GNU（通用神经单元/General Neural Unit）Implementation、UCL、Mantra 1、Biologically-Inspired Emulator、INPG Architecture、BACHUS 和 ZISC036。阅读地址：https://pdfs.semanticscholar.org/5841/73aa4886f87da4501571957c2b14a8fb9069.pdf好吧，东西还真多，但实际上还排除了软件和加速器板/CPU 组合，比如 ANZA plus、SAIC SIGMA-1、NT6000、Balboa 860 协处理器、Ni1000 识别加速器硬件（英特尔）、IBM NEP、NBC、Neuro Turbo I、Neuro Turbo II、WISARD、Mark II & IV、Sandy/8、GCN（索尼）、Topsi、BSP400（400 微处理器）、DREAM Machine、RAP、COKOS、REMAP、通用并行神经计算机（General Purpose Parallel Neurocomputer）、TI NETSIM 和 GeNet。另外还有一些模拟和混合模拟的实现，包括英特尔的电气式可训练模拟神经网络（801770NX）。你懂我要表达的意思了，那时候的东西还真是多。这在 1994 年迎来了一次爆发：乐观的摩尔定律告诉我们，TeraCPS 即将实现：「未来十年，微电子很可能将继续主导神经网络实现的领域。如果进展和过去进步得一样快，那就意味着神经计算机的性能将会增长大约 2 个数量级。因此，神经计算机将会接近 TeraCPS（10^12 CPS）的性能。由 100 万个节点（每个节点有大约 1000 个输入）组成的网络可以达到大脑的计算速度（100-1000 Hz）。这将能为实验合理巨大的网络提供良好的机会。」由于 Minsky 和 Papert 对隐藏层的不正确简单概括，打击了 Rosenblatt 的感知器梦想并最终导致了他不幸的死亡，神经网络研究遭遇了第一个冬天，研究资金被残酷地撤销了。1995 年，又出现了另一次神经网络冬天，尽管那时候我其实并不知道。作为温水锅里的一只青蛙，我没有注意到正在加热。第二个冬天的主要原因是缺乏激动人心的进展，让人们普遍感到无聊了。到了 2012 年，多亏了 Geoffrey Hinton 的冬季生存技能，多伦多大学基于 AlexNet 开发的 SuperVision 在 ImageNet 处理上实现了极大的提升，第二个神经网络冬天也由此终结了。之后谷歌的 LeNet Inception 模型在 2014 年打破了它的记录。所以据我估计，感知时代始于 2012 年。将它记在你的日历里面吧，五年已经过去了。谷歌在几千台普通机器上进行了出色的并行 CPU 有损更新研究。吴恩达教授和他的朋友们让数十台 GPU 就能完成数千台 CPU 的工作，从而让规模化成为了可能。因此，我们从需要很好的资助的神经处理前景中解放了出来。好吧，或多或少吧，现在最先进的网络有时候需要数千台 GPU 或专用芯片。更多数据和更多处理能力是其中的关键。让我们进入本文的重点，列出一些感知时代大数据之战中的一些关键平台：英伟达的 GPU这一家是很难被战胜的。来自大型视频处理市场的收益驱动着巨大的规模经济。新款英伟达 V100 带有一种新的 Tensor Core 架构，速度可达 15 TFlops（单精度/SP）或 120 TFlops（浮点精度，其中带有 FP16 的乘法和 FP32 的累加或加法，非常适合机器学习）。英伟达在它们的 DGX-1 中装入了 8 块计算卡，速度可达 960 Tensor TFlops.AMD 的 GPU在机器学习领域，AMD 一直是英伟达的追赶者。即将发布的 AMD Radeon Instinct MI25 有希望达到 12.3 TFlops（SP）或 24.6 TFlops（FP16）。如果你把英伟达的 Tensor Core 也算进来，则 AMD 完全无力竞争。英伟达设备的带宽 900GB/s 也是 AMD 484GB/s 的两倍。谷歌的 TPU谷歌原来的 TPU 相比于 GPU 有很大的领先，并帮助 DeepMind 的 AlphaGo 赢得了与李世石的围棋大战。据描述，原来的 700 MHz TPU 有 95 TFlops 的 8 位计算能力或 23 TFlops 的 16 位计算能力，同时功耗仅有 40W。这可比当时的 GPU 快得多，但现在落后于英伟达的 V100；但在单位功耗的计算能力上，TPU 并没落后。据称新的 TPU2 是一款带有 4 块芯片的 TPU 设备，速度可达到 180 TFlops 左右。每块芯片的性能都翻倍了，达到了 45 TFlops 的 16 位计算能力。你可以看到与英伟达 V100 的差距正在变小。你没法买到 TPU 或 TPU2。谷歌正在通过它们的云提供这些 TPU 服务，包含 64 台设备的 TPU pod 速度高达 11.5 PetaFlops。TPU2 上巨大的散热片说明了一些原因，但市场正在发生变化——从单独的设备转向了设备的组合以及将这些组合以云的形式提供。Wave ComputingWave 之父澳洲人 CTO Chris Nicol 博士的成果斐然，领导开发了 Wave 的 Compute Appliance 中的异步数据流处理器（asynchronous data flow processor）。几年前，Metamako 的创始人 Charles Thomas 在加州简单介绍了我和 Chris 认识。他们俩都曾在 NICTA 研究过无时钟异步。这两人都很出色。我不确定 Wave 的设备最早是不是针对机器学习设计的，但在他们的 3RU appliance 上运行 TensorFlow 的速度可以达到 2.9 PetaOPS/s，这实在了不起。Wave 将他们的处理器称为 DPU，一个 appliance 有 16 个 DPU。Wave 使用了他们称之为粗粒度可重构阵列（CGRA：Coarse Grained Reconfigurable Array）的处理元素。我还不清楚 2.9 PetaOPS/s 的速度对应多少位宽。根据他们的白皮书，其 ALU 可以执行 1 位、8 位、16 位和 32 位计算：「算术单元是分区的。它们可以并行执行 8 位运算（完美适用于 DNN 推理）以及 16 位和 32 位运算（或以上的任何组合）。也可以执行一些 64 位运算，而且可以使用软件扩展到任意精度。」关于其 appliance 中的 16 个 DPU，还有一些额外的信息：「Wave Computing DPU 是一种包含了 16384 PE 的 SoC，它们被配置成了一个 32×32 集群的 CGRA。它包含了 4 个 HMC（Hybrid Memory Cube）第二代接口、一个 PCIe 第三代 16 通道接口和一个用于 SoC 资源管理的嵌入式 32 位 RISC 微控制器。这款 Wave DPU 可以在没有主机 CPU 时自动执行。」对于 TensoFlow 指令：「Wave DNN Library 团队为 TensorFlow 等工作流程中所使用的常见 DNN 函数创建了预编译的可重新定位的 kernel。它们可以被组合到 Agent 中并且可以被实例化成机器，以构建大型的张量数据流图和 DNN kernel。」「……一个可与 TensorFlow、CNTK、Caffe 和 MXNet 等机器学习工作流程交互的 session 管理器，可作为用于训练和推理的工作器过程（worker process）。这些工作流程可为工作器过程提供张量的数据流图。在运行时，Wave 的 session 管理器会分析数据流图并将这些软件智能体放置到 DPU 芯片中，然后将它们连接起来以构建数据流图。这些软件智能体会被分配用于输入缓冲和本地存储的全局内存区域。CGRA kernel 的静态本质和分布式的内存架构可以让一个性能模型准确估计智能体的延迟。session 管理器可使用该性能模型来在智能体之间插入 FIFO 缓冲，这有助于 DPU 中通信和计算的重叠。这个可变智能体支持穿过整个图的数据流的软件流程，从而可以进一步增加并发性和性能。这个 session 管理器可以监控数据流图运行时的性能（通过监控卡顿、缓冲区下溢出和/或上溢出），并且可以动态地调节 FIFO 缓冲区的大小以实现吞吐量的最大化。在附加了 DPU 的处理器中，有一个分布式运行时管理系统会在运行时安装和卸载数据流图的一些部分，从而平衡计算和内存的使用量。这种在数据流计算机中的数据流图上的运行时重新配置还是有史以来第一次。」是的，我也觉得非常酷。这个平台的惊人之处是它在架构方面比 FPGA 更加粗粒度，因此灵活性更低，但却很可能表现更好。非常有意思。KnuEdge 的 KnuPath在 2016 年 6 月时我在 Twitter 上谈过 KnuPath。那以后他们的产品页面却失踪了。我不确定他们要把那 1 亿美元如何用到他们的 MIMD 架构上。那时候他们向我这样描述的：每个 ASIC 中有 256 个微型 DSP（即 tDSP）核以及一个 ARM 控制器，适用于 35W 包络中的稀疏矩阵处理。它的性能未知，但他们把自己的芯片与一款当时的英伟达芯片进行了比较，那时候他们说实现了 2.5 倍的性能。我们知道英伟达现在凭借 Tensor 内核已经提速了十倍以上，所以 KnuEdge 还要努力才能跟上节奏。MIMD 或 DSP 方法必须要得到非常好的效果才能在这一领域占据一席之地。时间会给我们答案。英特尔的 NervanaNervana Systems 曾经除了开发他们的 Nervana Engine ASIC，还开发着一种 GPU/软件方法，后来英特尔收购了这家公司。性能比较方面还不清楚。英特尔也在规划通过一个 Knights Crest 项目将其集成到 Phi 平台中。NextPlatform 认为其 2017 年在 28nm 节点上的目标是在某个位宽的运算速度达到 55 TOPS/s。英特尔还安排一个 NervanaCon，将在 12 月份举办，所以那时候我们也许将能看到他们的第一批成果。地平线机器人这家中国创业公司正在研发一种大脑处理单元（BPU：Brain Processing Unit）。余凯博士是正规军出身，他曾是百度深度学习研究院的负责人。今年早些时候，一段 YouTube 视频演示了基于 Arria 10 FPGA 的 BPU 仿真：https://youtu.be/GI9U9lUFaDo。目前关于这一平台的公开消息还很少。EyerissEyeriss 是 MIT 的一个项目，开发出了一款具有出色原始性能表现的 64nm ASIC。在 AlexNet 上，这款芯片的速度大约是英伟达 TK1 的一半。其优势在于借助于其行固定（row stationary）方法，仅需要一个 278mW 的可重新配置加速器就能实现这样中规中矩的性能。赞。Graphcore去年 Graphcore 拿到了 3000 万美元的 A 轮融资，以开发他们的智能处理单元（IPU： Intelligence Processing Unit）。他们的网站还缺少细节，只是给出了一些亮眼的事实，比如多于 14000 个独立的处理器线程和大于 100 倍的内存带宽。根据 NextPlatform 报道的零星信息，其在一块芯片上具有多于 1000 个真实内核，并且采用了定制的互连方式。它的 PCIe 板具有一个 16 个处理器的元件。听起来似乎是数据流。抛开公关的言论，这个团队确实有很强的背景，而且投资者也不并不傻，所以就拭目以待吧。TenstorrentTenstorrent 是加拿大多伦多的一家小创业公司，它宣称在深度学习的效率上实现了一个数量级的提升，和大多数公司一样，还没有什么公开的细节，但该公司入选了 Cognitive 300 榜单。CerebrasCerebras 值得一提，因为它得到了 Benchmark 的支持，而且其创始人是 SeaMicro 的 CEO。它似乎已经融资 2500 万美元了，而且仍然处于隐身模式。ThinciThinci 正在美国萨克拉门托开发视觉处理器，并且在印度也有员工。他们宣称即将推出他们的第一款硅芯片 Thinci-tc500，而且已经开始进行标准评测和赢得客户了。但除了「一切都并行处理」之外，我们所知甚少。KonikuKoniku 的网站正在倒计时，现在还有 20 几天。我已经等不及了。他们没有融到多少钱，而看过它们在福布斯上的这个视频之后（https://goo.gl/VA1PJx），你很可能也不得不相信他们，但你也无法预料究竟会如何。利用生物细胞肯定是不一样的。听起来就像是一个科研项目，但他们这样说：「我们是一个企业。我们不是一个科研项目。」下周将在维也纳的 Pioneers Festival 上发表演讲的 Agabi 这样说，「今天有些需求是硅所不能满足的，而我们可以通过我们的系统提供。」Koniku 提供的核心是所谓的神经元壳（neuron-shell），这家创业公司称其内部可以控制神经元彼此通信的方式，加上一种正在申请专利的电极，就可以在神经元上读写信息。所有这些都可以装在一个 iPad 大小的设备里，他们还希望能在 2018 年之前将其减小到一枚五美分硬币大小。AdaptevaAdapteva 是我最喜欢的一家小技术公司，正如你在之前文章《Adapteva tapes out Epiphany-V：一款 1024 核 64 位 RISC 处理器》：https://goo.gl/6ZH7JP。去年年底时 Andreas Olofsson 拿出了他的 1024 核芯片，我们都等着看它的性能表现。Epiphany-V 有用于深度学习的新指令，我们必须要看看这种带有 64MB 片上内存的更少内存控制器的设计是否具有合适的扩展能力。Andrea 的设计和构建的出色效率可能能让我们真正负担得起这种芯片，所以让我们希望它有良好的表现吧。KnowmKnown 研究的是 Anti-Hebbian and Hebbian（AHaH）可塑性和忆阻器。这里有篇覆盖这一主题的论文《AHaH 计算：从 Metastable Switches 到 Attractors 到机器学》：https://doi.org/10.1371/journal.pone.0085175。这对我来说有点太高级了。简单看了看，我看不出这项技术和胡言乱语的区别，但看起来确实科学味道十足。我需要亲眼看到才能相信。神经忆阻式处理器（neuromemristive processor）的思想是很有趣的。我早上确实需要一个好的流行术语。MythicMythic 的一款电池驱动的神经芯片具有低 50 倍的功耗。目前还看不到太多真正的细节。这款芯片大约纽扣大小，但大多数芯片不都这样吗？「Mythic 的平台能在纽扣大小的芯片上提供桌面 GPU 的性能。」也许这又是一款适合无人机和手机的芯片，很可能被用在手机中，也可能被排除在外。高通手机显然是机器学习硬件的一大用武之地。我们希望能够识别狗的品种、花朵、树叶、癌症痣、翻译标识、理解口语等等。我们口袋里的超级计算机愿意用上它能得到的所有帮助，以便能迈入感知时代。高通一直以来都在鼓捣机器学习，推出了 Zeroth SDK 和 Snapdragon 神经处理引擎（NPE）。这种 NPE 显然在高通所用的 Hexagon DSP 上效果良好。Hexagon DSP 已经远远不止是一个非常广泛的并行平台了，Yann LeCun 已经证实高通和 Facebook 正在合作开发一种更好的方法，参见 Wired 的文章《业界 | 谷歌 TPU 之后还有高通，人工智能芯片竞赛已经展开》：「最近，高通已经开始制造执行神经网络的专用芯片，这条消息来自 LeCun，因为 Facebook 正帮助高通开发机器学习相关技术，所以他对高通的计划很了解；高通技术副总裁 Jeff Gehlhaar 证实了这个项目，他说：『在原型设计和开发方面，我们还有很长的路要走。』」也许我们很快就会看到 Kryo CPU、Adreno GPU、Hexagon DSP 和 Hexagon Vector Extensions 之外的其它东西。对于这一领域的创业公司来说，和高通的机器学习竞争将会艰难无比。Pezy-SC 和 Pezy-SC2这两者是 Pezy 开发的 1024 核和 2048 核处理器。Pezy-SC 1024 核芯片可以驱动 2015 年 Green500 超级计算机榜单的前三名的系统。Pezy-SC2 是现在已经开始提供的后续芯片，我在 6 月份也做过关于它的演讲，但相关细节还很少，不过仍然很吸引人：「PEZY-SC2 HPC Brick：单个机箱中有 32 个带有 64GB DDR4 DIMM（2.1 PetaFLOPS（DP））的 Pezy-SC2 模块卡，速度可达 6.4 Tb/s.」不知道 2048 个 MIMD MIPS Warrior 64 位核的机器能做到什么？在 6 月份的 2017 年 Green500 榜单中，一个英伟达 P100 系统拿到了头把交椅，而排名第 7 的是一个 Pezy-SC2 系统。所以看起来这款芯片还活着，但相关细节却很少。Motoaki Saito（齊藤元章）当然值得一看。Kalray尽管做了很多承诺，但 Kalray 的芯片还没有超过 256 核，我在 2015 年的一篇文章就谈到：https://goo.gl/pxqn7Z。Kalray 宣传自己的产品说是适合嵌入式自动驾驶汽车应用，但我觉得其目前产品架构的形式并不是一种完美的 CNN 平台。Kalray 有一个 Kalray Neural Network（KaNN）软件包并且宣称有比 GPU 更好的效率，在芯片上能实现高达 1 TFlop/s 的速度。随着即将到来的产品更新，Kalray 的神经网络财富可能还会提升，就在这个月 Kalray 完成了新一轮 2600 万美元的融资。他们新的 Coolidge 处理器预计将在 2018 年年中上市，它会带有 80 或 160 个核，另外还有 80 或 160 个为视觉和深度学习优化的协处理器。这在他们的多于 1000 核的方法上有了很大的改变，而我认为这是最明智的。IBM TrueNorthTrueNorth 是 IBM 的神经形态 CMOS ASIC，是与 DARPA 的 StNAPSE 项目一起开发的。这是一种单个芯片设计上的多核处理器网络，具有 4096 个核，每个核模拟 256 个可编程的硅「神经元」，总共就超过了 100 万个神经元。另外，每个神经元还有 256 个可编程的「突触」，信号可以在这些突触之间传递。因此，可编程突触的总是超过了 2.68 亿（2^28）。在基本构建模块方面，它的晶体管数量为 54 亿。因为存储、计算和通信都是在这 4096 个神经突触核中各自处理的，所以 TrueNorth 避开了冯·诺依曼架构的瓶颈，而且能效非常高，功耗为 70 mW，大约是传统微处理器的功率密度的万分之一（来自维基百科）。IBM 之前还在批评脉冲神经网络（spiking neural network）无法适配深度学习，现在 IBM 开发了一种在 TureNorth 上运行 CNN 的新算法。这些神经元不会在每个周期内都放电，脉冲神经网络中的神经元必须逐渐积累它们的电势，然后才能放电……深度学习专家普遍认为脉冲神经网络在深度学习上没有效率——至少和卷积神经网络比起来是这样。Facebook 人工智能研究院主任和深度学习先驱 Yann LeCun 曾经批评过 IBM 的 TureNorth 芯片，因为它主要支持脉冲神经网络…………这种神经形态芯片不会太激动人心，因为它们注重的脉冲神经网络在深度学习领域并不受欢迎。为了让 TrueNorth 芯片很好地契合深度学习，IBM 不得不开发了一种新算法，让卷积神经网络可以在这款神经形态计算硬件上很好地运行。这种组合方法实现了 IBM 所谓的「接近当前最佳」的分类准确度，实验涉及了视觉和语音挑战赛方面的 8 个数据集。在最好的情况下，它们的准确度达到了 65% 到 97%。当只使用一块 TrueNorth 芯片时，它只在这 8 个数据集中的 1 个数据集上超越了当前最佳的准确度。但如果使用多达 8 块芯片，IBM 的研究者可以极大提升这款硬件在深度学习挑战上的准确度。这使 TureNorth 可以在其中 3 个数据集上媲美或超越当前最佳的准确度。这个 TureNorth 测试也做到了每秒处理 1200 到 2600 视频帧。这意味着单个 TureNorth 芯片就能实时检测来自多达 100 个同时工作的相机的数据的模式……（来自 IEEE Spectrum）TrueNorth 的功率效率非常出色，所以非常值得考虑。Brainchip 的脉冲神经元自适应处理器（SNAP：Spiking Neuron Adaptive Processor）SNAP 不能做深度学习，这只是一个源于好奇心的项目，还没有实际落地成 CNN 工程解决方案，至少现在还没有。如果你想探索这条路，IBM 的随机相变神经元（stochastic phase-change neurons）似乎更有意思。苹果的神经引擎（Neural Engine）到底会不会有？彭博社报道称这会是一款次级处理器，但没有什么细节信息。对苹果来说，这不仅是一个重要领域，而且也有助于与高通的竞争。其它1. 寒武纪（Cambricon）——中国科学院在芯片上投入的 140 万美元。它是一个用于神经网络的指令集架构，具有数据级的并行、自定义向量/矩阵指令、片上 scratchpad memory。宣称速度是 x86 CPU 的 91 倍，是 K40M 的 3 倍，且峰值功率仅有 1%，即 1.695W。参阅这两篇论文：Cambricon-X：一个用于稀疏神经网络的加速器：http://cslt.riit.tsinghua.edu.cn/mediawiki/images/f/f1/Cambricon-X.pdfCambricon：一个用于神经网络的指令集架构：http://dl.acm.org/citation.cfm?id=30011792. 前谷歌员工创立的 Groq Inc.，也许是另一种 TPU？3. Aimotive：https://aimotive.com/4. Deep Vision 正在开发用于深度学习的低功耗芯片，也许这两篇创始人的论文能提供一点线索：Convolution Engine: Balancing Efficiency &amp; Flexibility in Specialized Computing [2013]：http://csl.stanford.edu/~christos/publications/2013.convolution.isca.pdfConvolution Engine: Balancing Efficiency and Flexibility in Specialized Computing [2015]：http://csl.stanford.edu/~christos/publications/2015.convolution_engine.cacm.pdf5. DeepScale&nbsp;6. Reduced Energy Microsystems 正在开发用于 CNN 推理的低功耗异步芯片。据 TechCrunch 报道，REM 是 Y Combinator 第一个 ASIC 领域的风险投资。7. Leapmind 也很忙。FPGA微软已经站队 FPGA 了。Wired 这篇文章说得很好：《深度 |《连线》长文揭秘微软 Project Catapult：人工智能时代押注 FPGA》。「Bing 占据着世界上 20% 的桌面搜索市场和 6% 的移动手机市场，在 Bing 上，这个芯片能帮助 Bing 适应新品种的人工智能：深度神经网络。」我对这种方法也有些兴趣。赛灵思和英特尔（收购了 Altera）的 FPGA 是很强大的引擎。赛灵思自然宣称他们的 FPGA 是对 INT8 最好的，他们的一份白皮书里面包含了下面的幻灯片：这两家供应商都很支持使用他们的 FPGA 做机器学习：Xilinx - Acceleration Zone：https://goo.gl/KheG5WIntel FPGA OpenCL（https://goo.gl/S62fMA）和 Solutions（https://goo.gl/zkYyXB）尽管 FPGA 单位功耗的性能是很出色的，但这些供应商的更大型的芯片的售价却长时间以来都高得吓人。赛灵思的 VU9P 在 Avnet 上的售价超过 5 万美元。寻找价格和能力之间的平衡点是 FPGA 的主要难题。FPGA 方法的一大优势是可以用来制定一些非常出色的架构决策。比如如果因为 HBM，你想在板上 DRAM 压缩并且实时地解压，然后希望借此改善你的内存的浮点数据流，如果你足够努力，你能找到解决方案。参阅《用于基于 FGPA 的高性能计算的浮点数值数据流的带宽压缩（Bandwidth Compression of Floating-Point Numerical Data Streams for FPGA-Based High-Performance Computing）》：http://dl.acm.org/citation.cfm?id=3053688。这种动态的架构敏捷性（architectural agility）很困难，几乎无法用其它任何方法实现。架构选择太多可能也是一个问题，但我自己还是挺喜欢这个问题的。这篇论文很不错《使用 TILT 减少 Soft Scalar CPU 和定制硬件之间的性能差距（Reducing the Performance Gap between Soft Scalar CPUs and Custom Hardware with TILT）》：http://dl.acm.org/citation.cfm?id=3079757，其中研究了定制硬件和 FPGA 处理器（带有基于 FPGA 的水平微编码的计算引擎）的性能差距，这让人想起了古老的 DISC 和很多个月前的离散指令集计算机（discrete instruction set computer）。谁是赢家？在这样的竞争中，预测谁是赢家是傻瓜的举动。高通凭借其手机市场的主导地位，可以轻松进入赢家榜单。苹果不管做什么都会成功的。英伟达的 V100 有 Tensor 单元，也基本上赢了。我不确定我能否看到谷歌的 TPU 在无休止的长期硅谷竞赛中幸存下来，尽管目前它的性能很出色。我很喜欢 FPGA 方法，但我也不禁想他们应该以远远更低的价格发布 DNN 版本，这样才不会被大众漠视。英特尔和 AMD 会做自己的协处理器。因为所有主要玩家都参战了，所以其中很多都会支持标准的工具包，比如 TensorFlow，这样我们就不必太在意规格了，关心基准就可以了。在更小型的玩家中，我很喜欢而且支持的是 Adapteva 方法，我认为他们的内存架构可能并不适合 DNN。我希望我错了。Wave Computing 可能是我继 FPGA 之后最喜欢的方法。他们的整个异步数据流方法是非常棒的。REM 似乎也在做类似的事情；但我认为他们可能太迟了。Wave Computing 能否在面对所有竞争对手时继续保持领先吗？也许只要他们的异步 CGRA 有一个本质优势，就可以。尽管我不确定他们是否只要 DNN 上的成功，因为他们的技术有更加广泛的应用能力。神经形态脉冲处理器也许现在暂时可以忽略，但也可以保持关注，因为它们有很大的功耗优势。量子计算反正会让所有这些都稍逊风骚。IBM 的 TrueNorth 可能会是个例外，因为它不仅能执行脉冲网络，还能有效地运行 DNN。&nbsp;原文链接：https://meanderful.blogspot.jp/2017/06/fpgas-and-ai-processors-dnn-and-cnn-for.html​"
"深度神经网络;;从GPU、TPU到FPGA及其它：一文读懂神经网络硬件平台战局在如今深度学习大爆发的时代，相关的硬件平台也在百花齐放，既有英伟达和谷歌这样的科技巨头，也有地平线机器人和 Graphcore 等创业公司——它们都各自提出了自己的解决方案。近日，多家公司的技术顾问 Matt Hurd 在其博客上发表了一篇全面评点各种神经网络硬件平台的长文，机器之心对本文进行了编译介绍。这是我几周前做的一个传统的 90 年代风格的性别识别神经网络的很好的隐藏节点。一个简单的性别识别器网络中的 90 年代风格的隐藏节点图像我的硕士项目是一种类似级联相关（cascade correlation）的神经网络 Multi-rate Optimising Order Statistic Equaliser（MOOSE：多速率优化顺序统计均衡器），可用于日内的 Bund（国库债券产品）交易。MOOSE 曾经是为获取高速的 LEO 卫星信号（McCaw 的 Teledesic）而设计的一点成果，后来在从 LIFFE 迁移到 DTB 时将目标转向了 Bund。作为一家投资银行的职业交易员，我可以购买很好的工具。我有那时候世界上最快的计算机：一个 IBM MicroChannel dual Pentium Pro 200MHz 处理器外加带有几 MB RAM 的 SCSI。在 1994 年那会儿，将 800,000 个数据点输入到我的 C++ stream/dag 处理器中看上去就像是黑魔法。有限差分方法让我可以做许多 O(1) 增量线性回归这样的运算，以获得 1000 倍的加速。那时候这看起来已经很好了。现在，你的手机都能嘲笑我的大方向。那时候，在神经网络领域有很多研究。倒不是说它有过人的生产力，只是因为有用。读到 Lindsay Fortado 和 Robin Wigglesworth 的 FT 文章《Machine learning set to shake up equity hedge funds》中 Eric Schmidt 关于机器学习和交易的看法，还真有点让人高兴：Eric Schmidt 是谷歌母公司 Alphabet 的执行董事长，他上周对一众对冲基金经理说他相信在 50 年内，所有交易都会有计算机解读数据和市场信号。「我期待出现在交易方面做机器学习的创业公司，看看我描述的这种模式识别能否比数据分析专家的传统线性回归算法做得更好。」他补充说，「我这个行业内的许多人都认为这注定将成为一种新的交易形式。」老朋友 Eric，我在 90 年代早期就已经算是迟到了，你真是有点后知后觉。好吧，现在情况已经不同了。我喜欢思考它，并喜欢将神经网络的这次新复兴称为感知时代（The Age of Perception）。这并不是智能，只是擅长模式而已。它仍然无力应对语言歧义。它还要一些时间才能理解基本的价值和概念，从而形成深刻的金融理解。深度学习既被夸大了，也被低估了。这不是智能，但会有助于帮我们实现智能。一些人将其夸大为将会给我们带来替代人的类人机器的人工智能突破。我们仍然还受困于常识以及用于推理的简单文本中的歧义。我们还有很长的路要走。相对简单的规划算法和启发式方法以及基于神奇的深度学习的视觉、声音、文本、雷达等等的感知能力将会带来深刻的影响，就像是每个人和他们的狗现在理解的那样。所以我叫它「感知时代」。就好像是我们口袋里的超级计算机突然有了眼睛，并且快速适应了真实世界所带来的闪光性致盲。深度学习将会带来巨大的影响，并且将会改变这颗行星上全人类的生活方式。但我们低估了其对我们的危险。不，我们不会和能激起或挑战我们最深刻的思想的深度图灵对话者约会——还不会。这将不可避免地到来，但在可见的未来里还不可见。借助语音、文本和 Watson 那样的数据库的智能代理可以实现非常先进的 Eliza，但不会更先进了。自动化运输、食物生产、建筑、协助家事将会极大地改变人们的生活方式和不动产的价值。除了这些泛泛之谈，本文的目的是收集一些关于芯片的思想见解——它们驱动着当前的神经网络革命。其中很多见解都不是最为激动人心的，但这对我来说是一个有用的锻炼。神经网络硬件与 20 年前相比，今天的神经处理方法并没有很大不同。深度更多的是一个品牌，而不是一项差异。激活函数已经得到了简化，以更好地适配硬件。主要的成功之处在于我们有了更多数据，对如何初始化权重、处理许多层、并行化和提升鲁棒性也有了更好的理解，其中要用到像是 dropout 这样的技术。1980 年的 Neocognitron 架构与今天的深度学习器或 CNN 并没有显著差异，但 Yann LeCun 让它具备了学习能力。在 90 年代那会儿也有很多神经硬件平台，比如 CNAPS（1990），它带有 64 个处理单元和 256kB 内存，可以在 8/16 位条件下达到 1.6 GCPS 的速度（CPS 是指每秒连接次数/ connections per second）或在 1 位条件下达到 12.8 GCPS 的速度。你可以在《神经硬件概述（Overview of neural hardware）》[Heemskerk, 1995, draft] 中读到 Synapse-1、CNAPS、SNAP、CNS Connectionist Supercomputer、Hitachi WSI、My-Neupower、LNeuro 1.0、UTAK1、GNU（通用神经单元/General Neural Unit）Implementation、UCL、Mantra 1、Biologically-Inspired Emulator、INPG Architecture、BACHUS 和 ZISC036。阅读地址：https://pdfs.semanticscholar.org/5841/73aa4886f87da4501571957c2b14a8fb9069.pdf好吧，东西还真多，但实际上还排除了软件和加速器板/CPU 组合，比如 ANZA plus、SAIC SIGMA-1、NT6000、Balboa 860 协处理器、Ni1000 识别加速器硬件（英特尔）、IBM NEP、NBC、Neuro Turbo I、Neuro Turbo II、WISARD、Mark II & IV、Sandy/8、GCN（索尼）、Topsi、BSP400（400 微处理器）、DREAM Machine、RAP、COKOS、REMAP、通用并行神经计算机（General Purpose Parallel Neurocomputer）、TI NETSIM 和 GeNet。另外还有一些模拟和混合模拟的实现，包括英特尔的电气式可训练模拟神经网络（801770NX）。你懂我要表达的意思了，那时候的东西还真是多。这在 1994 年迎来了一次爆发：乐观的摩尔定律告诉我们，TeraCPS 即将实现：「未来十年，微电子很可能将继续主导神经网络实现的领域。如果进展和过去进步得一样快，那就意味着神经计算机的性能将会增长大约 2 个数量级。因此，神经计算机将会接近 TeraCPS（10^12 CPS）的性能。由 100 万个节点（每个节点有大约 1000 个输入）组成的网络可以达到大脑的计算速度（100-1000 Hz）。这将能为实验合理巨大的网络提供良好的机会。」由于 Minsky 和 Papert 对隐藏层的不正确简单概括，打击了 Rosenblatt 的感知器梦想并最终导致了他不幸的死亡，神经网络研究遭遇了第一个冬天，研究资金被残酷地撤销了。1995 年，又出现了另一次神经网络冬天，尽管那时候我其实并不知道。作为温水锅里的一只青蛙，我没有注意到正在加热。第二个冬天的主要原因是缺乏激动人心的进展，让人们普遍感到无聊了。到了 2012 年，多亏了 Geoffrey Hinton 的冬季生存技能，多伦多大学基于 AlexNet 开发的 SuperVision 在 ImageNet 处理上实现了极大的提升，第二个神经网络冬天也由此终结了。之后谷歌的 LeNet Inception 模型在 2014 年打破了它的记录。所以据我估计，感知时代始于 2012 年。将它记在你的日历里面吧，五年已经过去了。谷歌在几千台普通机器上进行了出色的并行 CPU 有损更新研究。吴恩达教授和他的朋友们让数十台 GPU 就能完成数千台 CPU 的工作，从而让规模化成为了可能。因此，我们从需要很好的资助的神经处理前景中解放了出来。好吧，或多或少吧，现在最先进的网络有时候需要数千台 GPU 或专用芯片。更多数据和更多处理能力是其中的关键。让我们进入本文的重点，列出一些感知时代大数据之战中的一些关键平台：英伟达的 GPU这一家是很难被战胜的。来自大型视频处理市场的收益驱动着巨大的规模经济。新款英伟达 V100 带有一种新的 Tensor Core 架构，速度可达 15 TFlops（单精度/SP）或 120 TFlops（浮点精度，其中带有 FP16 的乘法和 FP32 的累加或加法，非常适合机器学习）。英伟达在它们的 DGX-1 中装入了 8 块计算卡，速度可达 960 Tensor TFlops.AMD 的 GPU在机器学习领域，AMD 一直是英伟达的追赶者。即将发布的 AMD Radeon Instinct MI25 有希望达到 12.3 TFlops（SP）或 24.6 TFlops（FP16）。如果你把英伟达的 Tensor Core 也算进来，则 AMD 完全无力竞争。英伟达设备的带宽 900GB/s 也是 AMD 484GB/s 的两倍。谷歌的 TPU谷歌原来的 TPU 相比于 GPU 有很大的领先，并帮助 DeepMind 的 AlphaGo 赢得了与李世石的围棋大战。据描述，原来的 700 MHz TPU 有 95 TFlops 的 8 位计算能力或 23 TFlops 的 16 位计算能力，同时功耗仅有 40W。这可比当时的 GPU 快得多，但现在落后于英伟达的 V100；但在单位功耗的计算能力上，TPU 并没落后。据称新的 TPU2 是一款带有 4 块芯片的 TPU 设备，速度可达到 180 TFlops 左右。每块芯片的性能都翻倍了，达到了 45 TFlops 的 16 位计算能力。你可以看到与英伟达 V100 的差距正在变小。你没法买到 TPU 或 TPU2。谷歌正在通过它们的云提供这些 TPU 服务，包含 64 台设备的 TPU pod 速度高达 11.5 PetaFlops。TPU2 上巨大的散热片说明了一些原因，但市场正在发生变化——从单独的设备转向了设备的组合以及将这些组合以云的形式提供。Wave ComputingWave 之父澳洲人 CTO Chris Nicol 博士的成果斐然，领导开发了 Wave 的 Compute Appliance 中的异步数据流处理器（asynchronous data flow processor）。几年前，Metamako 的创始人 Charles Thomas 在加州简单介绍了我和 Chris 认识。他们俩都曾在 NICTA 研究过无时钟异步。这两人都很出色。我不确定 Wave 的设备最早是不是针对机器学习设计的，但在他们的 3RU appliance 上运行 TensorFlow 的速度可以达到 2.9 PetaOPS/s，这实在了不起。Wave 将他们的处理器称为 DPU，一个 appliance 有 16 个 DPU。Wave 使用了他们称之为粗粒度可重构阵列（CGRA：Coarse Grained Reconfigurable Array）的处理元素。我还不清楚 2.9 PetaOPS/s 的速度对应多少位宽。根据他们的白皮书，其 ALU 可以执行 1 位、8 位、16 位和 32 位计算：「算术单元是分区的。它们可以并行执行 8 位运算（完美适用于 DNN 推理）以及 16 位和 32 位运算（或以上的任何组合）。也可以执行一些 64 位运算，而且可以使用软件扩展到任意精度。」关于其 appliance 中的 16 个 DPU，还有一些额外的信息：「Wave Computing DPU 是一种包含了 16384 PE 的 SoC，它们被配置成了一个 32×32 集群的 CGRA。它包含了 4 个 HMC（Hybrid Memory Cube）第二代接口、一个 PCIe 第三代 16 通道接口和一个用于 SoC 资源管理的嵌入式 32 位 RISC 微控制器。这款 Wave DPU 可以在没有主机 CPU 时自动执行。」对于 TensoFlow 指令：「Wave DNN Library 团队为 TensorFlow 等工作流程中所使用的常见 DNN 函数创建了预编译的可重新定位的 kernel。它们可以被组合到 Agent 中并且可以被实例化成机器，以构建大型的张量数据流图和 DNN kernel。」「……一个可与 TensorFlow、CNTK、Caffe 和 MXNet 等机器学习工作流程交互的 session 管理器，可作为用于训练和推理的工作器过程（worker process）。这些工作流程可为工作器过程提供张量的数据流图。在运行时，Wave 的 session 管理器会分析数据流图并将这些软件智能体放置到 DPU 芯片中，然后将它们连接起来以构建数据流图。这些软件智能体会被分配用于输入缓冲和本地存储的全局内存区域。CGRA kernel 的静态本质和分布式的内存架构可以让一个性能模型准确估计智能体的延迟。session 管理器可使用该性能模型来在智能体之间插入 FIFO 缓冲，这有助于 DPU 中通信和计算的重叠。这个可变智能体支持穿过整个图的数据流的软件流程，从而可以进一步增加并发性和性能。这个 session 管理器可以监控数据流图运行时的性能（通过监控卡顿、缓冲区下溢出和/或上溢出），并且可以动态地调节 FIFO 缓冲区的大小以实现吞吐量的最大化。在附加了 DPU 的处理器中，有一个分布式运行时管理系统会在运行时安装和卸载数据流图的一些部分，从而平衡计算和内存的使用量。这种在数据流计算机中的数据流图上的运行时重新配置还是有史以来第一次。」是的，我也觉得非常酷。这个平台的惊人之处是它在架构方面比 FPGA 更加粗粒度，因此灵活性更低，但却很可能表现更好。非常有意思。KnuEdge 的 KnuPath在 2016 年 6 月时我在 Twitter 上谈过 KnuPath。那以后他们的产品页面却失踪了。我不确定他们要把那 1 亿美元如何用到他们的 MIMD 架构上。那时候他们向我这样描述的：每个 ASIC 中有 256 个微型 DSP（即 tDSP）核以及一个 ARM 控制器，适用于 35W 包络中的稀疏矩阵处理。它的性能未知，但他们把自己的芯片与一款当时的英伟达芯片进行了比较，那时候他们说实现了 2.5 倍的性能。我们知道英伟达现在凭借 Tensor 内核已经提速了十倍以上，所以 KnuEdge 还要努力才能跟上节奏。MIMD 或 DSP 方法必须要得到非常好的效果才能在这一领域占据一席之地。时间会给我们答案。英特尔的 NervanaNervana Systems 曾经除了开发他们的 Nervana Engine ASIC，还开发着一种 GPU/软件方法，后来英特尔收购了这家公司。性能比较方面还不清楚。英特尔也在规划通过一个 Knights Crest 项目将其集成到 Phi 平台中。NextPlatform 认为其 2017 年在 28nm 节点上的目标是在某个位宽的运算速度达到 55 TOPS/s。英特尔还安排一个 NervanaCon，将在 12 月份举办，所以那时候我们也许将能看到他们的第一批成果。地平线机器人这家中国创业公司正在研发一种大脑处理单元（BPU：Brain Processing Unit）。余凯博士是正规军出身，他曾是百度深度学习研究院的负责人。今年早些时候，一段 YouTube 视频演示了基于 Arria 10 FPGA 的 BPU 仿真：https://youtu.be/GI9U9lUFaDo。目前关于这一平台的公开消息还很少。EyerissEyeriss 是 MIT 的一个项目，开发出了一款具有出色原始性能表现的 64nm ASIC。在 AlexNet 上，这款芯片的速度大约是英伟达 TK1 的一半。其优势在于借助于其行固定（row stationary）方法，仅需要一个 278mW 的可重新配置加速器就能实现这样中规中矩的性能。赞。Graphcore去年 Graphcore 拿到了 3000 万美元的 A 轮融资，以开发他们的智能处理单元（IPU： Intelligence Processing Unit）。他们的网站还缺少细节，只是给出了一些亮眼的事实，比如多于 14000 个独立的处理器线程和大于 100 倍的内存带宽。根据 NextPlatform 报道的零星信息，其在一块芯片上具有多于 1000 个真实内核，并且采用了定制的互连方式。它的 PCIe 板具有一个 16 个处理器的元件。听起来似乎是数据流。抛开公关的言论，这个团队确实有很强的背景，而且投资者也不并不傻，所以就拭目以待吧。TenstorrentTenstorrent 是加拿大多伦多的一家小创业公司，它宣称在深度学习的效率上实现了一个数量级的提升，和大多数公司一样，还没有什么公开的细节，但该公司入选了 Cognitive 300 榜单。CerebrasCerebras 值得一提，因为它得到了 Benchmark 的支持，而且其创始人是 SeaMicro 的 CEO。它似乎已经融资 2500 万美元了，而且仍然处于隐身模式。ThinciThinci 正在美国萨克拉门托开发视觉处理器，并且在印度也有员工。他们宣称即将推出他们的第一款硅芯片 Thinci-tc500，而且已经开始进行标准评测和赢得客户了。但除了「一切都并行处理」之外，我们所知甚少。KonikuKoniku 的网站正在倒计时，现在还有 20 几天。我已经等不及了。他们没有融到多少钱，而看过它们在福布斯上的这个视频之后（https://goo.gl/VA1PJx），你很可能也不得不相信他们，但你也无法预料究竟会如何。利用生物细胞肯定是不一样的。听起来就像是一个科研项目，但他们这样说：「我们是一个企业。我们不是一个科研项目。」下周将在维也纳的 Pioneers Festival 上发表演讲的 Agabi 这样说，「今天有些需求是硅所不能满足的，而我们可以通过我们的系统提供。」Koniku 提供的核心是所谓的神经元壳（neuron-shell），这家创业公司称其内部可以控制神经元彼此通信的方式，加上一种正在申请专利的电极，就可以在神经元上读写信息。所有这些都可以装在一个 iPad 大小的设备里，他们还希望能在 2018 年之前将其减小到一枚五美分硬币大小。AdaptevaAdapteva 是我最喜欢的一家小技术公司，正如你在之前文章《Adapteva tapes out Epiphany-V：一款 1024 核 64 位 RISC 处理器》：https://goo.gl/6ZH7JP。去年年底时 Andreas Olofsson 拿出了他的 1024 核芯片，我们都等着看它的性能表现。Epiphany-V 有用于深度学习的新指令，我们必须要看看这种带有 64MB 片上内存的更少内存控制器的设计是否具有合适的扩展能力。Andrea 的设计和构建的出色效率可能能让我们真正负担得起这种芯片，所以让我们希望它有良好的表现吧。KnowmKnown 研究的是 Anti-Hebbian and Hebbian（AHaH）可塑性和忆阻器。这里有篇覆盖这一主题的论文《AHaH 计算：从 Metastable Switches 到 Attractors 到机器学》：https://doi.org/10.1371/journal.pone.0085175。这对我来说有点太高级了。简单看了看，我看不出这项技术和胡言乱语的区别，但看起来确实科学味道十足。我需要亲眼看到才能相信。神经忆阻式处理器（neuromemristive processor）的思想是很有趣的。我早上确实需要一个好的流行术语。MythicMythic 的一款电池驱动的神经芯片具有低 50 倍的功耗。目前还看不到太多真正的细节。这款芯片大约纽扣大小，但大多数芯片不都这样吗？「Mythic 的平台能在纽扣大小的芯片上提供桌面 GPU 的性能。」也许这又是一款适合无人机和手机的芯片，很可能被用在手机中，也可能被排除在外。高通手机显然是机器学习硬件的一大用武之地。我们希望能够识别狗的品种、花朵、树叶、癌症痣、翻译标识、理解口语等等。我们口袋里的超级计算机愿意用上它能得到的所有帮助，以便能迈入感知时代。高通一直以来都在鼓捣机器学习，推出了 Zeroth SDK 和 Snapdragon 神经处理引擎（NPE）。这种 NPE 显然在高通所用的 Hexagon DSP 上效果良好。Hexagon DSP 已经远远不止是一个非常广泛的并行平台了，Yann LeCun 已经证实高通和 Facebook 正在合作开发一种更好的方法，参见 Wired 的文章《业界 | 谷歌 TPU 之后还有高通，人工智能芯片竞赛已经展开》：「最近，高通已经开始制造执行神经网络的专用芯片，这条消息来自 LeCun，因为 Facebook 正帮助高通开发机器学习相关技术，所以他对高通的计划很了解；高通技术副总裁 Jeff Gehlhaar 证实了这个项目，他说：『在原型设计和开发方面，我们还有很长的路要走。』」也许我们很快就会看到 Kryo CPU、Adreno GPU、Hexagon DSP 和 Hexagon Vector Extensions 之外的其它东西。对于这一领域的创业公司来说，和高通的机器学习竞争将会艰难无比。Pezy-SC 和 Pezy-SC2这两者是 Pezy 开发的 1024 核和 2048 核处理器。Pezy-SC 1024 核芯片可以驱动 2015 年 Green500 超级计算机榜单的前三名的系统。Pezy-SC2 是现在已经开始提供的后续芯片，我在 6 月份也做过关于它的演讲，但相关细节还很少，不过仍然很吸引人：「PEZY-SC2 HPC Brick：单个机箱中有 32 个带有 64GB DDR4 DIMM（2.1 PetaFLOPS（DP））的 Pezy-SC2 模块卡，速度可达 6.4 Tb/s.」不知道 2048 个 MIMD MIPS Warrior 64 位核的机器能做到什么？在 6 月份的 2017 年 Green500 榜单中，一个英伟达 P100 系统拿到了头把交椅，而排名第 7 的是一个 Pezy-SC2 系统。所以看起来这款芯片还活着，但相关细节却很少。Motoaki Saito（齊藤元章）当然值得一看。Kalray尽管做了很多承诺，但 Kalray 的芯片还没有超过 256 核，我在 2015 年的一篇文章就谈到：https://goo.gl/pxqn7Z。Kalray 宣传自己的产品说是适合嵌入式自动驾驶汽车应用，但我觉得其目前产品架构的形式并不是一种完美的 CNN 平台。Kalray 有一个 Kalray Neural Network（KaNN）软件包并且宣称有比 GPU 更好的效率，在芯片上能实现高达 1 TFlop/s 的速度。随着即将到来的产品更新，Kalray 的神经网络财富可能还会提升，就在这个月 Kalray 完成了新一轮 2600 万美元的融资。他们新的 Coolidge 处理器预计将在 2018 年年中上市，它会带有 80 或 160 个核，另外还有 80 或 160 个为视觉和深度学习优化的协处理器。这在他们的多于 1000 核的方法上有了很大的改变，而我认为这是最明智的。IBM TrueNorthTrueNorth 是 IBM 的神经形态 CMOS ASIC，是与 DARPA 的 StNAPSE 项目一起开发的。这是一种单个芯片设计上的多核处理器网络，具有 4096 个核，每个核模拟 256 个可编程的硅「神经元」，总共就超过了 100 万个神经元。另外，每个神经元还有 256 个可编程的「突触」，信号可以在这些突触之间传递。因此，可编程突触的总是超过了 2.68 亿（2^28）。在基本构建模块方面，它的晶体管数量为 54 亿。因为存储、计算和通信都是在这 4096 个神经突触核中各自处理的，所以 TrueNorth 避开了冯·诺依曼架构的瓶颈，而且能效非常高，功耗为 70 mW，大约是传统微处理器的功率密度的万分之一（来自维基百科）。IBM 之前还在批评脉冲神经网络（spiking neural network）无法适配深度学习，现在 IBM 开发了一种在 TureNorth 上运行 CNN 的新算法。这些神经元不会在每个周期内都放电，脉冲神经网络中的神经元必须逐渐积累它们的电势，然后才能放电……深度学习专家普遍认为脉冲神经网络在深度学习上没有效率——至少和卷积神经网络比起来是这样。Facebook 人工智能研究院主任和深度学习先驱 Yann LeCun 曾经批评过 IBM 的 TureNorth 芯片，因为它主要支持脉冲神经网络…………这种神经形态芯片不会太激动人心，因为它们注重的脉冲神经网络在深度学习领域并不受欢迎。为了让 TrueNorth 芯片很好地契合深度学习，IBM 不得不开发了一种新算法，让卷积神经网络可以在这款神经形态计算硬件上很好地运行。这种组合方法实现了 IBM 所谓的「接近当前最佳」的分类准确度，实验涉及了视觉和语音挑战赛方面的 8 个数据集。在最好的情况下，它们的准确度达到了 65% 到 97%。当只使用一块 TrueNorth 芯片时，它只在这 8 个数据集中的 1 个数据集上超越了当前最佳的准确度。但如果使用多达 8 块芯片，IBM 的研究者可以极大提升这款硬件在深度学习挑战上的准确度。这使 TureNorth 可以在其中 3 个数据集上媲美或超越当前最佳的准确度。这个 TureNorth 测试也做到了每秒处理 1200 到 2600 视频帧。这意味着单个 TureNorth 芯片就能实时检测来自多达 100 个同时工作的相机的数据的模式……（来自 IEEE Spectrum）TrueNorth 的功率效率非常出色，所以非常值得考虑。Brainchip 的脉冲神经元自适应处理器（SNAP：Spiking Neuron Adaptive Processor）SNAP 不能做深度学习，这只是一个源于好奇心的项目，还没有实际落地成 CNN 工程解决方案，至少现在还没有。如果你想探索这条路，IBM 的随机相变神经元（stochastic phase-change neurons）似乎更有意思。苹果的神经引擎（Neural Engine）到底会不会有？彭博社报道称这会是一款次级处理器，但没有什么细节信息。对苹果来说，这不仅是一个重要领域，而且也有助于与高通的竞争。其它1. 寒武纪（Cambricon）——中国科学院在芯片上投入的 140 万美元。它是一个用于神经网络的指令集架构，具有数据级的并行、自定义向量/矩阵指令、片上 scratchpad memory。宣称速度是 x86 CPU 的 91 倍，是 K40M 的 3 倍，且峰值功率仅有 1%，即 1.695W。参阅这两篇论文：Cambricon-X：一个用于稀疏神经网络的加速器：http://cslt.riit.tsinghua.edu.cn/mediawiki/images/f/f1/Cambricon-X.pdfCambricon：一个用于神经网络的指令集架构：http://dl.acm.org/citation.cfm?id=30011792. 前谷歌员工创立的 Groq Inc.，也许是另一种 TPU？3. Aimotive：https://aimotive.com/4. Deep Vision 正在开发用于深度学习的低功耗芯片，也许这两篇创始人的论文能提供一点线索：Convolution Engine: Balancing Efficiency &amp; Flexibility in Specialized Computing [2013]：http://csl.stanford.edu/~christos/publications/2013.convolution.isca.pdfConvolution Engine: Balancing Efficiency and Flexibility in Specialized Computing [2015]：http://csl.stanford.edu/~christos/publications/2015.convolution_engine.cacm.pdf5. DeepScale&nbsp;6. Reduced Energy Microsystems 正在开发用于 CNN 推理的低功耗异步芯片。据 TechCrunch 报道，REM 是 Y Combinator 第一个 ASIC 领域的风险投资。7. Leapmind 也很忙。FPGA微软已经站队 FPGA 了。Wired 这篇文章说得很好：《深度 |《连线》长文揭秘微软 Project Catapult：人工智能时代押注 FPGA》。「Bing 占据着世界上 20% 的桌面搜索市场和 6% 的移动手机市场，在 Bing 上，这个芯片能帮助 Bing 适应新品种的人工智能：深度神经网络。」我对这种方法也有些兴趣。赛灵思和英特尔（收购了 Altera）的 FPGA 是很强大的引擎。赛灵思自然宣称他们的 FPGA 是对 INT8 最好的，他们的一份白皮书里面包含了下面的幻灯片：这两家供应商都很支持使用他们的 FPGA 做机器学习：Xilinx - Acceleration Zone：https://goo.gl/KheG5WIntel FPGA OpenCL（https://goo.gl/S62fMA）和 Solutions（https://goo.gl/zkYyXB）尽管 FPGA 单位功耗的性能是很出色的，但这些供应商的更大型的芯片的售价却长时间以来都高得吓人。赛灵思的 VU9P 在 Avnet 上的售价超过 5 万美元。寻找价格和能力之间的平衡点是 FPGA 的主要难题。FPGA 方法的一大优势是可以用来制定一些非常出色的架构决策。比如如果因为 HBM，你想在板上 DRAM 压缩并且实时地解压，然后希望借此改善你的内存的浮点数据流，如果你足够努力，你能找到解决方案。参阅《用于基于 FGPA 的高性能计算的浮点数值数据流的带宽压缩（Bandwidth Compression of Floating-Point Numerical Data Streams for FPGA-Based High-Performance Computing）》：http://dl.acm.org/citation.cfm?id=3053688。这种动态的架构敏捷性（architectural agility）很困难，几乎无法用其它任何方法实现。架构选择太多可能也是一个问题，但我自己还是挺喜欢这个问题的。这篇论文很不错《使用 TILT 减少 Soft Scalar CPU 和定制硬件之间的性能差距（Reducing the Performance Gap between Soft Scalar CPUs and Custom Hardware with TILT）》：http://dl.acm.org/citation.cfm?id=3079757，其中研究了定制硬件和 FPGA 处理器（带有基于 FPGA 的水平微编码的计算引擎）的性能差距，这让人想起了古老的 DISC 和很多个月前的离散指令集计算机（discrete instruction set computer）。谁是赢家？在这样的竞争中，预测谁是赢家是傻瓜的举动。高通凭借其手机市场的主导地位，可以轻松进入赢家榜单。苹果不管做什么都会成功的。英伟达的 V100 有 Tensor 单元，也基本上赢了。我不确定我能否看到谷歌的 TPU 在无休止的长期硅谷竞赛中幸存下来，尽管目前它的性能很出色。我很喜欢 FPGA 方法，但我也不禁想他们应该以远远更低的价格发布 DNN 版本，这样才不会被大众漠视。英特尔和 AMD 会做自己的协处理器。因为所有主要玩家都参战了，所以其中很多都会支持标准的工具包，比如 TensorFlow，这样我们就不必太在意规格了，关心基准就可以了。在更小型的玩家中，我很喜欢而且支持的是 Adapteva 方法，我认为他们的内存架构可能并不适合 DNN。我希望我错了。Wave Computing 可能是我继 FPGA 之后最喜欢的方法。他们的整个异步数据流方法是非常棒的。REM 似乎也在做类似的事情；但我认为他们可能太迟了。Wave Computing 能否在面对所有竞争对手时继续保持领先吗？也许只要他们的异步 CGRA 有一个本质优势，就可以。尽管我不确定他们是否只要 DNN 上的成功，因为他们的技术有更加广泛的应用能力。神经形态脉冲处理器也许现在暂时可以忽略，但也可以保持关注，因为它们有很大的功耗优势。量子计算反正会让所有这些都稍逊风骚。IBM 的 TrueNorth 可能会是个例外，因为它不仅能执行脉冲网络，还能有效地运行 DNN。&nbsp;原文链接：https://meanderful.blogspot.jp/2017/06/fpgas-and-ai-processors-dnn-and-cnn-for.html​"
"平台;;从GPU、TPU到FPGA及其它：一文读懂神经网络硬件平台战局在如今深度学习大爆发的时代，相关的硬件平台也在百花齐放，既有英伟达和谷歌这样的科技巨头，也有地平线机器人和 Graphcore 等创业公司——它们都各自提出了自己的解决方案。近日，多家公司的技术顾问 Matt Hurd 在其博客上发表了一篇全面评点各种神经网络硬件平台的长文，机器之心对本文进行了编译介绍。这是我几周前做的一个传统的 90 年代风格的性别识别神经网络的很好的隐藏节点。一个简单的性别识别器网络中的 90 年代风格的隐藏节点图像我的硕士项目是一种类似级联相关（cascade correlation）的神经网络 Multi-rate Optimising Order Statistic Equaliser（MOOSE：多速率优化顺序统计均衡器），可用于日内的 Bund（国库债券产品）交易。MOOSE 曾经是为获取高速的 LEO 卫星信号（McCaw 的 Teledesic）而设计的一点成果，后来在从 LIFFE 迁移到 DTB 时将目标转向了 Bund。作为一家投资银行的职业交易员，我可以购买很好的工具。我有那时候世界上最快的计算机：一个 IBM MicroChannel dual Pentium Pro 200MHz 处理器外加带有几 MB RAM 的 SCSI。在 1994 年那会儿，将 800,000 个数据点输入到我的 C++ stream/dag 处理器中看上去就像是黑魔法。有限差分方法让我可以做许多 O(1) 增量线性回归这样的运算，以获得 1000 倍的加速。那时候这看起来已经很好了。现在，你的手机都能嘲笑我的大方向。那时候，在神经网络领域有很多研究。倒不是说它有过人的生产力，只是因为有用。读到 Lindsay Fortado 和 Robin Wigglesworth 的 FT 文章《Machine learning set to shake up equity hedge funds》中 Eric Schmidt 关于机器学习和交易的看法，还真有点让人高兴：Eric Schmidt 是谷歌母公司 Alphabet 的执行董事长，他上周对一众对冲基金经理说他相信在 50 年内，所有交易都会有计算机解读数据和市场信号。「我期待出现在交易方面做机器学习的创业公司，看看我描述的这种模式识别能否比数据分析专家的传统线性回归算法做得更好。」他补充说，「我这个行业内的许多人都认为这注定将成为一种新的交易形式。」老朋友 Eric，我在 90 年代早期就已经算是迟到了，你真是有点后知后觉。好吧，现在情况已经不同了。我喜欢思考它，并喜欢将神经网络的这次新复兴称为感知时代（The Age of Perception）。这并不是智能，只是擅长模式而已。它仍然无力应对语言歧义。它还要一些时间才能理解基本的价值和概念，从而形成深刻的金融理解。深度学习既被夸大了，也被低估了。这不是智能，但会有助于帮我们实现智能。一些人将其夸大为将会给我们带来替代人的类人机器的人工智能突破。我们仍然还受困于常识以及用于推理的简单文本中的歧义。我们还有很长的路要走。相对简单的规划算法和启发式方法以及基于神奇的深度学习的视觉、声音、文本、雷达等等的感知能力将会带来深刻的影响，就像是每个人和他们的狗现在理解的那样。所以我叫它「感知时代」。就好像是我们口袋里的超级计算机突然有了眼睛，并且快速适应了真实世界所带来的闪光性致盲。深度学习将会带来巨大的影响，并且将会改变这颗行星上全人类的生活方式。但我们低估了其对我们的危险。不，我们不会和能激起或挑战我们最深刻的思想的深度图灵对话者约会——还不会。这将不可避免地到来，但在可见的未来里还不可见。借助语音、文本和 Watson 那样的数据库的智能代理可以实现非常先进的 Eliza，但不会更先进了。自动化运输、食物生产、建筑、协助家事将会极大地改变人们的生活方式和不动产的价值。除了这些泛泛之谈，本文的目的是收集一些关于芯片的思想见解——它们驱动着当前的神经网络革命。其中很多见解都不是最为激动人心的，但这对我来说是一个有用的锻炼。神经网络硬件与 20 年前相比，今天的神经处理方法并没有很大不同。深度更多的是一个品牌，而不是一项差异。激活函数已经得到了简化，以更好地适配硬件。主要的成功之处在于我们有了更多数据，对如何初始化权重、处理许多层、并行化和提升鲁棒性也有了更好的理解，其中要用到像是 dropout 这样的技术。1980 年的 Neocognitron 架构与今天的深度学习器或 CNN 并没有显著差异，但 Yann LeCun 让它具备了学习能力。在 90 年代那会儿也有很多神经硬件平台，比如 CNAPS（1990），它带有 64 个处理单元和 256kB 内存，可以在 8/16 位条件下达到 1.6 GCPS 的速度（CPS 是指每秒连接次数/ connections per second）或在 1 位条件下达到 12.8 GCPS 的速度。你可以在《神经硬件概述（Overview of neural hardware）》[Heemskerk, 1995, draft] 中读到 Synapse-1、CNAPS、SNAP、CNS Connectionist Supercomputer、Hitachi WSI、My-Neupower、LNeuro 1.0、UTAK1、GNU（通用神经单元/General Neural Unit）Implementation、UCL、Mantra 1、Biologically-Inspired Emulator、INPG Architecture、BACHUS 和 ZISC036。阅读地址：https://pdfs.semanticscholar.org/5841/73aa4886f87da4501571957c2b14a8fb9069.pdf好吧，东西还真多，但实际上还排除了软件和加速器板/CPU 组合，比如 ANZA plus、SAIC SIGMA-1、NT6000、Balboa 860 协处理器、Ni1000 识别加速器硬件（英特尔）、IBM NEP、NBC、Neuro Turbo I、Neuro Turbo II、WISARD、Mark II & IV、Sandy/8、GCN（索尼）、Topsi、BSP400（400 微处理器）、DREAM Machine、RAP、COKOS、REMAP、通用并行神经计算机（General Purpose Parallel Neurocomputer）、TI NETSIM 和 GeNet。另外还有一些模拟和混合模拟的实现，包括英特尔的电气式可训练模拟神经网络（801770NX）。你懂我要表达的意思了，那时候的东西还真是多。这在 1994 年迎来了一次爆发：乐观的摩尔定律告诉我们，TeraCPS 即将实现：「未来十年，微电子很可能将继续主导神经网络实现的领域。如果进展和过去进步得一样快，那就意味着神经计算机的性能将会增长大约 2 个数量级。因此，神经计算机将会接近 TeraCPS（10^12 CPS）的性能。由 100 万个节点（每个节点有大约 1000 个输入）组成的网络可以达到大脑的计算速度（100-1000 Hz）。这将能为实验合理巨大的网络提供良好的机会。」由于 Minsky 和 Papert 对隐藏层的不正确简单概括，打击了 Rosenblatt 的感知器梦想并最终导致了他不幸的死亡，神经网络研究遭遇了第一个冬天，研究资金被残酷地撤销了。1995 年，又出现了另一次神经网络冬天，尽管那时候我其实并不知道。作为温水锅里的一只青蛙，我没有注意到正在加热。第二个冬天的主要原因是缺乏激动人心的进展，让人们普遍感到无聊了。到了 2012 年，多亏了 Geoffrey Hinton 的冬季生存技能，多伦多大学基于 AlexNet 开发的 SuperVision 在 ImageNet 处理上实现了极大的提升，第二个神经网络冬天也由此终结了。之后谷歌的 LeNet Inception 模型在 2014 年打破了它的记录。所以据我估计，感知时代始于 2012 年。将它记在你的日历里面吧，五年已经过去了。谷歌在几千台普通机器上进行了出色的并行 CPU 有损更新研究。吴恩达教授和他的朋友们让数十台 GPU 就能完成数千台 CPU 的工作，从而让规模化成为了可能。因此，我们从需要很好的资助的神经处理前景中解放了出来。好吧，或多或少吧，现在最先进的网络有时候需要数千台 GPU 或专用芯片。更多数据和更多处理能力是其中的关键。让我们进入本文的重点，列出一些感知时代大数据之战中的一些关键平台：英伟达的 GPU这一家是很难被战胜的。来自大型视频处理市场的收益驱动着巨大的规模经济。新款英伟达 V100 带有一种新的 Tensor Core 架构，速度可达 15 TFlops（单精度/SP）或 120 TFlops（浮点精度，其中带有 FP16 的乘法和 FP32 的累加或加法，非常适合机器学习）。英伟达在它们的 DGX-1 中装入了 8 块计算卡，速度可达 960 Tensor TFlops.AMD 的 GPU在机器学习领域，AMD 一直是英伟达的追赶者。即将发布的 AMD Radeon Instinct MI25 有希望达到 12.3 TFlops（SP）或 24.6 TFlops（FP16）。如果你把英伟达的 Tensor Core 也算进来，则 AMD 完全无力竞争。英伟达设备的带宽 900GB/s 也是 AMD 484GB/s 的两倍。谷歌的 TPU谷歌原来的 TPU 相比于 GPU 有很大的领先，并帮助 DeepMind 的 AlphaGo 赢得了与李世石的围棋大战。据描述，原来的 700 MHz TPU 有 95 TFlops 的 8 位计算能力或 23 TFlops 的 16 位计算能力，同时功耗仅有 40W。这可比当时的 GPU 快得多，但现在落后于英伟达的 V100；但在单位功耗的计算能力上，TPU 并没落后。据称新的 TPU2 是一款带有 4 块芯片的 TPU 设备，速度可达到 180 TFlops 左右。每块芯片的性能都翻倍了，达到了 45 TFlops 的 16 位计算能力。你可以看到与英伟达 V100 的差距正在变小。你没法买到 TPU 或 TPU2。谷歌正在通过它们的云提供这些 TPU 服务，包含 64 台设备的 TPU pod 速度高达 11.5 PetaFlops。TPU2 上巨大的散热片说明了一些原因，但市场正在发生变化——从单独的设备转向了设备的组合以及将这些组合以云的形式提供。Wave ComputingWave 之父澳洲人 CTO Chris Nicol 博士的成果斐然，领导开发了 Wave 的 Compute Appliance 中的异步数据流处理器（asynchronous data flow processor）。几年前，Metamako 的创始人 Charles Thomas 在加州简单介绍了我和 Chris 认识。他们俩都曾在 NICTA 研究过无时钟异步。这两人都很出色。我不确定 Wave 的设备最早是不是针对机器学习设计的，但在他们的 3RU appliance 上运行 TensorFlow 的速度可以达到 2.9 PetaOPS/s，这实在了不起。Wave 将他们的处理器称为 DPU，一个 appliance 有 16 个 DPU。Wave 使用了他们称之为粗粒度可重构阵列（CGRA：Coarse Grained Reconfigurable Array）的处理元素。我还不清楚 2.9 PetaOPS/s 的速度对应多少位宽。根据他们的白皮书，其 ALU 可以执行 1 位、8 位、16 位和 32 位计算：「算术单元是分区的。它们可以并行执行 8 位运算（完美适用于 DNN 推理）以及 16 位和 32 位运算（或以上的任何组合）。也可以执行一些 64 位运算，而且可以使用软件扩展到任意精度。」关于其 appliance 中的 16 个 DPU，还有一些额外的信息：「Wave Computing DPU 是一种包含了 16384 PE 的 SoC，它们被配置成了一个 32×32 集群的 CGRA。它包含了 4 个 HMC（Hybrid Memory Cube）第二代接口、一个 PCIe 第三代 16 通道接口和一个用于 SoC 资源管理的嵌入式 32 位 RISC 微控制器。这款 Wave DPU 可以在没有主机 CPU 时自动执行。」对于 TensoFlow 指令：「Wave DNN Library 团队为 TensorFlow 等工作流程中所使用的常见 DNN 函数创建了预编译的可重新定位的 kernel。它们可以被组合到 Agent 中并且可以被实例化成机器，以构建大型的张量数据流图和 DNN kernel。」「……一个可与 TensorFlow、CNTK、Caffe 和 MXNet 等机器学习工作流程交互的 session 管理器，可作为用于训练和推理的工作器过程（worker process）。这些工作流程可为工作器过程提供张量的数据流图。在运行时，Wave 的 session 管理器会分析数据流图并将这些软件智能体放置到 DPU 芯片中，然后将它们连接起来以构建数据流图。这些软件智能体会被分配用于输入缓冲和本地存储的全局内存区域。CGRA kernel 的静态本质和分布式的内存架构可以让一个性能模型准确估计智能体的延迟。session 管理器可使用该性能模型来在智能体之间插入 FIFO 缓冲，这有助于 DPU 中通信和计算的重叠。这个可变智能体支持穿过整个图的数据流的软件流程，从而可以进一步增加并发性和性能。这个 session 管理器可以监控数据流图运行时的性能（通过监控卡顿、缓冲区下溢出和/或上溢出），并且可以动态地调节 FIFO 缓冲区的大小以实现吞吐量的最大化。在附加了 DPU 的处理器中，有一个分布式运行时管理系统会在运行时安装和卸载数据流图的一些部分，从而平衡计算和内存的使用量。这种在数据流计算机中的数据流图上的运行时重新配置还是有史以来第一次。」是的，我也觉得非常酷。这个平台的惊人之处是它在架构方面比 FPGA 更加粗粒度，因此灵活性更低，但却很可能表现更好。非常有意思。KnuEdge 的 KnuPath在 2016 年 6 月时我在 Twitter 上谈过 KnuPath。那以后他们的产品页面却失踪了。我不确定他们要把那 1 亿美元如何用到他们的 MIMD 架构上。那时候他们向我这样描述的：每个 ASIC 中有 256 个微型 DSP（即 tDSP）核以及一个 ARM 控制器，适用于 35W 包络中的稀疏矩阵处理。它的性能未知，但他们把自己的芯片与一款当时的英伟达芯片进行了比较，那时候他们说实现了 2.5 倍的性能。我们知道英伟达现在凭借 Tensor 内核已经提速了十倍以上，所以 KnuEdge 还要努力才能跟上节奏。MIMD 或 DSP 方法必须要得到非常好的效果才能在这一领域占据一席之地。时间会给我们答案。英特尔的 NervanaNervana Systems 曾经除了开发他们的 Nervana Engine ASIC，还开发着一种 GPU/软件方法，后来英特尔收购了这家公司。性能比较方面还不清楚。英特尔也在规划通过一个 Knights Crest 项目将其集成到 Phi 平台中。NextPlatform 认为其 2017 年在 28nm 节点上的目标是在某个位宽的运算速度达到 55 TOPS/s。英特尔还安排一个 NervanaCon，将在 12 月份举办，所以那时候我们也许将能看到他们的第一批成果。地平线机器人这家中国创业公司正在研发一种大脑处理单元（BPU：Brain Processing Unit）。余凯博士是正规军出身，他曾是百度深度学习研究院的负责人。今年早些时候，一段 YouTube 视频演示了基于 Arria 10 FPGA 的 BPU 仿真：https://youtu.be/GI9U9lUFaDo。目前关于这一平台的公开消息还很少。EyerissEyeriss 是 MIT 的一个项目，开发出了一款具有出色原始性能表现的 64nm ASIC。在 AlexNet 上，这款芯片的速度大约是英伟达 TK1 的一半。其优势在于借助于其行固定（row stationary）方法，仅需要一个 278mW 的可重新配置加速器就能实现这样中规中矩的性能。赞。Graphcore去年 Graphcore 拿到了 3000 万美元的 A 轮融资，以开发他们的智能处理单元（IPU： Intelligence Processing Unit）。他们的网站还缺少细节，只是给出了一些亮眼的事实，比如多于 14000 个独立的处理器线程和大于 100 倍的内存带宽。根据 NextPlatform 报道的零星信息，其在一块芯片上具有多于 1000 个真实内核，并且采用了定制的互连方式。它的 PCIe 板具有一个 16 个处理器的元件。听起来似乎是数据流。抛开公关的言论，这个团队确实有很强的背景，而且投资者也不并不傻，所以就拭目以待吧。TenstorrentTenstorrent 是加拿大多伦多的一家小创业公司，它宣称在深度学习的效率上实现了一个数量级的提升，和大多数公司一样，还没有什么公开的细节，但该公司入选了 Cognitive 300 榜单。CerebrasCerebras 值得一提，因为它得到了 Benchmark 的支持，而且其创始人是 SeaMicro 的 CEO。它似乎已经融资 2500 万美元了，而且仍然处于隐身模式。ThinciThinci 正在美国萨克拉门托开发视觉处理器，并且在印度也有员工。他们宣称即将推出他们的第一款硅芯片 Thinci-tc500，而且已经开始进行标准评测和赢得客户了。但除了「一切都并行处理」之外，我们所知甚少。KonikuKoniku 的网站正在倒计时，现在还有 20 几天。我已经等不及了。他们没有融到多少钱，而看过它们在福布斯上的这个视频之后（https://goo.gl/VA1PJx），你很可能也不得不相信他们，但你也无法预料究竟会如何。利用生物细胞肯定是不一样的。听起来就像是一个科研项目，但他们这样说：「我们是一个企业。我们不是一个科研项目。」下周将在维也纳的 Pioneers Festival 上发表演讲的 Agabi 这样说，「今天有些需求是硅所不能满足的，而我们可以通过我们的系统提供。」Koniku 提供的核心是所谓的神经元壳（neuron-shell），这家创业公司称其内部可以控制神经元彼此通信的方式，加上一种正在申请专利的电极，就可以在神经元上读写信息。所有这些都可以装在一个 iPad 大小的设备里，他们还希望能在 2018 年之前将其减小到一枚五美分硬币大小。AdaptevaAdapteva 是我最喜欢的一家小技术公司，正如你在之前文章《Adapteva tapes out Epiphany-V：一款 1024 核 64 位 RISC 处理器》：https://goo.gl/6ZH7JP。去年年底时 Andreas Olofsson 拿出了他的 1024 核芯片，我们都等着看它的性能表现。Epiphany-V 有用于深度学习的新指令，我们必须要看看这种带有 64MB 片上内存的更少内存控制器的设计是否具有合适的扩展能力。Andrea 的设计和构建的出色效率可能能让我们真正负担得起这种芯片，所以让我们希望它有良好的表现吧。KnowmKnown 研究的是 Anti-Hebbian and Hebbian（AHaH）可塑性和忆阻器。这里有篇覆盖这一主题的论文《AHaH 计算：从 Metastable Switches 到 Attractors 到机器学》：https://doi.org/10.1371/journal.pone.0085175。这对我来说有点太高级了。简单看了看，我看不出这项技术和胡言乱语的区别，但看起来确实科学味道十足。我需要亲眼看到才能相信。神经忆阻式处理器（neuromemristive processor）的思想是很有趣的。我早上确实需要一个好的流行术语。MythicMythic 的一款电池驱动的神经芯片具有低 50 倍的功耗。目前还看不到太多真正的细节。这款芯片大约纽扣大小，但大多数芯片不都这样吗？「Mythic 的平台能在纽扣大小的芯片上提供桌面 GPU 的性能。」也许这又是一款适合无人机和手机的芯片，很可能被用在手机中，也可能被排除在外。高通手机显然是机器学习硬件的一大用武之地。我们希望能够识别狗的品种、花朵、树叶、癌症痣、翻译标识、理解口语等等。我们口袋里的超级计算机愿意用上它能得到的所有帮助，以便能迈入感知时代。高通一直以来都在鼓捣机器学习，推出了 Zeroth SDK 和 Snapdragon 神经处理引擎（NPE）。这种 NPE 显然在高通所用的 Hexagon DSP 上效果良好。Hexagon DSP 已经远远不止是一个非常广泛的并行平台了，Yann LeCun 已经证实高通和 Facebook 正在合作开发一种更好的方法，参见 Wired 的文章《业界 | 谷歌 TPU 之后还有高通，人工智能芯片竞赛已经展开》：「最近，高通已经开始制造执行神经网络的专用芯片，这条消息来自 LeCun，因为 Facebook 正帮助高通开发机器学习相关技术，所以他对高通的计划很了解；高通技术副总裁 Jeff Gehlhaar 证实了这个项目，他说：『在原型设计和开发方面，我们还有很长的路要走。』」也许我们很快就会看到 Kryo CPU、Adreno GPU、Hexagon DSP 和 Hexagon Vector Extensions 之外的其它东西。对于这一领域的创业公司来说，和高通的机器学习竞争将会艰难无比。Pezy-SC 和 Pezy-SC2这两者是 Pezy 开发的 1024 核和 2048 核处理器。Pezy-SC 1024 核芯片可以驱动 2015 年 Green500 超级计算机榜单的前三名的系统。Pezy-SC2 是现在已经开始提供的后续芯片，我在 6 月份也做过关于它的演讲，但相关细节还很少，不过仍然很吸引人：「PEZY-SC2 HPC Brick：单个机箱中有 32 个带有 64GB DDR4 DIMM（2.1 PetaFLOPS（DP））的 Pezy-SC2 模块卡，速度可达 6.4 Tb/s.」不知道 2048 个 MIMD MIPS Warrior 64 位核的机器能做到什么？在 6 月份的 2017 年 Green500 榜单中，一个英伟达 P100 系统拿到了头把交椅，而排名第 7 的是一个 Pezy-SC2 系统。所以看起来这款芯片还活着，但相关细节却很少。Motoaki Saito（齊藤元章）当然值得一看。Kalray尽管做了很多承诺，但 Kalray 的芯片还没有超过 256 核，我在 2015 年的一篇文章就谈到：https://goo.gl/pxqn7Z。Kalray 宣传自己的产品说是适合嵌入式自动驾驶汽车应用，但我觉得其目前产品架构的形式并不是一种完美的 CNN 平台。Kalray 有一个 Kalray Neural Network（KaNN）软件包并且宣称有比 GPU 更好的效率，在芯片上能实现高达 1 TFlop/s 的速度。随着即将到来的产品更新，Kalray 的神经网络财富可能还会提升，就在这个月 Kalray 完成了新一轮 2600 万美元的融资。他们新的 Coolidge 处理器预计将在 2018 年年中上市，它会带有 80 或 160 个核，另外还有 80 或 160 个为视觉和深度学习优化的协处理器。这在他们的多于 1000 核的方法上有了很大的改变，而我认为这是最明智的。IBM TrueNorthTrueNorth 是 IBM 的神经形态 CMOS ASIC，是与 DARPA 的 StNAPSE 项目一起开发的。这是一种单个芯片设计上的多核处理器网络，具有 4096 个核，每个核模拟 256 个可编程的硅「神经元」，总共就超过了 100 万个神经元。另外，每个神经元还有 256 个可编程的「突触」，信号可以在这些突触之间传递。因此，可编程突触的总是超过了 2.68 亿（2^28）。在基本构建模块方面，它的晶体管数量为 54 亿。因为存储、计算和通信都是在这 4096 个神经突触核中各自处理的，所以 TrueNorth 避开了冯·诺依曼架构的瓶颈，而且能效非常高，功耗为 70 mW，大约是传统微处理器的功率密度的万分之一（来自维基百科）。IBM 之前还在批评脉冲神经网络（spiking neural network）无法适配深度学习，现在 IBM 开发了一种在 TureNorth 上运行 CNN 的新算法。这些神经元不会在每个周期内都放电，脉冲神经网络中的神经元必须逐渐积累它们的电势，然后才能放电……深度学习专家普遍认为脉冲神经网络在深度学习上没有效率——至少和卷积神经网络比起来是这样。Facebook 人工智能研究院主任和深度学习先驱 Yann LeCun 曾经批评过 IBM 的 TureNorth 芯片，因为它主要支持脉冲神经网络…………这种神经形态芯片不会太激动人心，因为它们注重的脉冲神经网络在深度学习领域并不受欢迎。为了让 TrueNorth 芯片很好地契合深度学习，IBM 不得不开发了一种新算法，让卷积神经网络可以在这款神经形态计算硬件上很好地运行。这种组合方法实现了 IBM 所谓的「接近当前最佳」的分类准确度，实验涉及了视觉和语音挑战赛方面的 8 个数据集。在最好的情况下，它们的准确度达到了 65% 到 97%。当只使用一块 TrueNorth 芯片时，它只在这 8 个数据集中的 1 个数据集上超越了当前最佳的准确度。但如果使用多达 8 块芯片，IBM 的研究者可以极大提升这款硬件在深度学习挑战上的准确度。这使 TureNorth 可以在其中 3 个数据集上媲美或超越当前最佳的准确度。这个 TureNorth 测试也做到了每秒处理 1200 到 2600 视频帧。这意味着单个 TureNorth 芯片就能实时检测来自多达 100 个同时工作的相机的数据的模式……（来自 IEEE Spectrum）TrueNorth 的功率效率非常出色，所以非常值得考虑。Brainchip 的脉冲神经元自适应处理器（SNAP：Spiking Neuron Adaptive Processor）SNAP 不能做深度学习，这只是一个源于好奇心的项目，还没有实际落地成 CNN 工程解决方案，至少现在还没有。如果你想探索这条路，IBM 的随机相变神经元（stochastic phase-change neurons）似乎更有意思。苹果的神经引擎（Neural Engine）到底会不会有？彭博社报道称这会是一款次级处理器，但没有什么细节信息。对苹果来说，这不仅是一个重要领域，而且也有助于与高通的竞争。其它1. 寒武纪（Cambricon）——中国科学院在芯片上投入的 140 万美元。它是一个用于神经网络的指令集架构，具有数据级的并行、自定义向量/矩阵指令、片上 scratchpad memory。宣称速度是 x86 CPU 的 91 倍，是 K40M 的 3 倍，且峰值功率仅有 1%，即 1.695W。参阅这两篇论文：Cambricon-X：一个用于稀疏神经网络的加速器：http://cslt.riit.tsinghua.edu.cn/mediawiki/images/f/f1/Cambricon-X.pdfCambricon：一个用于神经网络的指令集架构：http://dl.acm.org/citation.cfm?id=30011792. 前谷歌员工创立的 Groq Inc.，也许是另一种 TPU？3. Aimotive：https://aimotive.com/4. Deep Vision 正在开发用于深度学习的低功耗芯片，也许这两篇创始人的论文能提供一点线索：Convolution Engine: Balancing Efficiency &amp; Flexibility in Specialized Computing [2013]：http://csl.stanford.edu/~christos/publications/2013.convolution.isca.pdfConvolution Engine: Balancing Efficiency and Flexibility in Specialized Computing [2015]：http://csl.stanford.edu/~christos/publications/2015.convolution_engine.cacm.pdf5. DeepScale&nbsp;6. Reduced Energy Microsystems 正在开发用于 CNN 推理的低功耗异步芯片。据 TechCrunch 报道，REM 是 Y Combinator 第一个 ASIC 领域的风险投资。7. Leapmind 也很忙。FPGA微软已经站队 FPGA 了。Wired 这篇文章说得很好：《深度 |《连线》长文揭秘微软 Project Catapult：人工智能时代押注 FPGA》。「Bing 占据着世界上 20% 的桌面搜索市场和 6% 的移动手机市场，在 Bing 上，这个芯片能帮助 Bing 适应新品种的人工智能：深度神经网络。」我对这种方法也有些兴趣。赛灵思和英特尔（收购了 Altera）的 FPGA 是很强大的引擎。赛灵思自然宣称他们的 FPGA 是对 INT8 最好的，他们的一份白皮书里面包含了下面的幻灯片：这两家供应商都很支持使用他们的 FPGA 做机器学习：Xilinx - Acceleration Zone：https://goo.gl/KheG5WIntel FPGA OpenCL（https://goo.gl/S62fMA）和 Solutions（https://goo.gl/zkYyXB）尽管 FPGA 单位功耗的性能是很出色的，但这些供应商的更大型的芯片的售价却长时间以来都高得吓人。赛灵思的 VU9P 在 Avnet 上的售价超过 5 万美元。寻找价格和能力之间的平衡点是 FPGA 的主要难题。FPGA 方法的一大优势是可以用来制定一些非常出色的架构决策。比如如果因为 HBM，你想在板上 DRAM 压缩并且实时地解压，然后希望借此改善你的内存的浮点数据流，如果你足够努力，你能找到解决方案。参阅《用于基于 FGPA 的高性能计算的浮点数值数据流的带宽压缩（Bandwidth Compression of Floating-Point Numerical Data Streams for FPGA-Based High-Performance Computing）》：http://dl.acm.org/citation.cfm?id=3053688。这种动态的架构敏捷性（architectural agility）很困难，几乎无法用其它任何方法实现。架构选择太多可能也是一个问题，但我自己还是挺喜欢这个问题的。这篇论文很不错《使用 TILT 减少 Soft Scalar CPU 和定制硬件之间的性能差距（Reducing the Performance Gap between Soft Scalar CPUs and Custom Hardware with TILT）》：http://dl.acm.org/citation.cfm?id=3079757，其中研究了定制硬件和 FPGA 处理器（带有基于 FPGA 的水平微编码的计算引擎）的性能差距，这让人想起了古老的 DISC 和很多个月前的离散指令集计算机（discrete instruction set computer）。谁是赢家？在这样的竞争中，预测谁是赢家是傻瓜的举动。高通凭借其手机市场的主导地位，可以轻松进入赢家榜单。苹果不管做什么都会成功的。英伟达的 V100 有 Tensor 单元，也基本上赢了。我不确定我能否看到谷歌的 TPU 在无休止的长期硅谷竞赛中幸存下来，尽管目前它的性能很出色。我很喜欢 FPGA 方法，但我也不禁想他们应该以远远更低的价格发布 DNN 版本，这样才不会被大众漠视。英特尔和 AMD 会做自己的协处理器。因为所有主要玩家都参战了，所以其中很多都会支持标准的工具包，比如 TensorFlow，这样我们就不必太在意规格了，关心基准就可以了。在更小型的玩家中，我很喜欢而且支持的是 Adapteva 方法，我认为他们的内存架构可能并不适合 DNN。我希望我错了。Wave Computing 可能是我继 FPGA 之后最喜欢的方法。他们的整个异步数据流方法是非常棒的。REM 似乎也在做类似的事情；但我认为他们可能太迟了。Wave Computing 能否在面对所有竞争对手时继续保持领先吗？也许只要他们的异步 CGRA 有一个本质优势，就可以。尽管我不确定他们是否只要 DNN 上的成功，因为他们的技术有更加广泛的应用能力。神经形态脉冲处理器也许现在暂时可以忽略，但也可以保持关注，因为它们有很大的功耗优势。量子计算反正会让所有这些都稍逊风骚。IBM 的 TrueNorth 可能会是个例外，因为它不仅能执行脉冲网络，还能有效地运行 DNN。&nbsp;原文链接：https://meanderful.blogspot.jp/2017/06/fpgas-and-ai-processors-dnn-and-cnn-for.html​"
"综述;;从GPU、TPU到FPGA及其它：一文读懂神经网络硬件平台战局在如今深度学习大爆发的时代，相关的硬件平台也在百花齐放，既有英伟达和谷歌这样的科技巨头，也有地平线机器人和 Graphcore 等创业公司——它们都各自提出了自己的解决方案。近日，多家公司的技术顾问 Matt Hurd 在其博客上发表了一篇全面评点各种神经网络硬件平台的长文，机器之心对本文进行了编译介绍。这是我几周前做的一个传统的 90 年代风格的性别识别神经网络的很好的隐藏节点。一个简单的性别识别器网络中的 90 年代风格的隐藏节点图像我的硕士项目是一种类似级联相关（cascade correlation）的神经网络 Multi-rate Optimising Order Statistic Equaliser（MOOSE：多速率优化顺序统计均衡器），可用于日内的 Bund（国库债券产品）交易。MOOSE 曾经是为获取高速的 LEO 卫星信号（McCaw 的 Teledesic）而设计的一点成果，后来在从 LIFFE 迁移到 DTB 时将目标转向了 Bund。作为一家投资银行的职业交易员，我可以购买很好的工具。我有那时候世界上最快的计算机：一个 IBM MicroChannel dual Pentium Pro 200MHz 处理器外加带有几 MB RAM 的 SCSI。在 1994 年那会儿，将 800,000 个数据点输入到我的 C++ stream/dag 处理器中看上去就像是黑魔法。有限差分方法让我可以做许多 O(1) 增量线性回归这样的运算，以获得 1000 倍的加速。那时候这看起来已经很好了。现在，你的手机都能嘲笑我的大方向。那时候，在神经网络领域有很多研究。倒不是说它有过人的生产力，只是因为有用。读到 Lindsay Fortado 和 Robin Wigglesworth 的 FT 文章《Machine learning set to shake up equity hedge funds》中 Eric Schmidt 关于机器学习和交易的看法，还真有点让人高兴：Eric Schmidt 是谷歌母公司 Alphabet 的执行董事长，他上周对一众对冲基金经理说他相信在 50 年内，所有交易都会有计算机解读数据和市场信号。「我期待出现在交易方面做机器学习的创业公司，看看我描述的这种模式识别能否比数据分析专家的传统线性回归算法做得更好。」他补充说，「我这个行业内的许多人都认为这注定将成为一种新的交易形式。」老朋友 Eric，我在 90 年代早期就已经算是迟到了，你真是有点后知后觉。好吧，现在情况已经不同了。我喜欢思考它，并喜欢将神经网络的这次新复兴称为感知时代（The Age of Perception）。这并不是智能，只是擅长模式而已。它仍然无力应对语言歧义。它还要一些时间才能理解基本的价值和概念，从而形成深刻的金融理解。深度学习既被夸大了，也被低估了。这不是智能，但会有助于帮我们实现智能。一些人将其夸大为将会给我们带来替代人的类人机器的人工智能突破。我们仍然还受困于常识以及用于推理的简单文本中的歧义。我们还有很长的路要走。相对简单的规划算法和启发式方法以及基于神奇的深度学习的视觉、声音、文本、雷达等等的感知能力将会带来深刻的影响，就像是每个人和他们的狗现在理解的那样。所以我叫它「感知时代」。就好像是我们口袋里的超级计算机突然有了眼睛，并且快速适应了真实世界所带来的闪光性致盲。深度学习将会带来巨大的影响，并且将会改变这颗行星上全人类的生活方式。但我们低估了其对我们的危险。不，我们不会和能激起或挑战我们最深刻的思想的深度图灵对话者约会——还不会。这将不可避免地到来，但在可见的未来里还不可见。借助语音、文本和 Watson 那样的数据库的智能代理可以实现非常先进的 Eliza，但不会更先进了。自动化运输、食物生产、建筑、协助家事将会极大地改变人们的生活方式和不动产的价值。除了这些泛泛之谈，本文的目的是收集一些关于芯片的思想见解——它们驱动着当前的神经网络革命。其中很多见解都不是最为激动人心的，但这对我来说是一个有用的锻炼。神经网络硬件与 20 年前相比，今天的神经处理方法并没有很大不同。深度更多的是一个品牌，而不是一项差异。激活函数已经得到了简化，以更好地适配硬件。主要的成功之处在于我们有了更多数据，对如何初始化权重、处理许多层、并行化和提升鲁棒性也有了更好的理解，其中要用到像是 dropout 这样的技术。1980 年的 Neocognitron 架构与今天的深度学习器或 CNN 并没有显著差异，但 Yann LeCun 让它具备了学习能力。在 90 年代那会儿也有很多神经硬件平台，比如 CNAPS（1990），它带有 64 个处理单元和 256kB 内存，可以在 8/16 位条件下达到 1.6 GCPS 的速度（CPS 是指每秒连接次数/ connections per second）或在 1 位条件下达到 12.8 GCPS 的速度。你可以在《神经硬件概述（Overview of neural hardware）》[Heemskerk, 1995, draft] 中读到 Synapse-1、CNAPS、SNAP、CNS Connectionist Supercomputer、Hitachi WSI、My-Neupower、LNeuro 1.0、UTAK1、GNU（通用神经单元/General Neural Unit）Implementation、UCL、Mantra 1、Biologically-Inspired Emulator、INPG Architecture、BACHUS 和 ZISC036。阅读地址：https://pdfs.semanticscholar.org/5841/73aa4886f87da4501571957c2b14a8fb9069.pdf好吧，东西还真多，但实际上还排除了软件和加速器板/CPU 组合，比如 ANZA plus、SAIC SIGMA-1、NT6000、Balboa 860 协处理器、Ni1000 识别加速器硬件（英特尔）、IBM NEP、NBC、Neuro Turbo I、Neuro Turbo II、WISARD、Mark II & IV、Sandy/8、GCN（索尼）、Topsi、BSP400（400 微处理器）、DREAM Machine、RAP、COKOS、REMAP、通用并行神经计算机（General Purpose Parallel Neurocomputer）、TI NETSIM 和 GeNet。另外还有一些模拟和混合模拟的实现，包括英特尔的电气式可训练模拟神经网络（801770NX）。你懂我要表达的意思了，那时候的东西还真是多。这在 1994 年迎来了一次爆发：乐观的摩尔定律告诉我们，TeraCPS 即将实现：「未来十年，微电子很可能将继续主导神经网络实现的领域。如果进展和过去进步得一样快，那就意味着神经计算机的性能将会增长大约 2 个数量级。因此，神经计算机将会接近 TeraCPS（10^12 CPS）的性能。由 100 万个节点（每个节点有大约 1000 个输入）组成的网络可以达到大脑的计算速度（100-1000 Hz）。这将能为实验合理巨大的网络提供良好的机会。」由于 Minsky 和 Papert 对隐藏层的不正确简单概括，打击了 Rosenblatt 的感知器梦想并最终导致了他不幸的死亡，神经网络研究遭遇了第一个冬天，研究资金被残酷地撤销了。1995 年，又出现了另一次神经网络冬天，尽管那时候我其实并不知道。作为温水锅里的一只青蛙，我没有注意到正在加热。第二个冬天的主要原因是缺乏激动人心的进展，让人们普遍感到无聊了。到了 2012 年，多亏了 Geoffrey Hinton 的冬季生存技能，多伦多大学基于 AlexNet 开发的 SuperVision 在 ImageNet 处理上实现了极大的提升，第二个神经网络冬天也由此终结了。之后谷歌的 LeNet Inception 模型在 2014 年打破了它的记录。所以据我估计，感知时代始于 2012 年。将它记在你的日历里面吧，五年已经过去了。谷歌在几千台普通机器上进行了出色的并行 CPU 有损更新研究。吴恩达教授和他的朋友们让数十台 GPU 就能完成数千台 CPU 的工作，从而让规模化成为了可能。因此，我们从需要很好的资助的神经处理前景中解放了出来。好吧，或多或少吧，现在最先进的网络有时候需要数千台 GPU 或专用芯片。更多数据和更多处理能力是其中的关键。让我们进入本文的重点，列出一些感知时代大数据之战中的一些关键平台：英伟达的 GPU这一家是很难被战胜的。来自大型视频处理市场的收益驱动着巨大的规模经济。新款英伟达 V100 带有一种新的 Tensor Core 架构，速度可达 15 TFlops（单精度/SP）或 120 TFlops（浮点精度，其中带有 FP16 的乘法和 FP32 的累加或加法，非常适合机器学习）。英伟达在它们的 DGX-1 中装入了 8 块计算卡，速度可达 960 Tensor TFlops.AMD 的 GPU在机器学习领域，AMD 一直是英伟达的追赶者。即将发布的 AMD Radeon Instinct MI25 有希望达到 12.3 TFlops（SP）或 24.6 TFlops（FP16）。如果你把英伟达的 Tensor Core 也算进来，则 AMD 完全无力竞争。英伟达设备的带宽 900GB/s 也是 AMD 484GB/s 的两倍。谷歌的 TPU谷歌原来的 TPU 相比于 GPU 有很大的领先，并帮助 DeepMind 的 AlphaGo 赢得了与李世石的围棋大战。据描述，原来的 700 MHz TPU 有 95 TFlops 的 8 位计算能力或 23 TFlops 的 16 位计算能力，同时功耗仅有 40W。这可比当时的 GPU 快得多，但现在落后于英伟达的 V100；但在单位功耗的计算能力上，TPU 并没落后。据称新的 TPU2 是一款带有 4 块芯片的 TPU 设备，速度可达到 180 TFlops 左右。每块芯片的性能都翻倍了，达到了 45 TFlops 的 16 位计算能力。你可以看到与英伟达 V100 的差距正在变小。你没法买到 TPU 或 TPU2。谷歌正在通过它们的云提供这些 TPU 服务，包含 64 台设备的 TPU pod 速度高达 11.5 PetaFlops。TPU2 上巨大的散热片说明了一些原因，但市场正在发生变化——从单独的设备转向了设备的组合以及将这些组合以云的形式提供。Wave ComputingWave 之父澳洲人 CTO Chris Nicol 博士的成果斐然，领导开发了 Wave 的 Compute Appliance 中的异步数据流处理器（asynchronous data flow processor）。几年前，Metamako 的创始人 Charles Thomas 在加州简单介绍了我和 Chris 认识。他们俩都曾在 NICTA 研究过无时钟异步。这两人都很出色。我不确定 Wave 的设备最早是不是针对机器学习设计的，但在他们的 3RU appliance 上运行 TensorFlow 的速度可以达到 2.9 PetaOPS/s，这实在了不起。Wave 将他们的处理器称为 DPU，一个 appliance 有 16 个 DPU。Wave 使用了他们称之为粗粒度可重构阵列（CGRA：Coarse Grained Reconfigurable Array）的处理元素。我还不清楚 2.9 PetaOPS/s 的速度对应多少位宽。根据他们的白皮书，其 ALU 可以执行 1 位、8 位、16 位和 32 位计算：「算术单元是分区的。它们可以并行执行 8 位运算（完美适用于 DNN 推理）以及 16 位和 32 位运算（或以上的任何组合）。也可以执行一些 64 位运算，而且可以使用软件扩展到任意精度。」关于其 appliance 中的 16 个 DPU，还有一些额外的信息：「Wave Computing DPU 是一种包含了 16384 PE 的 SoC，它们被配置成了一个 32×32 集群的 CGRA。它包含了 4 个 HMC（Hybrid Memory Cube）第二代接口、一个 PCIe 第三代 16 通道接口和一个用于 SoC 资源管理的嵌入式 32 位 RISC 微控制器。这款 Wave DPU 可以在没有主机 CPU 时自动执行。」对于 TensoFlow 指令：「Wave DNN Library 团队为 TensorFlow 等工作流程中所使用的常见 DNN 函数创建了预编译的可重新定位的 kernel。它们可以被组合到 Agent 中并且可以被实例化成机器，以构建大型的张量数据流图和 DNN kernel。」「……一个可与 TensorFlow、CNTK、Caffe 和 MXNet 等机器学习工作流程交互的 session 管理器，可作为用于训练和推理的工作器过程（worker process）。这些工作流程可为工作器过程提供张量的数据流图。在运行时，Wave 的 session 管理器会分析数据流图并将这些软件智能体放置到 DPU 芯片中，然后将它们连接起来以构建数据流图。这些软件智能体会被分配用于输入缓冲和本地存储的全局内存区域。CGRA kernel 的静态本质和分布式的内存架构可以让一个性能模型准确估计智能体的延迟。session 管理器可使用该性能模型来在智能体之间插入 FIFO 缓冲，这有助于 DPU 中通信和计算的重叠。这个可变智能体支持穿过整个图的数据流的软件流程，从而可以进一步增加并发性和性能。这个 session 管理器可以监控数据流图运行时的性能（通过监控卡顿、缓冲区下溢出和/或上溢出），并且可以动态地调节 FIFO 缓冲区的大小以实现吞吐量的最大化。在附加了 DPU 的处理器中，有一个分布式运行时管理系统会在运行时安装和卸载数据流图的一些部分，从而平衡计算和内存的使用量。这种在数据流计算机中的数据流图上的运行时重新配置还是有史以来第一次。」是的，我也觉得非常酷。这个平台的惊人之处是它在架构方面比 FPGA 更加粗粒度，因此灵活性更低，但却很可能表现更好。非常有意思。KnuEdge 的 KnuPath在 2016 年 6 月时我在 Twitter 上谈过 KnuPath。那以后他们的产品页面却失踪了。我不确定他们要把那 1 亿美元如何用到他们的 MIMD 架构上。那时候他们向我这样描述的：每个 ASIC 中有 256 个微型 DSP（即 tDSP）核以及一个 ARM 控制器，适用于 35W 包络中的稀疏矩阵处理。它的性能未知，但他们把自己的芯片与一款当时的英伟达芯片进行了比较，那时候他们说实现了 2.5 倍的性能。我们知道英伟达现在凭借 Tensor 内核已经提速了十倍以上，所以 KnuEdge 还要努力才能跟上节奏。MIMD 或 DSP 方法必须要得到非常好的效果才能在这一领域占据一席之地。时间会给我们答案。英特尔的 NervanaNervana Systems 曾经除了开发他们的 Nervana Engine ASIC，还开发着一种 GPU/软件方法，后来英特尔收购了这家公司。性能比较方面还不清楚。英特尔也在规划通过一个 Knights Crest 项目将其集成到 Phi 平台中。NextPlatform 认为其 2017 年在 28nm 节点上的目标是在某个位宽的运算速度达到 55 TOPS/s。英特尔还安排一个 NervanaCon，将在 12 月份举办，所以那时候我们也许将能看到他们的第一批成果。地平线机器人这家中国创业公司正在研发一种大脑处理单元（BPU：Brain Processing Unit）。余凯博士是正规军出身，他曾是百度深度学习研究院的负责人。今年早些时候，一段 YouTube 视频演示了基于 Arria 10 FPGA 的 BPU 仿真：https://youtu.be/GI9U9lUFaDo。目前关于这一平台的公开消息还很少。EyerissEyeriss 是 MIT 的一个项目，开发出了一款具有出色原始性能表现的 64nm ASIC。在 AlexNet 上，这款芯片的速度大约是英伟达 TK1 的一半。其优势在于借助于其行固定（row stationary）方法，仅需要一个 278mW 的可重新配置加速器就能实现这样中规中矩的性能。赞。Graphcore去年 Graphcore 拿到了 3000 万美元的 A 轮融资，以开发他们的智能处理单元（IPU： Intelligence Processing Unit）。他们的网站还缺少细节，只是给出了一些亮眼的事实，比如多于 14000 个独立的处理器线程和大于 100 倍的内存带宽。根据 NextPlatform 报道的零星信息，其在一块芯片上具有多于 1000 个真实内核，并且采用了定制的互连方式。它的 PCIe 板具有一个 16 个处理器的元件。听起来似乎是数据流。抛开公关的言论，这个团队确实有很强的背景，而且投资者也不并不傻，所以就拭目以待吧。TenstorrentTenstorrent 是加拿大多伦多的一家小创业公司，它宣称在深度学习的效率上实现了一个数量级的提升，和大多数公司一样，还没有什么公开的细节，但该公司入选了 Cognitive 300 榜单。CerebrasCerebras 值得一提，因为它得到了 Benchmark 的支持，而且其创始人是 SeaMicro 的 CEO。它似乎已经融资 2500 万美元了，而且仍然处于隐身模式。ThinciThinci 正在美国萨克拉门托开发视觉处理器，并且在印度也有员工。他们宣称即将推出他们的第一款硅芯片 Thinci-tc500，而且已经开始进行标准评测和赢得客户了。但除了「一切都并行处理」之外，我们所知甚少。KonikuKoniku 的网站正在倒计时，现在还有 20 几天。我已经等不及了。他们没有融到多少钱，而看过它们在福布斯上的这个视频之后（https://goo.gl/VA1PJx），你很可能也不得不相信他们，但你也无法预料究竟会如何。利用生物细胞肯定是不一样的。听起来就像是一个科研项目，但他们这样说：「我们是一个企业。我们不是一个科研项目。」下周将在维也纳的 Pioneers Festival 上发表演讲的 Agabi 这样说，「今天有些需求是硅所不能满足的，而我们可以通过我们的系统提供。」Koniku 提供的核心是所谓的神经元壳（neuron-shell），这家创业公司称其内部可以控制神经元彼此通信的方式，加上一种正在申请专利的电极，就可以在神经元上读写信息。所有这些都可以装在一个 iPad 大小的设备里，他们还希望能在 2018 年之前将其减小到一枚五美分硬币大小。AdaptevaAdapteva 是我最喜欢的一家小技术公司，正如你在之前文章《Adapteva tapes out Epiphany-V：一款 1024 核 64 位 RISC 处理器》：https://goo.gl/6ZH7JP。去年年底时 Andreas Olofsson 拿出了他的 1024 核芯片，我们都等着看它的性能表现。Epiphany-V 有用于深度学习的新指令，我们必须要看看这种带有 64MB 片上内存的更少内存控制器的设计是否具有合适的扩展能力。Andrea 的设计和构建的出色效率可能能让我们真正负担得起这种芯片，所以让我们希望它有良好的表现吧。KnowmKnown 研究的是 Anti-Hebbian and Hebbian（AHaH）可塑性和忆阻器。这里有篇覆盖这一主题的论文《AHaH 计算：从 Metastable Switches 到 Attractors 到机器学》：https://doi.org/10.1371/journal.pone.0085175。这对我来说有点太高级了。简单看了看，我看不出这项技术和胡言乱语的区别，但看起来确实科学味道十足。我需要亲眼看到才能相信。神经忆阻式处理器（neuromemristive processor）的思想是很有趣的。我早上确实需要一个好的流行术语。MythicMythic 的一款电池驱动的神经芯片具有低 50 倍的功耗。目前还看不到太多真正的细节。这款芯片大约纽扣大小，但大多数芯片不都这样吗？「Mythic 的平台能在纽扣大小的芯片上提供桌面 GPU 的性能。」也许这又是一款适合无人机和手机的芯片，很可能被用在手机中，也可能被排除在外。高通手机显然是机器学习硬件的一大用武之地。我们希望能够识别狗的品种、花朵、树叶、癌症痣、翻译标识、理解口语等等。我们口袋里的超级计算机愿意用上它能得到的所有帮助，以便能迈入感知时代。高通一直以来都在鼓捣机器学习，推出了 Zeroth SDK 和 Snapdragon 神经处理引擎（NPE）。这种 NPE 显然在高通所用的 Hexagon DSP 上效果良好。Hexagon DSP 已经远远不止是一个非常广泛的并行平台了，Yann LeCun 已经证实高通和 Facebook 正在合作开发一种更好的方法，参见 Wired 的文章《业界 | 谷歌 TPU 之后还有高通，人工智能芯片竞赛已经展开》：「最近，高通已经开始制造执行神经网络的专用芯片，这条消息来自 LeCun，因为 Facebook 正帮助高通开发机器学习相关技术，所以他对高通的计划很了解；高通技术副总裁 Jeff Gehlhaar 证实了这个项目，他说：『在原型设计和开发方面，我们还有很长的路要走。』」也许我们很快就会看到 Kryo CPU、Adreno GPU、Hexagon DSP 和 Hexagon Vector Extensions 之外的其它东西。对于这一领域的创业公司来说，和高通的机器学习竞争将会艰难无比。Pezy-SC 和 Pezy-SC2这两者是 Pezy 开发的 1024 核和 2048 核处理器。Pezy-SC 1024 核芯片可以驱动 2015 年 Green500 超级计算机榜单的前三名的系统。Pezy-SC2 是现在已经开始提供的后续芯片，我在 6 月份也做过关于它的演讲，但相关细节还很少，不过仍然很吸引人：「PEZY-SC2 HPC Brick：单个机箱中有 32 个带有 64GB DDR4 DIMM（2.1 PetaFLOPS（DP））的 Pezy-SC2 模块卡，速度可达 6.4 Tb/s.」不知道 2048 个 MIMD MIPS Warrior 64 位核的机器能做到什么？在 6 月份的 2017 年 Green500 榜单中，一个英伟达 P100 系统拿到了头把交椅，而排名第 7 的是一个 Pezy-SC2 系统。所以看起来这款芯片还活着，但相关细节却很少。Motoaki Saito（齊藤元章）当然值得一看。Kalray尽管做了很多承诺，但 Kalray 的芯片还没有超过 256 核，我在 2015 年的一篇文章就谈到：https://goo.gl/pxqn7Z。Kalray 宣传自己的产品说是适合嵌入式自动驾驶汽车应用，但我觉得其目前产品架构的形式并不是一种完美的 CNN 平台。Kalray 有一个 Kalray Neural Network（KaNN）软件包并且宣称有比 GPU 更好的效率，在芯片上能实现高达 1 TFlop/s 的速度。随着即将到来的产品更新，Kalray 的神经网络财富可能还会提升，就在这个月 Kalray 完成了新一轮 2600 万美元的融资。他们新的 Coolidge 处理器预计将在 2018 年年中上市，它会带有 80 或 160 个核，另外还有 80 或 160 个为视觉和深度学习优化的协处理器。这在他们的多于 1000 核的方法上有了很大的改变，而我认为这是最明智的。IBM TrueNorthTrueNorth 是 IBM 的神经形态 CMOS ASIC，是与 DARPA 的 StNAPSE 项目一起开发的。这是一种单个芯片设计上的多核处理器网络，具有 4096 个核，每个核模拟 256 个可编程的硅「神经元」，总共就超过了 100 万个神经元。另外，每个神经元还有 256 个可编程的「突触」，信号可以在这些突触之间传递。因此，可编程突触的总是超过了 2.68 亿（2^28）。在基本构建模块方面，它的晶体管数量为 54 亿。因为存储、计算和通信都是在这 4096 个神经突触核中各自处理的，所以 TrueNorth 避开了冯·诺依曼架构的瓶颈，而且能效非常高，功耗为 70 mW，大约是传统微处理器的功率密度的万分之一（来自维基百科）。IBM 之前还在批评脉冲神经网络（spiking neural network）无法适配深度学习，现在 IBM 开发了一种在 TureNorth 上运行 CNN 的新算法。这些神经元不会在每个周期内都放电，脉冲神经网络中的神经元必须逐渐积累它们的电势，然后才能放电……深度学习专家普遍认为脉冲神经网络在深度学习上没有效率——至少和卷积神经网络比起来是这样。Facebook 人工智能研究院主任和深度学习先驱 Yann LeCun 曾经批评过 IBM 的 TureNorth 芯片，因为它主要支持脉冲神经网络…………这种神经形态芯片不会太激动人心，因为它们注重的脉冲神经网络在深度学习领域并不受欢迎。为了让 TrueNorth 芯片很好地契合深度学习，IBM 不得不开发了一种新算法，让卷积神经网络可以在这款神经形态计算硬件上很好地运行。这种组合方法实现了 IBM 所谓的「接近当前最佳」的分类准确度，实验涉及了视觉和语音挑战赛方面的 8 个数据集。在最好的情况下，它们的准确度达到了 65% 到 97%。当只使用一块 TrueNorth 芯片时，它只在这 8 个数据集中的 1 个数据集上超越了当前最佳的准确度。但如果使用多达 8 块芯片，IBM 的研究者可以极大提升这款硬件在深度学习挑战上的准确度。这使 TureNorth 可以在其中 3 个数据集上媲美或超越当前最佳的准确度。这个 TureNorth 测试也做到了每秒处理 1200 到 2600 视频帧。这意味着单个 TureNorth 芯片就能实时检测来自多达 100 个同时工作的相机的数据的模式……（来自 IEEE Spectrum）TrueNorth 的功率效率非常出色，所以非常值得考虑。Brainchip 的脉冲神经元自适应处理器（SNAP：Spiking Neuron Adaptive Processor）SNAP 不能做深度学习，这只是一个源于好奇心的项目，还没有实际落地成 CNN 工程解决方案，至少现在还没有。如果你想探索这条路，IBM 的随机相变神经元（stochastic phase-change neurons）似乎更有意思。苹果的神经引擎（Neural Engine）到底会不会有？彭博社报道称这会是一款次级处理器，但没有什么细节信息。对苹果来说，这不仅是一个重要领域，而且也有助于与高通的竞争。其它1. 寒武纪（Cambricon）——中国科学院在芯片上投入的 140 万美元。它是一个用于神经网络的指令集架构，具有数据级的并行、自定义向量/矩阵指令、片上 scratchpad memory。宣称速度是 x86 CPU 的 91 倍，是 K40M 的 3 倍，且峰值功率仅有 1%，即 1.695W。参阅这两篇论文：Cambricon-X：一个用于稀疏神经网络的加速器：http://cslt.riit.tsinghua.edu.cn/mediawiki/images/f/f1/Cambricon-X.pdfCambricon：一个用于神经网络的指令集架构：http://dl.acm.org/citation.cfm?id=30011792. 前谷歌员工创立的 Groq Inc.，也许是另一种 TPU？3. Aimotive：https://aimotive.com/4. Deep Vision 正在开发用于深度学习的低功耗芯片，也许这两篇创始人的论文能提供一点线索：Convolution Engine: Balancing Efficiency &amp; Flexibility in Specialized Computing [2013]：http://csl.stanford.edu/~christos/publications/2013.convolution.isca.pdfConvolution Engine: Balancing Efficiency and Flexibility in Specialized Computing [2015]：http://csl.stanford.edu/~christos/publications/2015.convolution_engine.cacm.pdf5. DeepScale&nbsp;6. Reduced Energy Microsystems 正在开发用于 CNN 推理的低功耗异步芯片。据 TechCrunch 报道，REM 是 Y Combinator 第一个 ASIC 领域的风险投资。7. Leapmind 也很忙。FPGA微软已经站队 FPGA 了。Wired 这篇文章说得很好：《深度 |《连线》长文揭秘微软 Project Catapult：人工智能时代押注 FPGA》。「Bing 占据着世界上 20% 的桌面搜索市场和 6% 的移动手机市场，在 Bing 上，这个芯片能帮助 Bing 适应新品种的人工智能：深度神经网络。」我对这种方法也有些兴趣。赛灵思和英特尔（收购了 Altera）的 FPGA 是很强大的引擎。赛灵思自然宣称他们的 FPGA 是对 INT8 最好的，他们的一份白皮书里面包含了下面的幻灯片：这两家供应商都很支持使用他们的 FPGA 做机器学习：Xilinx - Acceleration Zone：https://goo.gl/KheG5WIntel FPGA OpenCL（https://goo.gl/S62fMA）和 Solutions（https://goo.gl/zkYyXB）尽管 FPGA 单位功耗的性能是很出色的，但这些供应商的更大型的芯片的售价却长时间以来都高得吓人。赛灵思的 VU9P 在 Avnet 上的售价超过 5 万美元。寻找价格和能力之间的平衡点是 FPGA 的主要难题。FPGA 方法的一大优势是可以用来制定一些非常出色的架构决策。比如如果因为 HBM，你想在板上 DRAM 压缩并且实时地解压，然后希望借此改善你的内存的浮点数据流，如果你足够努力，你能找到解决方案。参阅《用于基于 FGPA 的高性能计算的浮点数值数据流的带宽压缩（Bandwidth Compression of Floating-Point Numerical Data Streams for FPGA-Based High-Performance Computing）》：http://dl.acm.org/citation.cfm?id=3053688。这种动态的架构敏捷性（architectural agility）很困难，几乎无法用其它任何方法实现。架构选择太多可能也是一个问题，但我自己还是挺喜欢这个问题的。这篇论文很不错《使用 TILT 减少 Soft Scalar CPU 和定制硬件之间的性能差距（Reducing the Performance Gap between Soft Scalar CPUs and Custom Hardware with TILT）》：http://dl.acm.org/citation.cfm?id=3079757，其中研究了定制硬件和 FPGA 处理器（带有基于 FPGA 的水平微编码的计算引擎）的性能差距，这让人想起了古老的 DISC 和很多个月前的离散指令集计算机（discrete instruction set computer）。谁是赢家？在这样的竞争中，预测谁是赢家是傻瓜的举动。高通凭借其手机市场的主导地位，可以轻松进入赢家榜单。苹果不管做什么都会成功的。英伟达的 V100 有 Tensor 单元，也基本上赢了。我不确定我能否看到谷歌的 TPU 在无休止的长期硅谷竞赛中幸存下来，尽管目前它的性能很出色。我很喜欢 FPGA 方法，但我也不禁想他们应该以远远更低的价格发布 DNN 版本，这样才不会被大众漠视。英特尔和 AMD 会做自己的协处理器。因为所有主要玩家都参战了，所以其中很多都会支持标准的工具包，比如 TensorFlow，这样我们就不必太在意规格了，关心基准就可以了。在更小型的玩家中，我很喜欢而且支持的是 Adapteva 方法，我认为他们的内存架构可能并不适合 DNN。我希望我错了。Wave Computing 可能是我继 FPGA 之后最喜欢的方法。他们的整个异步数据流方法是非常棒的。REM 似乎也在做类似的事情；但我认为他们可能太迟了。Wave Computing 能否在面对所有竞争对手时继续保持领先吗？也许只要他们的异步 CGRA 有一个本质优势，就可以。尽管我不确定他们是否只要 DNN 上的成功，因为他们的技术有更加广泛的应用能力。神经形态脉冲处理器也许现在暂时可以忽略，但也可以保持关注，因为它们有很大的功耗优势。量子计算反正会让所有这些都稍逊风骚。IBM 的 TrueNorth 可能会是个例外，因为它不仅能执行脉冲网络，还能有效地运行 DNN。&nbsp;原文链接：https://meanderful.blogspot.jp/2017/06/fpgas-and-ai-processors-dnn-and-cnn-for.html​"
"硬件;;内存带宽与计算能力，谁才是决定深度学习执行性能的关键？随着深度学习的不断发展，计算能力得到了深度学习社区越来越多的注意。任何深度学习模型，归根到底都是需要跑在设备上的，而模型对设备性能的要求越低，则能得到越多的运用——千万不能让硬件成为了模型普及的瓶颈！说到模型对于硬件的要求，大家第一个想到的就是计算量，即一个深度学习模型需要多少次计算才能完成一次前馈。然而，除了运算量之外，模型对于内存带宽的需求也是影响实际计算所需要时间的重要参数。我们下面会看到，在内存带宽有限的情况下，仅仅缩小计算量并不能让计算时间等比例下降！内存带宽对于硬件系统的性能影响如上图所示。如果把内存比做瓶子，运算单元比作杯子，那么数据就是瓶子里的各色颗粒，而内存接口就是瓶口，通过瓶口数据才能进入杯子被消费（处理）掉。而内存带宽就是瓶口的宽度了。瓶口宽度越窄，则数据需要越多时间才能进入杯子（处理单元）。正所谓「巧妇难为无米之炊」，如果带宽有限，那么即使处理单元无限快，在大多数时候也是处理单元在空等数据，造成了计算力的浪费。深度学习网络与 Roofline 模型对于工程师来说，定性分析并不够，我们还需要能定量分析算法对于内存带宽的需求，以及对于计算性能的影响。算法对于内存带宽的需求通常使用「运算强度 (operational intensity，或称 arithmetic intensity)」这个量来表示，单位是 OPs/byte。这个量的意思是，在算法中平均每读入单位数据，能支持多少次运算操作。运算强度越大，则表示单位数据能支持更多次运算，也就是说算法对于内存带宽的要求越低。所以，运算强度大是好事！我们来举一个例子。对于步长（stride）为 1 的 3x3 卷积运算，假设输入数据平面大小为 64x64。简单起见，假设输入和输出 feature 都为 1。这时候，总共需要进行 62x62 次卷积运算，每次卷积需要做 3x3=9 次乘加运算，所以总共的计算次数为 34596，而数据量为（假设数据和卷积核都用单精度浮点数 2byte）：64x64x2（输入数据）+ 3x3x2（卷积核数据）= 8210 byte，所以运算强度为 34596/8210=4.21。如果我们换成 1x1 卷积，那么总的计算次数变成了 64x64=4096，而所需的数据量为 64x64x2 + 1x1x2=8194。显然，切换为 1x1 卷积可以把计算量降低接近 9 倍，但是运算强度也降低为 0.5，即对于内存带宽的需求也上升了接近 9 倍。因此，如果内存带宽无法满足 1x1 卷积计算，那么切换成 1x1 卷积计算虽然降低了接近 9 倍计算量，但是无法把计算速度提升 9 倍。这里，我们可以看到，深度学习计算设备存在两个瓶颈，一个是处理器计算能力，另一个是计算带宽。如何分析究竟是哪一个限制了计算性能呢？可以使用 Roofline 模型。典型的 Roofline 曲线模型如上图所示，坐标轴分别是计算性能（纵轴）和算法的运算强度（横轴）。Roofline 曲线分成了两部分：左边的上升区，以及右边的饱和区。当算法的运算强度较小时，曲线处于上升区，即计算性能实际被内存带宽所限制，有很多计算处理单元是闲置的。随着算法运算强度上升，即在相同数量的数据下算法可以完成更多运算，于是闲置的运算单元越来越少，这时候计算性能就会上升。然后，随着运算强度越来越高，闲置的计算单元越来越少，最后所有计算单元都被用上了，Roofline 曲线就进入了饱和区，此时运算强度再变大也没有更多的计算单元可用了，于是计算性能不再上升，或者说计算性能遇到了由计算能力（而非内存带宽）决定的「屋顶」（roof）。拿之前 3x3 和 1x1 卷积的例子来说，3x3 卷积可能在 roofline 曲线右边的饱和区，而 1x1 卷积由于运算强度下降，有可能到了 roofline 左边的上升区，这样 1x1 卷积在计算时的计算性能就会下降无法到达峰值性能。虽然 1x1 卷积的计算量下降了接近 9 倍，但是由于计算性能下降，因此实际的计算时间并不是 3x3 卷积的九分之一。显然，一个计算系统的内存带宽如果很宽，则算法不需要运算强度很大也能轻易碰到计算能力上限决定的「屋顶」。在下图中，计算能力不变，而随着内存带宽的上升，达到计算力屋顶所需的运算强度也越低。Roofline 模型在算法-硬件协同设计中非常有用，可以确定算法和硬件优化的方向：到底应该增加内存带宽／减小内存带宽需求，还是提升计算能力／降低计算量？如果算法在 roofline 曲线的上升区，那么我们应该增加内存带宽／减小内存带宽需求，提升计算能力／降低计算量对于这类情况并没有帮助。反之亦然。我们来看一个实际的例子，比较一下各种机器学习算法在 roofline 模型上所处的位置。下图取自 Google 的 TPU 论文《In-Datacenter Performance Analysis of a Tensor Processing Unit》。由图中可见，LSTM 算法的运算强度最低，所以被卡在了 roofline 模型的上升区中间的地方，即 TPU 在执行 LSTM 算法的时候，由于内存带宽限制所以性能只有 3TOPS 左右，仅为峰值性能（90TOPS）的三十分之一。经典全联接神经网络（multi-layer perceptrons, MLP）的运算强度略好于 LSTM，也被卡在 roofline 曲线的上升区，实际执行性能大约在 10TOPS 左右。而卷积神经网络模型，尤其是 CNN0，由于卷积神经网络中能实现卷积核复用，因此运算强度非常高，于是可以非常接近 TPU roofline 曲线的屋顶（86 TOPS）。CNN1 模型虽然运算强度也很高，但是由于种种其他原因（论文中表示是由于 CNN1 模型的特征深度较浅无法完全利用 TPU 的计算单元）无法到达屋顶。这个例子又让我们看到了硬件-算法协同设计时的另一个要点：除了内存带宽之外还有「其他原因」可能让算法无法到达屋顶，我们要尽量减小这些「其他因素」！"
"深度学习;;内存带宽与计算能力，谁才是决定深度学习执行性能的关键？随着深度学习的不断发展，计算能力得到了深度学习社区越来越多的注意。任何深度学习模型，归根到底都是需要跑在设备上的，而模型对设备性能的要求越低，则能得到越多的运用——千万不能让硬件成为了模型普及的瓶颈！说到模型对于硬件的要求，大家第一个想到的就是计算量，即一个深度学习模型需要多少次计算才能完成一次前馈。然而，除了运算量之外，模型对于内存带宽的需求也是影响实际计算所需要时间的重要参数。我们下面会看到，在内存带宽有限的情况下，仅仅缩小计算量并不能让计算时间等比例下降！内存带宽对于硬件系统的性能影响如上图所示。如果把内存比做瓶子，运算单元比作杯子，那么数据就是瓶子里的各色颗粒，而内存接口就是瓶口，通过瓶口数据才能进入杯子被消费（处理）掉。而内存带宽就是瓶口的宽度了。瓶口宽度越窄，则数据需要越多时间才能进入杯子（处理单元）。正所谓「巧妇难为无米之炊」，如果带宽有限，那么即使处理单元无限快，在大多数时候也是处理单元在空等数据，造成了计算力的浪费。深度学习网络与 Roofline 模型对于工程师来说，定性分析并不够，我们还需要能定量分析算法对于内存带宽的需求，以及对于计算性能的影响。算法对于内存带宽的需求通常使用「运算强度 (operational intensity，或称 arithmetic intensity)」这个量来表示，单位是 OPs/byte。这个量的意思是，在算法中平均每读入单位数据，能支持多少次运算操作。运算强度越大，则表示单位数据能支持更多次运算，也就是说算法对于内存带宽的要求越低。所以，运算强度大是好事！我们来举一个例子。对于步长（stride）为 1 的 3x3 卷积运算，假设输入数据平面大小为 64x64。简单起见，假设输入和输出 feature 都为 1。这时候，总共需要进行 62x62 次卷积运算，每次卷积需要做 3x3=9 次乘加运算，所以总共的计算次数为 34596，而数据量为（假设数据和卷积核都用单精度浮点数 2byte）：64x64x2（输入数据）+ 3x3x2（卷积核数据）= 8210 byte，所以运算强度为 34596/8210=4.21。如果我们换成 1x1 卷积，那么总的计算次数变成了 64x64=4096，而所需的数据量为 64x64x2 + 1x1x2=8194。显然，切换为 1x1 卷积可以把计算量降低接近 9 倍，但是运算强度也降低为 0.5，即对于内存带宽的需求也上升了接近 9 倍。因此，如果内存带宽无法满足 1x1 卷积计算，那么切换成 1x1 卷积计算虽然降低了接近 9 倍计算量，但是无法把计算速度提升 9 倍。这里，我们可以看到，深度学习计算设备存在两个瓶颈，一个是处理器计算能力，另一个是计算带宽。如何分析究竟是哪一个限制了计算性能呢？可以使用 Roofline 模型。典型的 Roofline 曲线模型如上图所示，坐标轴分别是计算性能（纵轴）和算法的运算强度（横轴）。Roofline 曲线分成了两部分：左边的上升区，以及右边的饱和区。当算法的运算强度较小时，曲线处于上升区，即计算性能实际被内存带宽所限制，有很多计算处理单元是闲置的。随着算法运算强度上升，即在相同数量的数据下算法可以完成更多运算，于是闲置的运算单元越来越少，这时候计算性能就会上升。然后，随着运算强度越来越高，闲置的计算单元越来越少，最后所有计算单元都被用上了，Roofline 曲线就进入了饱和区，此时运算强度再变大也没有更多的计算单元可用了，于是计算性能不再上升，或者说计算性能遇到了由计算能力（而非内存带宽）决定的「屋顶」（roof）。拿之前 3x3 和 1x1 卷积的例子来说，3x3 卷积可能在 roofline 曲线右边的饱和区，而 1x1 卷积由于运算强度下降，有可能到了 roofline 左边的上升区，这样 1x1 卷积在计算时的计算性能就会下降无法到达峰值性能。虽然 1x1 卷积的计算量下降了接近 9 倍，但是由于计算性能下降，因此实际的计算时间并不是 3x3 卷积的九分之一。显然，一个计算系统的内存带宽如果很宽，则算法不需要运算强度很大也能轻易碰到计算能力上限决定的「屋顶」。在下图中，计算能力不变，而随着内存带宽的上升，达到计算力屋顶所需的运算强度也越低。Roofline 模型在算法-硬件协同设计中非常有用，可以确定算法和硬件优化的方向：到底应该增加内存带宽／减小内存带宽需求，还是提升计算能力／降低计算量？如果算法在 roofline 曲线的上升区，那么我们应该增加内存带宽／减小内存带宽需求，提升计算能力／降低计算量对于这类情况并没有帮助。反之亦然。我们来看一个实际的例子，比较一下各种机器学习算法在 roofline 模型上所处的位置。下图取自 Google 的 TPU 论文《In-Datacenter Performance Analysis of a Tensor Processing Unit》。由图中可见，LSTM 算法的运算强度最低，所以被卡在了 roofline 模型的上升区中间的地方，即 TPU 在执行 LSTM 算法的时候，由于内存带宽限制所以性能只有 3TOPS 左右，仅为峰值性能（90TOPS）的三十分之一。经典全联接神经网络（multi-layer perceptrons, MLP）的运算强度略好于 LSTM，也被卡在 roofline 曲线的上升区，实际执行性能大约在 10TOPS 左右。而卷积神经网络模型，尤其是 CNN0，由于卷积神经网络中能实现卷积核复用，因此运算强度非常高，于是可以非常接近 TPU roofline 曲线的屋顶（86 TOPS）。CNN1 模型虽然运算强度也很高，但是由于种种其他原因（论文中表示是由于 CNN1 模型的特征深度较浅无法完全利用 TPU 的计算单元）无法到达屋顶。这个例子又让我们看到了硬件-算法协同设计时的另一个要点：除了内存带宽之外还有「其他原因」可能让算法无法到达屋顶，我们要尽量减小这些「其他因素」！"
"TPU;;内存带宽与计算能力，谁才是决定深度学习执行性能的关键？随着深度学习的不断发展，计算能力得到了深度学习社区越来越多的注意。任何深度学习模型，归根到底都是需要跑在设备上的，而模型对设备性能的要求越低，则能得到越多的运用——千万不能让硬件成为了模型普及的瓶颈！说到模型对于硬件的要求，大家第一个想到的就是计算量，即一个深度学习模型需要多少次计算才能完成一次前馈。然而，除了运算量之外，模型对于内存带宽的需求也是影响实际计算所需要时间的重要参数。我们下面会看到，在内存带宽有限的情况下，仅仅缩小计算量并不能让计算时间等比例下降！内存带宽对于硬件系统的性能影响如上图所示。如果把内存比做瓶子，运算单元比作杯子，那么数据就是瓶子里的各色颗粒，而内存接口就是瓶口，通过瓶口数据才能进入杯子被消费（处理）掉。而内存带宽就是瓶口的宽度了。瓶口宽度越窄，则数据需要越多时间才能进入杯子（处理单元）。正所谓「巧妇难为无米之炊」，如果带宽有限，那么即使处理单元无限快，在大多数时候也是处理单元在空等数据，造成了计算力的浪费。深度学习网络与 Roofline 模型对于工程师来说，定性分析并不够，我们还需要能定量分析算法对于内存带宽的需求，以及对于计算性能的影响。算法对于内存带宽的需求通常使用「运算强度 (operational intensity，或称 arithmetic intensity)」这个量来表示，单位是 OPs/byte。这个量的意思是，在算法中平均每读入单位数据，能支持多少次运算操作。运算强度越大，则表示单位数据能支持更多次运算，也就是说算法对于内存带宽的要求越低。所以，运算强度大是好事！我们来举一个例子。对于步长（stride）为 1 的 3x3 卷积运算，假设输入数据平面大小为 64x64。简单起见，假设输入和输出 feature 都为 1。这时候，总共需要进行 62x62 次卷积运算，每次卷积需要做 3x3=9 次乘加运算，所以总共的计算次数为 34596，而数据量为（假设数据和卷积核都用单精度浮点数 2byte）：64x64x2（输入数据）+ 3x3x2（卷积核数据）= 8210 byte，所以运算强度为 34596/8210=4.21。如果我们换成 1x1 卷积，那么总的计算次数变成了 64x64=4096，而所需的数据量为 64x64x2 + 1x1x2=8194。显然，切换为 1x1 卷积可以把计算量降低接近 9 倍，但是运算强度也降低为 0.5，即对于内存带宽的需求也上升了接近 9 倍。因此，如果内存带宽无法满足 1x1 卷积计算，那么切换成 1x1 卷积计算虽然降低了接近 9 倍计算量，但是无法把计算速度提升 9 倍。这里，我们可以看到，深度学习计算设备存在两个瓶颈，一个是处理器计算能力，另一个是计算带宽。如何分析究竟是哪一个限制了计算性能呢？可以使用 Roofline 模型。典型的 Roofline 曲线模型如上图所示，坐标轴分别是计算性能（纵轴）和算法的运算强度（横轴）。Roofline 曲线分成了两部分：左边的上升区，以及右边的饱和区。当算法的运算强度较小时，曲线处于上升区，即计算性能实际被内存带宽所限制，有很多计算处理单元是闲置的。随着算法运算强度上升，即在相同数量的数据下算法可以完成更多运算，于是闲置的运算单元越来越少，这时候计算性能就会上升。然后，随着运算强度越来越高，闲置的计算单元越来越少，最后所有计算单元都被用上了，Roofline 曲线就进入了饱和区，此时运算强度再变大也没有更多的计算单元可用了，于是计算性能不再上升，或者说计算性能遇到了由计算能力（而非内存带宽）决定的「屋顶」（roof）。拿之前 3x3 和 1x1 卷积的例子来说，3x3 卷积可能在 roofline 曲线右边的饱和区，而 1x1 卷积由于运算强度下降，有可能到了 roofline 左边的上升区，这样 1x1 卷积在计算时的计算性能就会下降无法到达峰值性能。虽然 1x1 卷积的计算量下降了接近 9 倍，但是由于计算性能下降，因此实际的计算时间并不是 3x3 卷积的九分之一。显然，一个计算系统的内存带宽如果很宽，则算法不需要运算强度很大也能轻易碰到计算能力上限决定的「屋顶」。在下图中，计算能力不变，而随着内存带宽的上升，达到计算力屋顶所需的运算强度也越低。Roofline 模型在算法-硬件协同设计中非常有用，可以确定算法和硬件优化的方向：到底应该增加内存带宽／减小内存带宽需求，还是提升计算能力／降低计算量？如果算法在 roofline 曲线的上升区，那么我们应该增加内存带宽／减小内存带宽需求，提升计算能力／降低计算量对于这类情况并没有帮助。反之亦然。我们来看一个实际的例子，比较一下各种机器学习算法在 roofline 模型上所处的位置。下图取自 Google 的 TPU 论文《In-Datacenter Performance Analysis of a Tensor Processing Unit》。由图中可见，LSTM 算法的运算强度最低，所以被卡在了 roofline 模型的上升区中间的地方，即 TPU 在执行 LSTM 算法的时候，由于内存带宽限制所以性能只有 3TOPS 左右，仅为峰值性能（90TOPS）的三十分之一。经典全联接神经网络（multi-layer perceptrons, MLP）的运算强度略好于 LSTM，也被卡在 roofline 曲线的上升区，实际执行性能大约在 10TOPS 左右。而卷积神经网络模型，尤其是 CNN0，由于卷积神经网络中能实现卷积核复用，因此运算强度非常高，于是可以非常接近 TPU roofline 曲线的屋顶（86 TOPS）。CNN1 模型虽然运算强度也很高，但是由于种种其他原因（论文中表示是由于 CNN1 模型的特征深度较浅无法完全利用 TPU 的计算单元）无法到达屋顶。这个例子又让我们看到了硬件-算法协同设计时的另一个要点：除了内存带宽之外还有「其他原因」可能让算法无法到达屋顶，我们要尽量减小这些「其他因素」！"
"工程;;内存带宽与计算能力，谁才是决定深度学习执行性能的关键？随着深度学习的不断发展，计算能力得到了深度学习社区越来越多的注意。任何深度学习模型，归根到底都是需要跑在设备上的，而模型对设备性能的要求越低，则能得到越多的运用——千万不能让硬件成为了模型普及的瓶颈！说到模型对于硬件的要求，大家第一个想到的就是计算量，即一个深度学习模型需要多少次计算才能完成一次前馈。然而，除了运算量之外，模型对于内存带宽的需求也是影响实际计算所需要时间的重要参数。我们下面会看到，在内存带宽有限的情况下，仅仅缩小计算量并不能让计算时间等比例下降！内存带宽对于硬件系统的性能影响如上图所示。如果把内存比做瓶子，运算单元比作杯子，那么数据就是瓶子里的各色颗粒，而内存接口就是瓶口，通过瓶口数据才能进入杯子被消费（处理）掉。而内存带宽就是瓶口的宽度了。瓶口宽度越窄，则数据需要越多时间才能进入杯子（处理单元）。正所谓「巧妇难为无米之炊」，如果带宽有限，那么即使处理单元无限快，在大多数时候也是处理单元在空等数据，造成了计算力的浪费。深度学习网络与 Roofline 模型对于工程师来说，定性分析并不够，我们还需要能定量分析算法对于内存带宽的需求，以及对于计算性能的影响。算法对于内存带宽的需求通常使用「运算强度 (operational intensity，或称 arithmetic intensity)」这个量来表示，单位是 OPs/byte。这个量的意思是，在算法中平均每读入单位数据，能支持多少次运算操作。运算强度越大，则表示单位数据能支持更多次运算，也就是说算法对于内存带宽的要求越低。所以，运算强度大是好事！我们来举一个例子。对于步长（stride）为 1 的 3x3 卷积运算，假设输入数据平面大小为 64x64。简单起见，假设输入和输出 feature 都为 1。这时候，总共需要进行 62x62 次卷积运算，每次卷积需要做 3x3=9 次乘加运算，所以总共的计算次数为 34596，而数据量为（假设数据和卷积核都用单精度浮点数 2byte）：64x64x2（输入数据）+ 3x3x2（卷积核数据）= 8210 byte，所以运算强度为 34596/8210=4.21。如果我们换成 1x1 卷积，那么总的计算次数变成了 64x64=4096，而所需的数据量为 64x64x2 + 1x1x2=8194。显然，切换为 1x1 卷积可以把计算量降低接近 9 倍，但是运算强度也降低为 0.5，即对于内存带宽的需求也上升了接近 9 倍。因此，如果内存带宽无法满足 1x1 卷积计算，那么切换成 1x1 卷积计算虽然降低了接近 9 倍计算量，但是无法把计算速度提升 9 倍。这里，我们可以看到，深度学习计算设备存在两个瓶颈，一个是处理器计算能力，另一个是计算带宽。如何分析究竟是哪一个限制了计算性能呢？可以使用 Roofline 模型。典型的 Roofline 曲线模型如上图所示，坐标轴分别是计算性能（纵轴）和算法的运算强度（横轴）。Roofline 曲线分成了两部分：左边的上升区，以及右边的饱和区。当算法的运算强度较小时，曲线处于上升区，即计算性能实际被内存带宽所限制，有很多计算处理单元是闲置的。随着算法运算强度上升，即在相同数量的数据下算法可以完成更多运算，于是闲置的运算单元越来越少，这时候计算性能就会上升。然后，随着运算强度越来越高，闲置的计算单元越来越少，最后所有计算单元都被用上了，Roofline 曲线就进入了饱和区，此时运算强度再变大也没有更多的计算单元可用了，于是计算性能不再上升，或者说计算性能遇到了由计算能力（而非内存带宽）决定的「屋顶」（roof）。拿之前 3x3 和 1x1 卷积的例子来说，3x3 卷积可能在 roofline 曲线右边的饱和区，而 1x1 卷积由于运算强度下降，有可能到了 roofline 左边的上升区，这样 1x1 卷积在计算时的计算性能就会下降无法到达峰值性能。虽然 1x1 卷积的计算量下降了接近 9 倍，但是由于计算性能下降，因此实际的计算时间并不是 3x3 卷积的九分之一。显然，一个计算系统的内存带宽如果很宽，则算法不需要运算强度很大也能轻易碰到计算能力上限决定的「屋顶」。在下图中，计算能力不变，而随着内存带宽的上升，达到计算力屋顶所需的运算强度也越低。Roofline 模型在算法-硬件协同设计中非常有用，可以确定算法和硬件优化的方向：到底应该增加内存带宽／减小内存带宽需求，还是提升计算能力／降低计算量？如果算法在 roofline 曲线的上升区，那么我们应该增加内存带宽／减小内存带宽需求，提升计算能力／降低计算量对于这类情况并没有帮助。反之亦然。我们来看一个实际的例子，比较一下各种机器学习算法在 roofline 模型上所处的位置。下图取自 Google 的 TPU 论文《In-Datacenter Performance Analysis of a Tensor Processing Unit》。由图中可见，LSTM 算法的运算强度最低，所以被卡在了 roofline 模型的上升区中间的地方，即 TPU 在执行 LSTM 算法的时候，由于内存带宽限制所以性能只有 3TOPS 左右，仅为峰值性能（90TOPS）的三十分之一。经典全联接神经网络（multi-layer perceptrons, MLP）的运算强度略好于 LSTM，也被卡在 roofline 曲线的上升区，实际执行性能大约在 10TOPS 左右。而卷积神经网络模型，尤其是 CNN0，由于卷积神经网络中能实现卷积核复用，因此运算强度非常高，于是可以非常接近 TPU roofline 曲线的屋顶（86 TOPS）。CNN1 模型虽然运算强度也很高，但是由于种种其他原因（论文中表示是由于 CNN1 模型的特征深度较浅无法完全利用 TPU 的计算单元）无法到达屋顶。这个例子又让我们看到了硬件-算法协同设计时的另一个要点：除了内存带宽之外还有「其他原因」可能让算法无法到达屋顶，我们要尽量减小这些「其他因素」！"
"日报;;【日报】谷歌AI与围棋冠军对战日程谷歌AI与围棋冠军对战日程，Facebook建VR社交团队，蝙蝠无人机……机器之心日报，精选一天前沿科技优质内容。谷歌人工智能挑战人类围棋冠军详情：将比赛5场继宣布AlphaGo实现突破性研究－计算机程序首次击败专业棋手之后， Google DeepMind今日公布了即将与过去十年最佳围棋手李世石之间的终极挑战的详细情况。3月9日至3月15日，AlphaGo将在韩国首尔与李世石进行5场挑战赛。比赛完全平等，获胜者将得到一百万美元奖金。如果AlphaGo获胜，奖金将捐赠给联合国儿童基金会(UNICEF)，STEM教育，以及围棋慈善机构(Go Charity)。因围棋步骤的绝对数量比宇宙的原子数还多，它一直被视为最复杂的电脑游戏之一，也是人工智能始终未解的挑战。DeepMind在上月的科学杂志Nature，以一篇论文公布了这一突破性进展的详细情况。

比赛将于北京时间中午12点在首尔四季酒店举行，具体日程如下：3月9日 (星期三)：首场比赛3月10日(星期四)：第二场比赛3月12日(星期六)：第三场比赛3月13日(星期日)：第四场比赛3月15日(星期二)：第五场比赛比赛将采用贴7.5目的中国规则(比赛结束时，后走棋的棋手贴目)。每位棋手各有两个小时布局时间，3次60秒的读秒，每场比赛预计需要大约4-5个小时。无人驾驶车缘何雪天就罢工？即便是测试里程已累积达100万英里的谷歌无人驾驶车，想要顺利通过考试拿到驾照，恐怕也得查查天气预报，挑个好天气才行。浙江大学控制科学与工程学院机器人实验室副教授刘勇说，无人驾驶系统往往包含多个传感器，其中广泛被用于构造三维地图的是激光雷达，是无人驾驶系统的核心元件。在冰雪天气，道路因结冰或积雪而导致反射特性发生变化，导致激光雷达的效果受到一定影响，从而影响到三维地图的构建，也就导致了无人车的“位置晕眩”。另外，无人车需要视觉传感器对车道线以及道路标志等信息进行识别，从而实现障碍避让并按规章行驶。冰雪天气对于道路标示检测也是一个很大的挑战，例如车道线和道路标志会被大雪局部覆盖，而车辆及道路两边的房子则会因为冰雪的覆盖而导致识别难度增大。“ Paper Skin”：拥有监测压力、温度等多种功能的纸质传感器在不久的将来可穿戴式多功能传感器不仅能被用来监测人体的生命体征，甚至可能改变人类与计算机的互动方式。 为了能尽快达成这一目标，日前来自沙特国王科技大学（KAUST）的一组研究人员使用低成本的日常用品研发了一种多功能纸质传感器，声称能实时测量压力、温度，另外还能监测湿度或酸碱度等。Facebook建地图：用人工智能详细标记哪些地方有人居住 &nbsp;

Facebook近日大秀在计算领域的肌肉，创建了能显示有那些人生活居住且他们的网络连接情况的地图。该地图项目由Facebook的Connectivity Labs推进负责，能够帮助公司更好为全球10%依然没有网络连接的公民提供更契合的连接解决方案。该实验室致力于通过无人机、卫星、激光等各种手段来为偏远地区提供网络连接。信用卡公司很希望将支付技术嵌入到服装中尽管当前人们已经习惯了用现金、信用卡、甚至手机和手表来支付，但是Visa和MasterCard的最新创意，却希望能将支付技术嵌入到每个人出门都会携带的物品——服装上！在巴塞罗那召开的移动世界大会（MWC 2016）期间，两家公司都宣布了进一步将支付技术推向连接设备的计划，比如服装、可穿戴设备、以及家用电器。小岛秀夫渴望开发VR恐怖游戏 感受真正的绝望在拉斯维加斯举办的DICE 2016展会上，小岛秀夫接受了IGN的采访，表达了自己对于VR设备的想法，透露出了他对于VR技术的激动之情。“我对VR感兴趣已经很久了，这个理念在20年前就已经提出来了，现在终于能够在实际项目中实现让我非常激动。”Facebook组建社交VR团队&nbsp;

&nbsp;

在三星的新品发布会上，Facebook CEO马克·扎克伯格（Mark Zuckerberg）的出现有些喧宾夺主，但他的观点为VR从业者带来一支强心针。扎克伯格在台上再次强调VR是“The Next Big Thing”。他说：“VR是下一个重要平台，任何人都将可以在上面创造和体验任何想要的东西。现在，VR技术大多用于游戏，但其很快将被应用到更多领域。”“被应用到更多领域”这句话说完没多久，扎克伯格就宣布，Facebook成立了一支名为Social VR（社交VR）的团队，探索人们如何利用VR技术进行沟通和连接，以及VR将来成为重要平台的进程。这意味着，通过VR头盔，我们可以在一分钟之内与远在千里的好友“见面”，甚至能“坐在对方的客厅中吃水果”。用脑洞来说，通过VR技术进行社交，人类可以获得“平行宇宙”的感觉。据了解，这支Social VR团队将由Oculus和Facebook内部员工组成，而两位领导人则来自视频游戏产业的高管Daniel James和Mike Booth。无人机将被用于野生生物研究David M. Bird博士是麦吉尔大学野生生物生物学名誉教师，他也是在BCIT无人机博览会2016上发言的7名发言人之一。Bird对Sunday Province说无人机可以以前所未有的方式帮助研究员们鸟瞰野生动物。“对于野生生物生物学而言，无人机的出现对该学科的研究带来了很大的改变，我个人认为改变将会极其深远。”Bird教授如是说。Bird说他与他的同事曾经利用无人机混进鹰巢清点鹰蛋和雏鹰的数目。他们还利用无人机监控过筑巢在悬崖边的水鸟与海鸟的栖息地。[视频]英国研发像蝙蝠一样飞行的新仿生无人机人类总是在向大自然获取很多东西，其中自然也包括智慧。毕竟，大自然蕴含的智慧可以说是无穷无尽的，而各种仿生技术的运用则也是人类向大自然获取智慧的一种方式。毕竟，有些物种合理的身体结构，使他们能够非常轻松的做一些事情，而这些都是值得我们去学习的。而在近期，英国科研人员则根据模仿蝙蝠的翅膀，设计出一种新型薄膜可变机翼。中国自主研发的复合型机器人开始批量生产隶属中国科学院的新松机器人自动化公司近日在沈阳宣布，其自主研发生产的复合型机器人已经开始批量生产，3个多月时间销售40余台，市场前景十分广阔。复合型机器人是手脚两项功能集于一身的新型机器人。在机器人领域，通用工业机器人被称为机械臂，用以替代人胳膊的功能；而智能移动机器人则用以替代人腿脚的功能。复合型机器人则是手脚并用，将两种功能组合在一起。诺克斯维尔房地产公司利用无人机增强客户购房体验诺克斯维尔房地产商正在东田纳西州利用无人机销售房屋。房地产公司The Voigt Group利用配备摄像头的无人机为房屋拍摄不同鸟瞰图像。“这是一件比较特立独行的行为，其他的房地厂商们目前没有这么做，而我们决定将无人机与我们的市场相结合。”委托代理Brittany Voigt如是说。宇航员“打广告” 微软全息眼镜上天了虽说距离普通百姓还有一段距离，微软HoloLens却在太空找到了新用途，近日，美国空间站启动了一个名为“Sidekick”的试验性计划，宇航员将佩戴HoloLens头盔，进行设备维修等工作。利用增强现实原理，在现实世界的画面上叠加信息和内容，在宇航员的头盔内部将会显示各种维修手册，这样宇航员可以腾出双手，提高维修效率。NASA希望将X战机召唤回来 以便测试新的航空技术战后的鼎盛时期，超音速的X系列验证机曾给查克·叶格（Chuck Yeager）等战机飞行员留下了相当浪漫的印象。作为奥巴马政府提出的10年计划的一部分，NASA航空工程部门的“新航空视野”（New Aviation Horizons）项目正试图在21世纪恢复这种验证机，并作为新兴绿色飞行技术的一个示范者。省时省伤亡：美军正在开发一款战地运输无人机美国军方正在探索为战地士兵提供送货无人机，尽管未来一段时间里，这项技术只会在民用领域铺开，但它终将改变战场的形态且有助于减少人员伤亡。当前，军用物资多由易受攻击的陆路运输提供，然而无人机则可将路线改到空中。对于那些身处高危险境的士兵来说，这一点尤为可贵。位于弗吉尼亚州Fort Lee的美军联合支援指挥部（CASCOM）军需战地实验室首席科学技术官Larry Perecko表示：“当我们用上空中自动运输的时候，就能够给对手造成更大的困境，因为我们将不仅限于地面路线”。新型植入式无线模块可从体内向外传输数据说到“植入式医疗设备”，很多人最先想到的会是心脏起搏器。由于缺少计算机分析或通信支持，这些设备通常只能很“愚笨”地工作。当大多数研究人员在尝试开发柔性逼真材料的时候，密歇根大学的大卫布牢（David Blaauw）教授却已经向着另一个方向进发。据IEEE Spectrum报道，他希望通过配备一个微型计算和通信系统的方式，提升这些设备的智能程度。微芯片人工肾脏有望助人类终结透析据美国范德堡大学最近消息，该校研究人员正在用微芯片滤膜和活的肾脏细胞创造一种可植入的人工肾脏，能将身体产生的废物过滤出去。这一技术或使肾病患者彻底摆脱透析。据科技日报近期消息，该校医学中心肾病学家、医学副教授威廉姆·费塞尔说，他们正在创造一种生物混合设备，能模拟肾脏清除废物、盐和水，让病人不再需要透析。现在的目标是将设备做得足够小，可植入病人体内。美军欲开发新技术使用3D打印 “现做现用”无人机美国陆军打算开发一项新技术，用3D打印方式在战场现场制造小型无人驾驶飞行器，旨在以最短时间响应前线士兵的不同需求，同时降低装备成本。战地无人机制 造系统包括设计单元和制造单元，使用计算机辅助设计软件，按照士兵的任务需求个性化定制多种无人机。电脑设计完成后，3D打印机制造必要的无人机部件，然 后装配特定任务所需的现成电子装置，就可交付使用。 &nbsp; &nbsp;机器之心编辑整理。"
"人工智能;;Facebook的人工智能通过读童书来学习「多看书」可能是对锻炼语言基础的学生最好的建议。其实，这个建议对人工智能来说同样奏效。

在2月18日，Facebook发布了多个用来训练他们的神经网络的数据集。其中一个数据集全是例如《丛林之书》，《彼得·潘》，《小妇人》，《圣诞颂歌》，《爱丽丝梦游仙境》等经典小说。总共有100多个故事，都来自免费网上图书馆Project Gutenberg。&nbsp;这是专门为初出茅庐的AI定制的阅读列表。Facebook把这种用阅读培训AI的方法叫做「儿童图书测试（Children’s Book Test）」，同时也用它测试电脑的阅读能力。

Facebook的研究人员利用阅读列表上的一些故事范本训练出神经网络。然后，他们在尚未读过的故事中找到一些短摘录，但最后一句话缺失一些单词，他们让神经网络从几个选项中选出正确的单词，将最后一句话填补完整。

例如，以下的例子是从桑顿·伯吉斯（Thornton Burgess）的《蟾蜍先生历险记》摘选的一段话。AI选择了用「惊吓」（红色部分）填写空白（蓝色部分）[/caption]

研究人员认为，AI能够回答这些问题证明了它们可以通过借鉴更广泛的语境来做决定，这是表征和记忆复杂信息的关键技能。类似的思维方式驱动了Facebook设计的另一个智力测验：让AI回答短篇故事人物之间的关系。

然而，Facebook的AI并不仅从书本中学习知识。当它不读书的时候，它还努力钻研电影中的机智问答（Movie Trivia）。Facebook不仅公布了他们的儿童图书测试数据，还公布了从在线常识素材库Freebase上获取的108 442个问题。这些问题包括：「另类嘻哈乐的知名艺术家有谁」和「哪个国家使用南岛语系中Tomadino语」等等。&nbsp;Facebook的AI要读的书单Andrew Lang Prince Prigio Andrew Lang Prince Prigio from &quot;His Own Fairy Book&quot; Andrew Lang Prince Ricardo of Pantouflia Andrew Lang The Blue Fairy Book Andrew Lang The Brown Fairy Book Andrew Lang The Crimson Fairy Book Andrew Lang The Grey Fairy Book Andrew Lang The Lilac Fairy Book Andrew Lang The Orange Fairy Book Andrew Lang The Pink Fairy Book Andrew Lang The Red Fairy Book Andrew Lang The Violet Fairy Book Andrew Lang The Yellow Fairy Book Charles Dickens A Christmas Carol Charles Dickens The Cricket on the Hearth Charles Kingsley The Water-Babies Harriet Elizabeth Beecher Stowe Uncle Tom&#39;s Cabin, Young Folks&#39; Edition James Matthew Barrie Peter Pan James Matthew Barrie Peter Pan in Kensington Gardens, Version 1James Matthew Barrie Peter Pan in Kensington Gardens, Version 2James Matthew Barrie Peter and Wendy Lewis Carroll Alice&#39;s Adventures Under Ground Lewis Carroll Through the Looking-Glass Lewis Carroll Alice&#39;s Adventures in Wonderland Louisa May Alcott Aunt Jo&#39;s Scrap-Bag, Volume 5 Louisa May Alcott Aunt Jo&#39;s Scrap-Bag Louisa May Alcott Aunt Jo&#39;s Scrap-Bag VI Louisa May Alcott Eight Cousins Louisa May Alcott Flower Fables Louisa May Alcott Jack and Jill Louisa May Alcott Jo&#39;s Boys Louisa May Alcott Kitty&#39;s Class Day And Other Stories Louisa May Alcott Little Men Louisa May Alcott Little Women Louisa May Alcott Marjorie&#39;s Three Gifts Louisa May Alcott The Mysterious Key And What It Opened Louisa May Alcott Under the Lilacs Lucy Maud Montgomery Anne&#39;s House of Dreams Lucy Maud Montgomery Anne Of Avonlea Lucy Maud Montgomery Anne Of Green Gables Lucy Maud Montgomery Anne Of The Island Lucy Maud Montgomery Chronicles of Avonlea Lucy Maud Montgomery Further Chronicles of Avonlea Lucy Maud Montgomery Kilmeny of the Orchard Lucy Maud Montgomery The Golden Road Lucy Maud Montgomery Lucy Maud Montgomery Short Stories, 1896 to 1901 Lucy Maud Montgomery Lucy Maud Montgomery Short Stories, 1902 to 1903 Lucy Maud Montgomery Lucy Maud Montgomery Short Stories, 1904 Lucy Maud Montgomery Lucy Maud Montgomery Short Stories, 1905 to 1906 Lucy Maud Montgomery Lucy Maud Montgomery Short Stories, 1907 to 1908 Lucy Maud Montgomery Lucy Maud Montgomery Short Stories, 1909 to 1922 Lucy Maud Montgomery Rainbow Valley Lucy Maud Montgomery Rilla of Ingleside Lucy Maud Montgomery The Story Girl Nathaniel Hawthorne Little Annie&#39;s Ramble (From &quot;Twice Told Tales&quot;) Nathaniel Hawthorne Snow Flakes (From &quot;Twice Told Tales&quot;) Nathaniel Hawthorne Sunday at Home (From &quot;Twice Told Tales&quot;) Nathaniel Hawthorne Tanglewood Tales Nathaniel Hawthorne The Gorgon&#39;s Head Nathaniel Hawthorne The Lily&#39;s Quest (From &quot;Twice Told Tales&quot;) Nathaniel Hawthorne The Miraculous Pitcher Nathaniel Hawthorne The Paradise of Children Nathaniel Hawthorne The Seven Vagabonds (From &quot;Twice Told Tales&quot;) Nathaniel Hawthorne The Sister Years (From &quot;Twice Told Tales&quot;) Nathaniel Hawthorne The Three Golden Apples Nathaniel Hawthorne The Threefold Destiny (From &quot;Twice Told Tales&quot;) Nathaniel Hawthorne The Toll Gatherer&#39;s Day (From &quot;Twice Told Tales&quot;) Nathaniel Hawthorne The Village Uncle (From &quot;Twice Told Tales&quot;) Nathaniel Hawthorne The Vision of the Fountain (From &quot;Twice Told Tales&quot;) Nathaniel Hawthorne Twice Told Tales Oscar Wilde The Happy Prince and Other Tales Robert Louis Stevenson Kidnapped Robert Louis Stevenson The Black Arrow Robert Louis Stevenson Treasure Island Rudyard Kipling Just So Stories Rudyard Kipling Kim Rudyard Kipling Puck of Pook&#39;s Hill Rudyard Kipling The Jungle Book Rudyard Kipling The Second Jungle Book Thornton Waldo Burgess Blacky the Crow Thornton Waldo Burgess Bowser The Hound Thornton Waldo Burgess Lightfoot the Deer Thornton Waldo Burgess Mother West Wind&#39;s Children Thornton Waldo Burgess Mother West Wind &quot;How&quot; Stories Thornton Waldo Burgess Mother West Wind &quot;Where&quot; Stories Thornton Waldo Burgess Mother West Wind “Why” Stories Thornton Waldo Burgess Mother West Wind&#39;s Animal Friends Thornton Waldo Burgess Mrs. Peter Rabbit Thornton Waldo Burgess Old Granny Fox Thornton Waldo Burgess Old Mother West Wind Thornton Waldo Burgess The Adventures of Buster Bear Thornton Waldo Burgess The Adventures of Chatterer the Red Squirrel Thornton Waldo Burgess The Adventures of Danny Meadow Mouse Thornton Waldo Burgess The Adventures of Grandfather Frog Thornton Waldo Burgess The Adventures of Old Mr. Toad Thornton Waldo Burgess The Adventures of Jerry Muskrat Thornton Waldo Burgess The Adventures of Jimmy Skunk Thornton Waldo Burgess The Adventures of Lightfoot the Deer Thornton Waldo Burgess The Adventures of Mr. Mocker Thornton Waldo Burgess The Adventures of Paddy the Beaver Thornton Waldo Burgess The Adventures of Poor Mrs. Quack Thornton Waldo Burgess The Adventures of Prickly Porky Thornton Waldo Burgess The Adventures of Reddy Fox Thornton Waldo Burgess The Adventures of Unc&#39; Billy Possum Thornton Waldo Burgess Whitefoot the Wood Mouse Thornton Waldo Burgess Happy Jack Thornton Waldo Burgess The Adventures of Johnny Chuck Washington Irving The Legend of Sleepy Hollow&nbsp;"
机器人;;为什么机器讲话的声音难以做到娓娓动听？你也许看过IBM Watson在2009年智力竞赛节目Jeopardy!上的精彩表现，又或者瞥过一眼该科技公司让Watson和网球女将Serena Williams，经济学家Richard Thaler，和摇滚音乐人Bob Dylan对话的最新广告。即使没有，你也应该在某个时间段和会说话的计算机接触过。但是，创造一个能交谈并且有说服力的电脑真的很难。在纽约时报周一的报道中，专栏作家John Markoff就讨论了创造IBM Watson语音功能背后的心血，并探讨了创造更加自然、可令人接受的电脑语音的不易。语音功能是人机交互的迷人挑战之一：当涉及到声音通讯，社交和情感线索是非常重要的。如果类似Apple Siri和Amazon Alexa的助手软件的声音不自然，沟通将会很不愉快。假设类似的系统不能够识别用户的语调并相应地适度调节自己的声音，那结果也会很令人烦恼。用户在无奈中反复问同一个问题，感觉就像被不断生产相同回复的人工声音侮辱了智商。自Siri问世以来，笔者就撰文写过在创造可以娱乐用户的产品同时，应该避免惹怒他们的重要性。事实上，把人工智能融入到现有社会结构的需求，解释了为什么我们觉得有必要给机器人分配类似性别的特征。也许这甚至可以解释为什么Apple最近收购了Emotient，一家专注于理解和应对人类情感的公司。Joaquin Phoenix在电影《她》中爱上人工智能同样有趣的是拥有一个真正引人入胜的情感功能强大的计算机，像导演Spike Jonze在电影《她》中刻画的那样。不过，就连它也很难解码并且模仿人类沟通的细微之处。正如IBM Watson多式联运实验室的高级经理Michael PichEny对纽约时报说的那样：「一个好电脑的界面是一件艺术品，并应该被视为艺术品来对待。」随着机器学习和运用人工智能的传播，技术人员正面临着人工智能变得太过真实的问题——近乎毛骨悚然的实感。其中一个突出的领域是科技驱动的语音技术，纽约时代专栏作家John Markoff写道。当类似Apple的Siri，Amazon的Alexa的声控助理被安装进像iPhone和Amazon Echo的设备，软件设计师开始注意语言和如何跨越让令拟人技术有毛骨悚然感觉的「恐怖谷」（uncanny valley）。该理论的假设指出，由于机器人与人类在外表、动作上相似，所以人类亦会对机器人产生正面的情感；直到一个特定的程度，他们的反应便会突然变得极其负面。哪怕机器人与人类只有一点点的差别，都会显得非常显眼刺目，使整个机器人显得非常僵硬可怖，使人有面对行尸走肉的感觉。可是，当机器人和人类的相似程度继续上升，相当于普通人之间的相似度的时候，人类对他们的情感会再度回到正面，产生人类与人类之间的移情作用。恐怖谷理论「恐怖谷理论」（uncanny valley）在1970年由日本机器人专家森政弘提出。随着机器学习创业公司的萌芽和类似Google和Facebook的巨型科技公司涉水人工智能领域，「恐怖谷理论」变成了一个越来越困扰技术人员的现象。它可以催生我们对人工智能的恐惧，导演Thomas Gibbon甚至在2014年指导过同名舞台剧。本文作者：Mandy，参考来源MIT TR和NYT。
"日报;;日报 | 李世石坚信自己能赢AlphaGo李世石坚信自己能赢AlphaGo，中国引力波「太极计划」，VR泳装秀……机器之心日报，精选一天前沿科技优质内容。又一家美国纸媒做了VR视频：这次是泳装秀继《纽约时报》之后，美国知名体育媒体《体育画报》也开始利用虚拟现实技术。今年，《体育画报》泳装特辑将发布iOS和Android应用，其中将提供新闻、照片，视频，以及可通过配套设备体验的虚拟现实视频片段。这一名为“2016泳装特辑”的应用可以通过iOS和Android设备免费下载。用户可以在应用内获得今年的所有照片。想在空间探测引力波？中科院有“太极计划”中国科学院16日举行“空间引力波探测与研究”媒体见面会。中国科学院从2008年开始发起“太极计划”，这是一个中欧合作的国际合作计划，它有两个方案。方案I是参加欧洲空间局的eLISA双边合作计划，今年秋天将召开第三次双方科学家会议，完成双边合作的可行性报告，然后各自向主管部门呈报，由双方主管部门审批后执行。方案II就是发射一组中国的引力波探测卫星组，与2035年左右发射的eLISA卫星组同时邀游太空，进行低频引力波探测。方案II拟于2033年前后发射，实现中国大型先进科学卫星计划的突破。届时，中国卫星组与eLISA卫星组同时在空间独立进行引力波探测，互相补充和检验测量结果，中国将成为国际上空间引力波研究最重要基地之一。Project Loon将于今年进入运营商测试阶段Google为偏远地区提供互联网接入的Project Loon高空热气球项目，在经历了残酷的开发阶段后仍幸存了下来。到了今年，它还将在印度尼西亚等地展开运营商测试。不过Google几乎还没有达成，因为它还忙着找到一个尽可能便宜、且足够坚固耐用的气球设计。它不仅可以漂浮，还可以导航至预定的平流层。延续摩尔定律: 新型超平面锡氧半导体材料有望让芯片提速百倍近年来，半导体行业总是笼罩在摩尔定律难以为继的阴霾之下，但是新材料的出现，或可让它迎来又一个拐点。美国犹他州大学的工程师们，已经发现了一种由一氧化锡制成、只有单原子厚度的新型平面材料。这种材料可让电荷以更快的速度通过，远胜硅与其它3D材料。相比之下，在传统电子设备上，电荷会以各个方向穿过晶体管、以及玻璃衬底上其它由硅层组成的部件。全新芭比娃娃梦之屋采用物联网与语音控制去年Mattel首次发布了他们的支持WiFi功能的芭比娃娃，这种芭比娃娃的娱乐性与可互动性大大增强。那么一个能联网的芭比娃娃显然也需要一个网络友好型的住所。在今年的玩具展上Mattel恰恰就展示了他们的新产品——芭比娃娃梦之屋。这间可以联网的玩具小屋还可以与配套的移动应用相连，联网功能使芭比娃娃梦之屋可以接受孩子们的语音命令，小屋里的设施就可以根据语音命令做出相应的行为，例如电灯的开启和关闭、电梯的上升与下降。NASA放出12张照片拼接的地质图：有望揭示冥王星的进化过程通过新地平线号在2015年7月14日飞掠冥王星时或获得的数据，美国宇航局（NASA）的科学家们已经拼接出了冥王星地表上大块区域地质图。根据新视野号获得的第一张图像，科学家们就已经被它的地址多样性给震撼到了，而这类地质图则有望帮助他们揭开这颗矮行星的形成与进化之谜。为了不至于眼花缭乱，并让大家搞清楚冥王星地质结构上的拼接状况，NASA的行星科学家们正在开发它的地表地图。印度将运行智能列车据《印度时报》2月15日报道，印度铁路将在新德里运行全新的智能列车，旅客们能够更加便捷地获取旅程的相关信息，并享受宾至如归的乘坐体验。车厢内部装饰豪华美观，内置GPS定位的到站提醒警报，显示车厢预定情况的LED显示图以及Wi-Fi设备，使乘客及时掌握车厢信息。乘客还能通过卧铺指示器查询到卧铺车厢的预定情况。传三星准备开发机器人 用在教育和工业领域北京时间2月16日消息，韩国媒体援引消息人士的话称，三星准备开发机器人，它认为机器人可以成为公司的营收源之一。三星已经从电信业务部抽调部分员工参与机器人的研发，该机器人主要用在教育和工业领域。消息人士称：“三星增加了机器业务的投资——比如机器人和无人机，这进一步说明三星的策略是降低对不稳定B2C业务的依赖，转而依靠更加稳定的B2B业务。”新型影像系统用光电纤维取代了镜头MIT的科学家们已经开发出了一种新型影像设备，特色是用一把松散的光纤束取代了传统的镜头，因此也无需防护罩。这管纤维束可以沉浸在液体中并输出图像，在油田、含水层和管道领域拥有不错的前景。紧缚的纤维束还可以当做小直径的内视镜，并且无需额外的电子设备，纤维的另一端则是光电传感器阵列。拖拉机也自动驾驶 日本发力IT农业日本企业正相继开展推进农业IT(信息技术)化的尝试。据《日本经济新闻》2月16日报道，生产农业机械的久保田1月19日推出了可完成无人耕作等的自动驾驶拖拉机。而日本另一家农机生产企业洋马则计划于近期开始利用小型无人机进行土壤测定。随着老龄化的加剧，日本农业出现了人手不足、生产难以顺利推进等问题，因此将借助IT加以克服。各企业计划首先在日本国内销售，今后力争拓展至亚洲市场。【视频】NASA使用HoloLens展示“Sidekick”项目美国航天航空局（NASA）的“Sidekick”项目旨在为宇航员提供虚拟助手，以缩短训练时间，提高完成任务的效率。此前NASA已与微软合作，为国际空间站的宇航员送去两台HoloLens设备。而在近日举行的“Vision Summit 2016”虚拟现实增强现实技术大会上，来自NASA喷气推进实验室的Jeff Norris向外界展示其“Sidekick”项目同时也能通过显示全息图像为航天器设计师提供支持。李世石：AlphaGo和我差2子 我绝不会输3月9日起，李世石将与谷歌人工智能程序AlphaGO展开悬赏100万美元的举世瞩目的围棋人机大战，这是1997年IBM“深蓝”计算机在国际象棋项目上向人类发起挑战并获得成功之后，人工智能程序向人类发出的又一重大挑战。围棋是唯一没有被计算机攻克的博弈游戏项目，2014年10月，谷歌Deep Mind团队开发的AlphaGO程序分先5比0战胜了欧洲围棋冠军樊麾二段，标志着计算机程序已具备了分先与围棋职业棋手对抗的实力。2月14日，李世石接受了韩《朝鲜日报》资深围棋记者李洪烈的电话采访，他表示没有任何紧张感，也不想可以联系。他表示：“除非出现不可理喻的低级失误，我绝不会输。”【视频】NASA将太阳动力学天文台第6年拍摄的太阳图像制作成视频美国航空航天局（NASA）在今天发布了一段由太阳动力学天文台（SDO）第6年拍摄的太阳图像制作而成的震撼视频，清晰地展示了太阳活动周期的表面活动状况。太阳动力学天文台（SDO）于2010年2月11日发射升空，SDO的使命是以小尺度的时间和空间下以多波段研究太阳大气层，以了解太阳对地球和近地球太空区域的影响。新3D打印机可“产出”器官、组织和骨骼美国维克弗斯特大学的科学家们表示，他们业已找到一种方法让3D打印机产出的骨骼、肌肉和软骨组织实现血液的流动，并让细胞得以生长，未来就可以打印出鲜活的组织供病人移植所用。据合众社报道，维克弗斯特大学再生医学研究所近10年来一直在研发组织和器官的整合打印系统，使用类塑料材质来塑造组织，运用水基凝胶来为组织递送鲜活的细胞。英国测量局绘制首张火星地图看过《火星救援》的读者应该对主人公星球自救时拿出地图规划逃生记忆犹新，据外媒报道，英国测量局终于在近日绘制完成首张火星地图。此次测绘基于NASA提供的海拔数据，简单易懂，包括陨石坑的深度等，分为实物版和电子版，大小覆盖了相当于英国国土面积40倍的火星亦即美国大小，占火星表面积的7%。十年黄蜂导航研究：有助智能无人机更快找到回家路澳大利亚国立大学的研究人员们，再次证明了“自然要比人类发明者更加聪明”。根据近十年来针对黄蜂（wasps）导航机制的研究，其发现有助于无人机和自动驾驶机器人变得更加智能。该团队仔细研究了在地面筑巢的黄蜂的飞行行为，尽管该物种会飞出很长远的距离，当仍能在觅食后轻松找到回家的路。只需拍拍手：日产“自驻办公椅”就会在桌边停好在不远的将来，我们或许只需拍拍手掌、或者按下一个触控按键，就可以让车子自行在车位上停好。不过在实现汽车的自动驾驶之前，日产（Nissan）的“自驻办公椅”（Intelligent Parking Chair）却已经从纸面跃为现实。无论是办公室还是会议间、椅子数量的多寡，只需拍一下手，这些智能办公椅都能够做到从容不迫。1.125Tb/s - 英国研究人员达成有史以来最快的数据传输速率伦敦大学学院（UCL）的研究人员已经达成数字信息有史以来最快的数据传输速率。作为专注于研究光传输系统容量限制的研究人员，他们成功取得1.125Tb/s的数据传输速率。这个速率是团队努力增加家庭和企业网络系统可用数据传输速度研究的一部分。美国研发可承受超过自身1000倍重量的新材料起重机或吊车这种类似设备，虽然我们平时在生活当中不经常使用，但是还是会经常接触到的。而起重机或吊车能够向上运输多重的物体，除了和动力有关之外，还 和他们的材料能够承受多重的重量有关。于是，我们平时看到吊车的“臂膀”本身就是非常强健有力的，而这也就造成了他们的制作成本较高，而使用起来也相对困 难。而现在，美国罗切斯特大学的一项新研究或许将能够使这种情况被改变。据悉，在近期，罗切斯特大学开发出一种新的聚合物材料，它可以承受超过自身 1000 倍重量的物体，这种能力是金属材料很难达到的！虽然在大自然当中，这种能力并非寻找不到。毕竟我们身边最不起眼的蚂蚁就能够举起超过自身重量最多达到 5000 倍的物体。然而以人类现在的科技来说，这依然是一个大的进步！

&nbsp;机器之心编辑出品。"
"nan;;寻找大脑内部的时钟来自波士顿大学的一项研究表明我们的大脑可能使用同一群神经元来表征时间和空间，论文发表在《神经》（Neuron）上，该项研究是约翰·奥基夫等人的空间定位细胞研究的延续，而约翰·奥基夫等人2014年凭借空间定位细胞得了诺贝尔生理及医学奖。我们的大脑具有非凡的监测时间的能力。一个司机可以准确地判断黄灯变红前还剩下多少时间够闯过去，一个舞者能够保持同一个节奏到毫秒精度。但大脑究竟如何追踪记录时间仍是个谜。研究者们已经定位了跟运动，记忆，颜色视觉以及其它功能相关的脑区，却并未发现感知时间的。事实上，我们的神经计时器已经难以捉摸到了这种地步，大多数科学家都认为这种机制是分布在整个大脑的，不同的脑区有不同的监测器以根据各自需要来追踪记录时间。在过去的几年里，一些研究者获得了越来越多的证据，表明那些监测个体在空间中位置的细胞同时也对时间的流逝进行标记。这意味着以在记忆和空间定位方面扮演重要角色而著称的海马体（hippocampus）和内嗅皮层（entorhinal cortex）同时也起到某种计时器的作用。在十一月发表的一项研究中，波士顿大学的神经科学家Howard Eichenbaum及其合作者表明，构成大鼠脑内GPS系统的细胞的可塑性好于预期。这些细胞被称作网格细胞（grid cells），它们有着典型行为方式，类似于航位推算法（译者注：知道当前时刻位置的条件下，通过测量移动的距离和方位，推算下一时刻位置），即当大鼠位于某个特定的位置时，某些特定的神经元会发放动作电位。（做出这一发现的科学家分享了2014年的诺贝尔奖）。Eichenbaum发现，当大鼠被保持在原地，譬如当它跑步机上跑动时，这些细胞对距离和时间都进行追踪记录。这项工作显示大脑对空间和时间的感知是相互交织的。这些发现有助于拓展我们对大脑的记忆和导航系统工作机制的理解。或许网格细胞（grid cells）和其他类似GPS的细胞都不仅响应空间变化，而且能够编码任何相关的性质：时间，气味，甚至味道。「这可能指向海马体的一个很宽泛的功能，」加州大学旧金山分校研究记忆和海马体的神经科学家Loran Frank这样说道。「它找出编码经历的关键神经元群体，随后把经历映射到其上。」接下来这些映射构建起记忆的框架，为我们永不止歇地流淌着的过去经验提供一个组织系统。「海马体就是记忆在时间和空间中了不起的组合者， 」Eichenbauma说，「它提供一个时空框架，其他的事件都在这个框架下发生。」为了研究海马体如何监测时间，科学家们让大鼠在滚轮或小跑步机上跑动。 这种装置能够让大鼠的位置和行为保持恒定，使研究者得以专注于跟时间相关的神经信号。（大鼠总是烦躁，无法静止不动，因此跑动有助于排除它们平时焦躁不安的行为的干扰。）当不同细胞发放动作电位时，深深植入在脑中的电极会记录到它们。在Eichenbaum的实验中，大鼠在跑步机上跑动一定长的时间，譬如说15秒，随后获得奖励。在一次次重复的过程中，它的大脑学习追踪记录这个15秒的时间间隔。一些神经元在第一秒发放，另一些在第二秒，以此类推，直到15秒结束。「每个细胞会在跑动时间中的不同时刻发放，直到它们填满整个时间间隔，」Eichenbaum说道。这种编码相当精确，研究者们能够通过仅仅观察哪些细胞是活跃的来预测动物在跑台上跑了多久。Eichenbaum的团队也在不同的跑步机速度下重复了这些实验，以确认这些细胞并不是在简单地标记跑动的距离。（一些细胞的确也追踪距离，但有一些似乎完全只跟时间有关。）这些细胞的行为酷似秒表—每次你开始计时，同样神经活动模式就出现。这些被叫做「时间细胞（time cells）」的神经元显然能够标记时间，然而它们是如何做到这一点的尚不清楚。这些细胞的行为酷似秒表——每当你开始计时，同样的神经活动模式就会出现。当研究者们改变实验条件，譬如说把跑动的持续时间从15秒增加到30秒时，海马体中的细胞就会创造一个新的发放模式填满这个新的时间区间。这就设置秒表来适应各种不同的时间尺度。此外，时间细胞的活动依赖于情景。当动物面临时间起重要作用的情况时，这些细胞才标记时间。当其他变量起作用时，这些细胞的表现就会不同。比如说，我们让老鼠去探索一个新环境，这些时间细胞就会映射到空间上去（而非时间）；单个特定的时间细胞会被动物所处的特定位置激活，而不是在特定的时间激活。所以，Eichenbaum的工作与神经科学15年来研究趋势契合，显示出海马比科学家预期的更加多能。传统研究认为它可以绘制方位图（40年前发现了位置编码细胞），但越来越多的证据表明，它也可以编码其他类型的信息。最新的研究图景表明，位置细胞不仅能映射空间，还对应其它相关的变量。包括时间，还有其它可能。比如根据Frank的说法，「品酒师（的海马）或许有酒的味道和气味的编码空间。」但许多科学家仍把海马大体上视作空间结构。按照他们的观点，其神经环路经过演化适合追踪记录位置，其它信息都只是记录空间位置的基础上。加州大学尔湾分校的神经科学家Bruce McNaughton说：「海马体所提供的编码，在本质上是空间性质的。」虽然Eichenbaum的发现挑战了这一观点，他们并不回避。「我们能明确的是，位置细胞可以代表空间之外的信息，」约翰霍普金斯大学的神经科学家David Foster说。「但不那么清楚的是，他们是否可以编码单纯的时间流逝。」在计时的跑步机实验中，大鼠似乎在做计数之类的事情。但这些细胞是否标记了时间本身的流逝，还是对仅仅看起来像时间的什么东西做出反应？「我们不知道什么原理驱使细胞在特定的时刻被激活，但我不认为是由于时间，」Eva Pastalkova说，她是霍华德休斯医学研究所Janelia研究园区的神经科学家（弗吉尼亚州）。「因为这不够精确，它们不像是滴答作响的时钟。」「大脑中有没有神经元专门负责记录时间？」György Buzsáki是纽约大学神经科学研究所的神经科学家，他的实验室做了不少开创性的实验，探索海马体如何追踪记录时间。他提出，这些细胞不是在检测时间，可能只是在做其它的事，比如回忆一条穿过迷宫的路，或者计划下一步行动。回忆和打算未来都是在时间维度上进行的，所以时间细胞可能只是在反映这种精神活动。「我认为这是头等问题：大脑中有没有神经元专门负责记录时间，不干别的？」György Buzsáki说，「还是说，所有的神经元的运作都能按照序列发生，在此实验中这些功能可以被理解成时间？」附论文摘要：在跑动过程中，网格细胞整合逝去的时间和跑过的距离论文作者：Kraus BJ, Brandon MP, Robinson RJ 2nd, Connerney MA, Hasselmo ME, Eichenbaum H.网格细胞的空间尺度可以由自身产生的运动信息提供，或者是来自从外部环境的感觉信息。为了确定网格细胞的活动是否与外界信息无关的反映了跑过的距离或着逝去的时间，我们记录了跑步机上跑动的大鼠脑中网格细胞的活动。此时，网格细胞的活动受到位置影响应该很微弱，但同一组电极上记录到的大多数网格细胞和其它神经元大批强烈地反映了距离和时间的组合，也有一些仅反应距离或时间。比起非网格细胞，网格细胞受时间和距离的调谐更为强烈。在跑步机上运行时，许多网格细胞表现出多个放电域，同时也观察到其周期性的激活，提示存在一种共同的信息处理模式。这些观察结果表明，在没有外部动态线索的情况下，网格细胞整合自身产生的距离和时间信息以编码表征经验。"
